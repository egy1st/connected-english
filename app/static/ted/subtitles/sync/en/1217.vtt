WEBVTT

00:00:12.150 --> 00:00:15.150
I didn't always love unintended consequences,

00:00:15.150 --> 00:00:17.150
but I've really learned to appreciate them.

00:00:17.150 --> 00:00:19.150
I've learned that they're really the essence

00:00:19.150 --> 00:00:21.150
of what makes for progress,

00:00:21.150 --> 00:00:24.150
even when they seem to be terrible.

00:00:24.150 --> 00:00:26.150
And I'd like to review

00:00:26.150 --> 00:00:29.150
just how unintended consequences

00:00:29.150 --> 00:00:32.150
play the part that they do.

00:00:32.150 --> 00:00:37.150
Let's go to 40,000 years before the present,

00:00:37.150 --> 00:00:41.150
to the time of the cultural explosion,

00:00:41.150 --> 00:00:46.150
when music, art, technology,

00:00:46.150 --> 00:00:48.150
so many of the things that we're enjoying today,

00:00:48.150 --> 00:00:51.150
so many of the things that are being demonstrated at TED

00:00:51.150 --> 00:00:53.150
were born.

00:00:53.150 --> 00:00:56.150
And the anthropologist Randall White

00:00:56.150 --> 00:00:59.150
has made a very interesting observation:

00:00:59.150 --> 00:01:01.150
that if our ancestors

00:01:01.150 --> 00:01:03.150
40,000 years ago

00:01:03.150 --> 00:01:06.150
had been able to see

00:01:06.150 --> 00:01:08.150
what they had done,

00:01:08.150 --> 00:01:10.150
they wouldn't have really understood it.

00:01:10.150 --> 00:01:12.150
They were responding

00:01:12.150 --> 00:01:15.150
to immediate concerns.

00:01:15.150 --> 00:01:17.150
They were making it possible for us

00:01:17.150 --> 00:01:19.150
to do what they do,

00:01:19.150 --> 00:01:21.150
and yet, they didn't really understand

00:01:21.150 --> 00:01:23.150
how they did it.

00:01:23.150 --> 00:01:28.150
Now let's advance to 10,000 years before the present.

00:01:28.150 --> 00:01:30.150
And this is when it really gets interesting.

00:01:30.150 --> 00:01:33.150
What about the domestication of grains?

00:01:33.150 --> 00:01:36.150
What about the origins of agriculture?

00:01:36.150 --> 00:01:39.150
What would our ancestors 10,000 years ago

00:01:39.150 --> 00:01:41.150
have said

00:01:41.150 --> 00:01:43.150
if they really had technology assessment?

00:01:43.150 --> 00:01:45.150
And I could just imagine the committees

00:01:45.150 --> 00:01:47.150
reporting back to them

00:01:47.150 --> 00:01:50.150
on where agriculture was going to take humanity,

00:01:50.150 --> 00:01:53.150
at least in the next few hundred years.

00:01:53.150 --> 00:01:55.150
It was really bad news.

00:01:55.150 --> 00:01:57.150
First of all, worse nutrition,

00:01:57.150 --> 00:01:59.150
maybe shorter life spans.

00:01:59.150 --> 00:02:01.150
It was simply awful for women.

00:02:01.150 --> 00:02:03.150
The skeletal remains from that period

00:02:03.150 --> 00:02:08.150
have shown that they were grinding grain morning, noon and night.

00:02:08.150 --> 00:02:11.150
And politically, it was awful.

00:02:11.150 --> 00:02:14.150
It was the beginning of a much higher degree

00:02:14.150 --> 00:02:17.150
of inequality among people.

00:02:17.150 --> 00:02:20.150
If there had been rational technology assessment then,

00:02:20.150 --> 00:02:22.150
I think they very well might have said,

00:02:22.150 --> 00:02:25.150
"Let's call the whole thing off."

00:02:25.150 --> 00:02:29.150
Even now, our choices are having unintended effects.

00:02:29.150 --> 00:02:31.150
Historically, for example,

00:02:31.150 --> 00:02:34.150
chopsticks -- according to one Japanese anthropologist

00:02:34.150 --> 00:02:36.150
who wrote a dissertation about it

00:02:36.150 --> 00:02:38.150
at the University of Michigan --

00:02:38.150 --> 00:02:41.150
resulted in long-term changes

00:02:41.150 --> 00:02:43.150
in the dentition, in the teeth,

00:02:43.150 --> 00:02:45.150
of the Japanese public.

00:02:45.150 --> 00:02:48.150
And we are also changing our teeth right now.

00:02:48.150 --> 00:02:50.150
There is evidence

00:02:50.150 --> 00:02:52.150
that the human mouth and teeth

00:02:52.150 --> 00:02:54.150
are growing smaller all the time.

00:02:54.150 --> 00:02:57.150
That's not necessarily a bad unintended consequence.

00:02:57.150 --> 00:02:59.150
But I think from the point of view of a Neanderthal,

00:02:59.150 --> 00:03:01.150
there would have been a lot of disapproval

00:03:01.150 --> 00:03:04.150
of the wimpish choppers that we now have.

00:03:04.150 --> 00:03:07.150
So these things are kind of relative

00:03:07.150 --> 00:03:11.150
to where you or your ancestors happen to stand.

00:03:11.150 --> 00:03:13.150
In the ancient world

00:03:13.150 --> 00:03:16.150
there was a lot of respect for unintended consequences,

00:03:16.150 --> 00:03:19.150
and there was a very healthy sense of caution,

00:03:19.150 --> 00:03:21.150
reflected in the Tree of Knowledge,

00:03:21.150 --> 00:03:23.150
in Pandora's Box,

00:03:23.150 --> 00:03:25.150
and especially in the myth of Prometheus

00:03:25.150 --> 00:03:27.150
that's been so important

00:03:27.150 --> 00:03:29.150
in recent metaphors about technology.

00:03:29.150 --> 00:03:32.150
And that's all very true.

00:03:32.150 --> 00:03:34.150
The physicians of the ancient world --

00:03:34.150 --> 00:03:36.150
especially the Egyptians,

00:03:36.150 --> 00:03:38.150
who started medicine as we know it --

00:03:38.150 --> 00:03:40.150
were very conscious

00:03:40.150 --> 00:03:42.150
of what they could and couldn't treat.

00:03:42.150 --> 00:03:47.150
And the translations of the surviving texts say,

00:03:47.150 --> 00:03:49.150
"This I will not treat. This I cannot treat."

00:03:49.150 --> 00:03:51.150
They were very conscious.

00:03:51.150 --> 00:03:53.150
So were the followers of Hippocrates.

00:03:53.150 --> 00:03:55.150
The Hippocratic manuscripts also --

00:03:55.150 --> 00:03:58.150
repeatedly, according to recent studies --

00:03:58.150 --> 00:04:01.150
show how important it is not to do harm.

00:04:01.150 --> 00:04:03.150
More recently,

00:04:03.150 --> 00:04:05.150
Harvey Cushing,

00:04:05.150 --> 00:04:07.150
who really developed neurosurgery as we know it,

00:04:07.150 --> 00:04:10.150
who changed it from a field of medicine

00:04:10.150 --> 00:04:14.150
that had a majority of deaths resulting from surgery

00:04:14.150 --> 00:04:17.150
to one in which there was a hopeful outlook,

00:04:17.150 --> 00:04:19.150
he was very conscious

00:04:19.150 --> 00:04:22.150
that he was not always going to do the right thing.

00:04:22.150 --> 00:04:24.150
But he did his best,

00:04:24.150 --> 00:04:26.150
and he kept meticulous records

00:04:26.150 --> 00:04:29.150
that let him transform that branch of medicine.

00:04:29.150 --> 00:04:32.150
Now if we look forward a bit

00:04:32.150 --> 00:04:34.150
to the 19th century,

00:04:34.150 --> 00:04:36.150
we find a new style of technology.

00:04:36.150 --> 00:04:38.150
What we find is,

00:04:38.150 --> 00:04:41.150
no longer simple tools,

00:04:41.150 --> 00:04:43.150
but systems.

00:04:43.150 --> 00:04:45.150
We find more and more

00:04:45.150 --> 00:04:47.150
complex arrangements of machines

00:04:47.150 --> 00:04:49.150
that make it harder and harder

00:04:49.150 --> 00:04:51.150
to diagnose what's going on.

00:04:51.150 --> 00:04:53.150
And the first people who saw that

00:04:53.150 --> 00:04:56.150
were the telegraphers of the mid-19th century,

00:04:56.150 --> 00:04:58.150
who were the original hackers.

00:04:58.150 --> 00:05:01.150
Thomas Edison would have been very, very comfortable

00:05:01.150 --> 00:05:04.150
in the atmosphere of a software firm today.

00:05:04.150 --> 00:05:07.150
And these hackers had a word

00:05:07.150 --> 00:05:10.150
for those mysterious bugs in telegraph systems

00:05:10.150 --> 00:05:12.150
that they called bugs.

00:05:12.150 --> 00:05:16.150
That was the origin of the word "bug."

00:05:16.150 --> 00:05:18.150
This consciousness, though,

00:05:18.150 --> 00:05:21.150
was a little slow to seep through the general population,

00:05:21.150 --> 00:05:24.150
even people who were very, very well informed.

00:05:24.150 --> 00:05:26.150
Samuel Clemens, Mark Twain,

00:05:26.150 --> 00:05:28.150
was a big investor

00:05:28.150 --> 00:05:31.150
in the most complex machine of all times --

00:05:31.150 --> 00:05:33.150
at least until 1918 --

00:05:33.150 --> 00:05:35.150
registered with the U.S. Patent Office.

00:05:35.150 --> 00:05:37.150
That was the Paige typesetter.

00:05:37.150 --> 00:05:39.150
The Paige typesetter

00:05:39.150 --> 00:05:41.150
had 18,000 parts.

00:05:41.150 --> 00:05:44.150
The patent had 64 pages of text

00:05:44.150 --> 00:05:48.150
and 271 figures.

00:05:48.150 --> 00:05:50.150
It was such a beautiful machine

00:05:50.150 --> 00:05:53.150
because it did everything that a human being did

00:05:53.150 --> 00:05:55.150
in setting type --

00:05:55.150 --> 00:05:57.150
including returning the type to its place,

00:05:57.150 --> 00:05:59.150
which was a very difficult thing.

00:05:59.150 --> 00:06:01.150
And Mark Twain, who knew all about typesetting,

00:06:01.150 --> 00:06:04.150
really was smitten by this machine.

00:06:04.150 --> 00:06:07.150
Unfortunately, he was smitten in more ways than one,

00:06:07.150 --> 00:06:09.150
because it made him bankrupt,

00:06:09.150 --> 00:06:11.150
and he had to tour the world speaking

00:06:11.150 --> 00:06:14.150
to recoup his money.

00:06:14.150 --> 00:06:16.150
And this was an important thing

00:06:16.150 --> 00:06:18.150
about 19th century technology,

00:06:18.150 --> 00:06:20.150
that all these relationships among parts

00:06:20.150 --> 00:06:24.150
could make the most brilliant idea fall apart,

00:06:24.150 --> 00:06:26.150
even when judged by the most expert people.

00:06:26.150 --> 00:06:29.150
Now there is something else, though, in the early 20th century

00:06:29.150 --> 00:06:32.150
that made things even more complicated.

00:06:32.150 --> 00:06:35.150
And that was that safety technology itself

00:06:35.150 --> 00:06:37.150
could be a source of danger.

00:06:37.150 --> 00:06:40.150
The lesson of the Titanic, for a lot of the contemporaries,

00:06:40.150 --> 00:06:42.150
was that you must have enough lifeboats

00:06:42.150 --> 00:06:44.150
for everyone on the ship.

00:06:44.150 --> 00:06:47.150
And this was the result

00:06:47.150 --> 00:06:49.150
of the tragic loss of lives

00:06:49.150 --> 00:06:51.150
of people who could not get into them.

00:06:51.150 --> 00:06:54.150
However, there was another case, the Eastland,

00:06:54.150 --> 00:06:58.150
a ship that capsized in Chicago Harbor in 1915,

00:06:58.150 --> 00:07:01.150
and it killed 841 people --

00:07:01.150 --> 00:07:03.150
that was 14 more

00:07:03.150 --> 00:07:06.150
than the passenger toll of the Titanic.

00:07:06.150 --> 00:07:08.150
The reason for it, in part, was

00:07:08.150 --> 00:07:11.150
the extra life boats that were added

00:07:11.150 --> 00:07:14.150
that made this already unstable ship

00:07:14.150 --> 00:07:16.150
even more unstable.

00:07:16.150 --> 00:07:18.150
And that again proves

00:07:18.150 --> 00:07:21.150
that when you're talking about unintended consequences,

00:07:21.150 --> 00:07:23.150
it's not that easy to know

00:07:23.150 --> 00:07:25.150
the right lessons to draw.

00:07:25.150 --> 00:07:28.150
It's really a question of the system, how the ship was loaded,

00:07:28.150 --> 00:07:31.150
the ballast and many other things.

00:07:32.150 --> 00:07:35.150
So the 20th century, then,

00:07:35.150 --> 00:07:37.150
saw how much more complex reality was,

00:07:37.150 --> 00:07:40.150
but it also saw a positive side.

00:07:40.150 --> 00:07:43.150
It saw that invention

00:07:43.150 --> 00:07:45.150
could actually benefit from emergencies.

00:07:45.150 --> 00:07:47.150
It could benefit

00:07:47.150 --> 00:07:50.150
from tragedies.

00:07:50.150 --> 00:07:52.150
And my favorite example of that --

00:07:52.150 --> 00:07:54.150
which is not really widely known

00:07:54.150 --> 00:07:56.150
as a technological miracle,

00:07:56.150 --> 00:07:59.150
but it may be one of the greatest of all times,

00:07:59.150 --> 00:08:03.150
was the scaling up of penicillin in the Second World War.

00:08:03.150 --> 00:08:06.150
Penicillin was discovered in 1928,

00:08:06.150 --> 00:08:08.150
but even by 1940,

00:08:08.150 --> 00:08:11.150
no commercially and medically useful quantities of it

00:08:11.150 --> 00:08:13.150
were being produced.

00:08:13.150 --> 00:08:16.150
A number of pharmaceutical companies were working on it.

00:08:16.150 --> 00:08:18.150
They were working on it independently,

00:08:18.150 --> 00:08:20.150
and they weren't getting anywhere.

00:08:20.150 --> 00:08:22.150
And the Government Research Bureau

00:08:22.150 --> 00:08:24.150
brought representatives together

00:08:24.150 --> 00:08:26.150
and told them that this is something

00:08:26.150 --> 00:08:28.150
that has to be done.

00:08:28.150 --> 00:08:30.150
And not only did they do it,

00:08:30.150 --> 00:08:32.150
but within two years,

00:08:32.150 --> 00:08:34.150
they scaled up penicillin

00:08:34.150 --> 00:08:37.150
from preparation in one-liter flasks

00:08:37.150 --> 00:08:41.150
to 10,000-gallon vats.

00:08:41.150 --> 00:08:45.150
That was how quickly penicillin was produced

00:08:45.150 --> 00:08:49.150
and became one of the greatest medical advances of all time.

00:08:49.150 --> 00:08:51.150
In the Second World War, too,

00:08:51.150 --> 00:08:53.150
the existence

00:08:53.150 --> 00:08:55.150
of solar radiation

00:08:55.150 --> 00:08:58.150
was demonstrated by studies of interference

00:08:58.150 --> 00:09:02.150
that was detected by the radar stations of Great Britain.

00:09:02.150 --> 00:09:05.150
So there were benefits in calamities --

00:09:05.150 --> 00:09:07.150
benefits to pure science,

00:09:07.150 --> 00:09:09.150
as well as to applied science

00:09:09.150 --> 00:09:12.150
and medicine.

00:09:12.150 --> 00:09:15.150
Now when we come to the period after the Second World War,

00:09:15.150 --> 00:09:19.150
unintended consequences get even more interesting.

00:09:19.150 --> 00:09:21.150
And my favorite example of that

00:09:21.150 --> 00:09:24.150
occurred beginning in 1976,

00:09:24.150 --> 00:09:26.150
when it was discovered

00:09:26.150 --> 00:09:29.150
that the bacteria causing Legionnaires disease

00:09:29.150 --> 00:09:32.150
had always been present in natural waters,

00:09:32.150 --> 00:09:36.150
but it was the precise temperature of the water

00:09:36.150 --> 00:09:39.150
in heating, ventilating and air conditioning systems

00:09:39.150 --> 00:09:43.150
that raised the right temperature

00:09:43.150 --> 00:09:46.150
for the maximum reproduction

00:09:46.150 --> 00:09:48.150
of Legionella bacillus.

00:09:48.150 --> 00:09:50.150
Well, technology to the rescue.

00:09:50.150 --> 00:09:52.150
So chemists got to work,

00:09:52.150 --> 00:09:54.150
and they developed a bactericide

00:09:54.150 --> 00:09:57.150
that became widely used in those systems.

00:09:57.150 --> 00:10:01.150
But something else happened in the early 1980s,

00:10:01.150 --> 00:10:03.150
and that was that there was a mysterious epidemic

00:10:03.150 --> 00:10:06.150
of failures of tape drives

00:10:06.150 --> 00:10:08.150
all over the United States.

00:10:08.150 --> 00:10:11.150
And IBM, which made them,

00:10:11.150 --> 00:10:14.150
just didn't know what to do.

00:10:14.150 --> 00:10:17.150
They commissioned a group of their best scientists

00:10:17.150 --> 00:10:19.150
to investigate,

00:10:19.150 --> 00:10:21.150
and what they found was

00:10:21.150 --> 00:10:23.150
that all these tape drives

00:10:23.150 --> 00:10:26.150
were located near ventilation ducts.

00:10:26.150 --> 00:10:29.150
What happened was the bactericide was formulated

00:10:29.150 --> 00:10:31.150
with minute traces of tin.

00:10:31.150 --> 00:10:34.150
And these tin particles were deposited on the tape heads

00:10:34.150 --> 00:10:37.150
and were crashing the tape heads.

00:10:37.150 --> 00:10:40.150
So they reformulated the bactericide.

00:10:40.150 --> 00:10:42.150
But what's interesting to me

00:10:42.150 --> 00:10:44.150
is that this was the first case

00:10:44.150 --> 00:10:46.150
of a mechanical device

00:10:46.150 --> 00:10:49.150
suffering, at least indirectly, from a human disease.

00:10:49.150 --> 00:10:52.150
So it shows that we're really all in this together.

00:10:52.150 --> 00:10:54.150
(Laughter)

00:10:54.150 --> 00:10:57.150
In fact, it also shows something interesting,

00:10:57.150 --> 00:11:00.150
that although our capabilities and technology

00:11:00.150 --> 00:11:02.150
have been expanding geometrically,

00:11:02.150 --> 00:11:05.150
unfortunately, our ability to model their long-term behavior,

00:11:05.150 --> 00:11:07.150
which has also been increasing,

00:11:07.150 --> 00:11:10.150
has been increasing only arithmetically.

00:11:10.150 --> 00:11:13.150
So one of the characteristic problems of our time

00:11:13.150 --> 00:11:15.150
is how to close this gap

00:11:15.150 --> 00:11:18.150
between capabilities and foresight.

00:11:18.150 --> 00:11:21.150
One other very positive consequence

00:11:21.150 --> 00:11:24.150
of 20th century technology, though,

00:11:24.150 --> 00:11:28.150
was the way in which other kinds of calamities

00:11:28.150 --> 00:11:31.150
could lead to positive advances.

00:11:31.150 --> 00:11:34.150
There are two historians of business

00:11:34.150 --> 00:11:36.150
at the University of Maryland,

00:11:36.150 --> 00:11:38.150
Brent Goldfarb and David Kirsch,

00:11:38.150 --> 00:11:40.150
who have done some extremely interesting work,

00:11:40.150 --> 00:11:43.150
much of it still unpublished,

00:11:43.150 --> 00:11:45.150
on the history of major innovations.

00:11:45.150 --> 00:11:48.150
They have combined the list of major innovations,

00:11:48.150 --> 00:11:51.150
and they've discovered that the greatest number, the greatest decade,

00:11:51.150 --> 00:11:53.150
for fundamental innovations,

00:11:53.150 --> 00:11:57.150
as reflected in all of the lists that others have made --

00:11:57.150 --> 00:11:59.150
a number of lists that they have merged --

00:11:59.150 --> 00:12:02.150
was the Great Depression.

00:12:02.150 --> 00:12:05.150
And nobody knows just why this was so,

00:12:05.150 --> 00:12:08.150
but one story can reflect something of it.

00:12:08.150 --> 00:12:11.150
It was the origin of the Xerox copier,

00:12:11.150 --> 00:12:14.150
which celebrated its 50th anniversary

00:12:14.150 --> 00:12:16.150
last year.

00:12:16.150 --> 00:12:21.150
And Chester Carlson, the inventor,

00:12:21.150 --> 00:12:24.150
was a patent attorney.

00:12:24.150 --> 00:12:27.150
He really was not intending

00:12:27.150 --> 00:12:29.150
to work in patent research,

00:12:29.150 --> 00:12:33.150
but he couldn't really find an alternative technical job.

00:12:33.150 --> 00:12:35.150
So this was the best job he could get.

00:12:35.150 --> 00:12:39.150
He was upset by the low quality and high cost

00:12:39.150 --> 00:12:42.150
of existing patent reproductions,

00:12:42.150 --> 00:12:45.150
and so he started to develop

00:12:45.150 --> 00:12:48.150
a system of dry photocopying,

00:12:48.150 --> 00:12:51.150
which he patented in the late 1930s --

00:12:51.150 --> 00:12:55.150
and which became the first dry photocopier

00:12:55.150 --> 00:12:57.150
that was commercially practical

00:12:57.150 --> 00:12:59.150
in 1960.

00:12:59.150 --> 00:13:01.150
So we see that sometimes,

00:13:01.150 --> 00:13:03.150
as a result of these dislocations,

00:13:03.150 --> 00:13:05.150
as a result of people

00:13:05.150 --> 00:13:08.150
leaving their original intended career

00:13:08.150 --> 00:13:10.150
and going into something else

00:13:10.150 --> 00:13:12.150
where their creativity could make a difference,

00:13:12.150 --> 00:13:14.150
that depressions

00:13:14.150 --> 00:13:17.150
and all kinds of other unfortunate events

00:13:17.150 --> 00:13:20.150
can have a paradoxically stimulating effect

00:13:20.150 --> 00:13:22.150
on creativity.

00:13:22.150 --> 00:13:24.150
What does this mean?

00:13:24.150 --> 00:13:26.150
It means, I think,

00:13:26.150 --> 00:13:28.150
that we're living in a time of unexpected possibilities.

00:13:28.150 --> 00:13:31.150
Think of the financial world, for example.

00:13:31.150 --> 00:13:34.150
The mentor of Warren Buffett, Benjamin Graham,

00:13:34.150 --> 00:13:39.150
developed his system of value investing

00:13:39.150 --> 00:13:41.150
as a result of his own losses

00:13:41.150 --> 00:13:43.150
in the 1929 crash.

00:13:43.150 --> 00:13:45.150
And he published that book

00:13:45.150 --> 00:13:48.150
in the early 1930s,

00:13:48.150 --> 00:13:50.150
and the book still exists in further editions

00:13:50.150 --> 00:13:52.150
and is still a fundamental textbook.

00:13:52.150 --> 00:13:56.150
So many important creative things can happen

00:13:56.150 --> 00:13:59.150
when people learn from disasters.

00:13:59.150 --> 00:14:03.150
Now think of the large and small plagues that we have now --

00:14:03.150 --> 00:14:08.150
bed bugs, killer bees, spam --

00:14:08.150 --> 00:14:11.150
and it's very possible that the solutions to those

00:14:11.150 --> 00:14:14.150
will really extend well beyond the immediate question.

00:14:14.150 --> 00:14:17.150
If we think, for example, of Louis Pasteur,

00:14:17.150 --> 00:14:19.150
who in the 1860s

00:14:19.150 --> 00:14:21.150
was asked to study

00:14:21.150 --> 00:14:25.150
the diseases of silk worms for the silk industry,

00:14:25.150 --> 00:14:28.150
and his discoveries were really the beginning

00:14:28.150 --> 00:14:30.150
of the germ theory of disease.

00:14:30.150 --> 00:14:33.150
So very often, some kind of disaster --

00:14:33.150 --> 00:14:36.150
sometimes the consequence, for example,

00:14:36.150 --> 00:14:39.150
of over-cultivation of silk worms,

00:14:39.150 --> 00:14:41.150
which was a problem in Europe at the time --

00:14:41.150 --> 00:14:43.150
can be the key to something much bigger.

00:14:43.150 --> 00:14:45.150
So this means

00:14:45.150 --> 00:14:47.150
that we need to take a different view

00:14:47.150 --> 00:14:49.150
of unintended consequences.

00:14:49.150 --> 00:14:52.150
We need to take a really positive view.

00:14:52.150 --> 00:14:55.150
We need to see what they can do for us.

00:14:55.150 --> 00:14:57.150
We need to learn

00:14:57.150 --> 00:14:59.150
from those figures that I mentioned.

00:14:59.150 --> 00:15:02.150
We need to learn, for example, from Dr. Cushing,

00:15:02.150 --> 00:15:04.150
who killed patients

00:15:04.150 --> 00:15:06.150
in the course of his early operations.

00:15:06.150 --> 00:15:09.150
He had to have some errors. He had to have some mistakes.

00:15:09.150 --> 00:15:12.150
And he learned meticulously from his mistakes.

00:15:12.150 --> 00:15:14.150
And as a result,

00:15:14.150 --> 00:15:17.150
when we say, "This isn't brain surgery,"

00:15:17.150 --> 00:15:20.150
that pays tribute to how difficult it was

00:15:20.150 --> 00:15:22.150
for anyone to learn from their mistakes

00:15:22.150 --> 00:15:24.150
in a field of medicine

00:15:24.150 --> 00:15:27.150
that was considered so discouraging in its prospects.

00:15:27.150 --> 00:15:30.150
And we can also remember

00:15:30.150 --> 00:15:32.150
how the pharmaceutical companies

00:15:32.150 --> 00:15:34.150
were willing to pool their knowledge,

00:15:34.150 --> 00:15:36.150
to share their knowledge,

00:15:36.150 --> 00:15:38.150
in the face of an emergency,

00:15:38.150 --> 00:15:41.150
which they hadn't really been for years and years.

00:15:41.150 --> 00:15:44.150
They might have been able to do it earlier.

00:15:44.150 --> 00:15:47.150
The message, then, for me,

00:15:47.150 --> 00:15:49.150
about unintended consequences

00:15:49.150 --> 00:15:52.150
is chaos happens;

00:15:52.150 --> 00:15:54.150
let's make better use of it.

00:15:54.150 --> 00:15:56.150
Thank you very much.

00:15:56.150 --> 00:16:00.150
(Applause)
