WEBVTT

00:00:12.910 --> 00:00:15.966
So you probably have the sense,
as most people do,

00:00:15.990 --> 00:00:19.646
that polarization
is getting worse in our country,

00:00:19.670 --> 00:00:23.126
that the divide
between the left and the right

00:00:23.150 --> 00:00:26.686
is as bad as it's been
in really any of our lifetimes.

00:00:26.710 --> 00:00:31.990
But you might also reasonably wonder
if research backs up your intuition.

00:00:32.710 --> 00:00:37.390
And in a nutshell,
the answer is sadly yes.

00:00:39.070 --> 00:00:41.086
In study after study, we find

00:00:41.110 --> 00:00:44.790
that liberals and conservatives
have grown further apart.

00:00:45.590 --> 00:00:50.366
They increasingly wall themselves off
in these ideological silos,

00:00:50.390 --> 00:00:54.526
consuming different news,
talking only to like-minded others

00:00:54.550 --> 00:00:57.790
and more and more choosing
to live in different parts of the country.

00:00:58.870 --> 00:01:02.086
And I think that
most alarming of all of it

00:01:02.110 --> 00:01:05.910
is seeing this rising
animosity on both sides.

00:01:06.590 --> 00:01:08.246
Liberals and conservatives,

00:01:08.270 --> 00:01:10.166
Democrats and Republicans,

00:01:10.190 --> 00:01:13.310
more and more they just
don't like one another.

00:01:14.470 --> 00:01:16.486
You see it in many different ways.

00:01:16.510 --> 00:01:20.166
They don't want to befriend one another.
They don't want to date one another.

00:01:20.190 --> 00:01:23.486
If they do, if they find out,
they find each other less attractive,

00:01:23.510 --> 00:01:26.606
and they more and more don't want
their children to marry someone

00:01:26.630 --> 00:01:28.326
who supports the other party,

00:01:28.350 --> 00:01:30.110
a particularly shocking statistic.

00:01:31.790 --> 00:01:34.606
You know, in my lab,
the students that I work with,

00:01:34.630 --> 00:01:38.086
we're talking about
some sort of social pattern --

00:01:38.110 --> 00:01:41.646
I'm a movie buff, and so I'm often like,

00:01:41.670 --> 00:01:44.630
what kind of movie are we in here
with this pattern?

00:01:45.230 --> 00:01:48.510
So what kind of movie are we in
with political polarization?

00:01:49.230 --> 00:01:51.950
Well, it could be a disaster movie.

00:01:53.030 --> 00:01:54.710
It certainly seems like a disaster.

00:01:55.070 --> 00:01:57.070
Could be a war movie.

00:01:57.790 --> 00:01:58.990
Also fits.

00:01:59.630 --> 00:02:03.446
But what I keep thinking is that
we're in a zombie apocalypse movie.

00:02:03.470 --> 00:02:04.926
(Laughter)

00:02:04.950 --> 00:02:07.246
Right? You know the kind.

00:02:07.270 --> 00:02:09.686
There's people wandering around in packs,

00:02:09.710 --> 00:02:11.486
not thinking for themselves,

00:02:11.510 --> 00:02:13.126
seized by this mob mentality

00:02:13.150 --> 00:02:16.390
trying to spread their disease
and destroy society.

00:02:17.630 --> 00:02:19.966
And you probably think, as I do,

00:02:19.990 --> 00:02:23.446
that you're the good guy
in the zombie apocalypse movie,

00:02:23.470 --> 00:02:27.166
and all this hate and polarization,
it's being propagated by the other people,

00:02:27.190 --> 00:02:29.070
because we're Brad Pitt, right?

00:02:29.910 --> 00:02:32.806
Free-thinking, righteous,

00:02:32.830 --> 00:02:35.126
just trying to hold on
to what we hold dear,

00:02:35.150 --> 00:02:38.726
you know, not foot soldiers
in the army of the undead.

00:02:38.750 --> 00:02:40.206
Not that.

00:02:40.230 --> 00:02:41.430
Never that.

00:02:42.230 --> 00:02:43.726
But here's the thing:

00:02:43.750 --> 00:02:46.470
what movie do you suppose
they think they're in?

00:02:47.630 --> 00:02:48.846
Right?

00:02:48.870 --> 00:02:51.406
Well, they absolutely think
that they're the good guys

00:02:51.430 --> 00:02:53.286
in the zombie apocalypse movie. Right?

00:02:53.310 --> 00:02:56.286
And you'd better believe
that they think that they're Brad Pitt

00:02:56.310 --> 00:02:58.430
and that we, we are the zombies.

00:03:01.270 --> 00:03:03.630
And who's to say that they're wrong?

00:03:04.590 --> 00:03:07.710
I think that the truth is
that we're all a part of this.

00:03:08.390 --> 00:03:11.550
And the good side of that
is that we can be a part of the solution.

00:03:12.430 --> 00:03:14.430
So what are we going to do?

00:03:15.470 --> 00:03:19.726
What can we do to chip away
at polarization in everyday life?

00:03:19.750 --> 00:03:23.566
What could we do to connect with
and communicate with

00:03:23.590 --> 00:03:25.310
our political counterparts?

00:03:25.870 --> 00:03:30.006
Well, these were exactly the questions
that I and my colleague, Matt Feinberg,

00:03:30.030 --> 00:03:31.888
became fascinated with a few years ago,

00:03:31.912 --> 00:03:34.112
and we started
doing research on this topic.

00:03:35.070 --> 00:03:38.046
And one of the first things
that we discovered

00:03:38.070 --> 00:03:41.526
that I think is really helpful
for understanding polarization

00:03:41.550 --> 00:03:42.766
is to understand

00:03:42.790 --> 00:03:47.206
that the political divide in our country
is undergirded by a deeper moral divide.

00:03:47.230 --> 00:03:52.006
So one of the most robust findings
in the history of political psychology

00:03:52.030 --> 00:03:55.726
is this pattern identified
by Jon Haidt and Jesse Graham,

00:03:55.750 --> 00:03:56.966
psychologists,

00:03:56.990 --> 00:04:01.006
that liberals and conservatives
tend to endorse different values

00:04:01.030 --> 00:04:02.230
to different degrees.

00:04:02.750 --> 00:04:08.246
So for example, we find that liberals
tend to endorse values like equality

00:04:08.270 --> 00:04:11.926
and fairness and care
and protection from harm

00:04:11.950 --> 00:04:14.086
more than conservatives do.

00:04:14.110 --> 00:04:19.366
And conservatives tend to endorse
values like loyalty, patriotism,

00:04:19.390 --> 00:04:22.846
respect for authority and moral purity

00:04:22.870 --> 00:04:24.950
more than liberals do.

00:04:26.070 --> 00:04:30.126
And Matt and I were thinking
that maybe this moral divide

00:04:30.150 --> 00:04:33.246
might be helpful
for understanding how it is

00:04:33.270 --> 00:04:35.686
that liberals and conservatives
talk to one another

00:04:35.710 --> 00:04:38.126
and why they so often
seem to talk past one another

00:04:38.150 --> 00:04:39.366
when they do.

00:04:39.390 --> 00:04:41.366
So we conducted a study

00:04:41.390 --> 00:04:44.486
where we recruited liberals to a study

00:04:44.510 --> 00:04:46.966
where they were supposed
to write a persuasive essay

00:04:46.990 --> 00:04:51.430
that would be compelling to a conservative
in support of same-sex marriage.

00:04:51.950 --> 00:04:55.206
And what we found was that liberals
tended to make arguments

00:04:55.230 --> 00:04:59.406
in terms of the liberal moral values
of equality and fairness.

00:04:59.430 --> 00:05:01.166
So they said things like,

00:05:01.190 --> 00:05:04.566
"Everyone should have the right
to love whoever they choose,"

00:05:04.590 --> 00:05:07.166
and, "They" -- they being gay Americans --

00:05:07.190 --> 00:05:09.950
"deserve the same equal rights
as other Americans."

00:05:10.510 --> 00:05:13.726
Overall, we found
that 69 percent of liberals

00:05:13.750 --> 00:05:19.166
invoked one of the more liberal
moral values in constructing their essay,

00:05:19.190 --> 00:05:22.886
and only nine percent invoked
one of the more conservative moral values,

00:05:22.910 --> 00:05:26.326
even though they were supposed
to be trying to persuade conservatives.

00:05:26.350 --> 00:05:30.646
And when we studied conservatives
and had them make persuasive arguments

00:05:30.670 --> 00:05:33.566
in support of making English
the official language of the US,

00:05:33.590 --> 00:05:36.126
a classically conservative
political position,

00:05:36.150 --> 00:05:38.366
we found that they weren't
much better at this.

00:05:38.390 --> 00:05:40.006
59 percent of them made arguments

00:05:40.030 --> 00:05:42.726
in terms of one of the more
conservative moral values,

00:05:42.750 --> 00:05:45.246
and just eight percent
invoked a liberal moral value,

00:05:45.270 --> 00:05:48.630
even though they were supposed
to be targeting liberals for persuasion.

00:05:49.630 --> 00:05:53.670
Now, you can see right away
why we're in trouble here. Right?

00:05:54.430 --> 00:05:57.926
People's moral values,
they're their most deeply held beliefs.

00:05:57.950 --> 00:06:01.350
People are willing
to fight and die for their values.

00:06:01.870 --> 00:06:04.566
Why are they going to give that up
just to agree with you

00:06:04.590 --> 00:06:08.126
on something that they don't particularly
want to agree with you on anyway?

00:06:08.150 --> 00:06:11.406
If that persuasive appeal that
you're making to your Republican uncle

00:06:11.430 --> 00:06:13.846
means that he doesn't
just have to change his view,

00:06:13.870 --> 00:06:16.036
he's got to change
his underlying values, too,

00:06:16.060 --> 00:06:17.620
that's not going to go very far.

00:06:18.230 --> 00:06:19.550
So what would work better?

00:06:20.350 --> 00:06:24.646
Well, we believe it's a technique
that we call moral reframing,

00:06:24.670 --> 00:06:27.286
and we've studied it
in a series of experiments.

00:06:27.310 --> 00:06:28.806
In one of these experiments,

00:06:28.830 --> 00:06:31.966
we recruited liberals
and conservatives to a study

00:06:31.990 --> 00:06:34.286
where they read one of three essays

00:06:34.310 --> 00:06:37.350
before having their environmental
attitudes surveyed.

00:06:37.790 --> 00:06:39.286
And the first of these essays

00:06:39.310 --> 00:06:42.686
was a relatively conventional
pro-environmental essay

00:06:42.710 --> 00:06:46.726
that invoked the liberal values
of care and protection from harm.

00:06:46.750 --> 00:06:49.286
It said things like,
"In many important ways

00:06:49.310 --> 00:06:52.126
we are causing real harm
to the places we live in,"

00:06:52.150 --> 00:06:54.966
and, "It is essential
that we take steps now

00:06:54.990 --> 00:06:57.910
to prevent further destruction
from being done to our Earth."

00:06:59.270 --> 00:07:00.686
Another group of participants

00:07:00.710 --> 00:07:02.926
were assigned to read
a really different essay

00:07:02.950 --> 00:07:07.390
that was designed to tap into
the conservative value of moral purity.

00:07:08.340 --> 00:07:10.326
It was a pro-environmental essay as well,

00:07:10.350 --> 00:07:11.846
and it said things like,

00:07:11.870 --> 00:07:16.110
"Keeping our forests, drinking water,
and skies pure is of vital importance."

00:07:17.150 --> 00:07:18.646
"We should regard the pollution

00:07:18.670 --> 00:07:20.710
of the places we live in
to be disgusting."

00:07:21.310 --> 00:07:23.406
And, "Reducing pollution
can help us preserve

00:07:23.430 --> 00:07:26.590
what is pure and beautiful
about the places we live."

00:07:28.030 --> 00:07:29.446
And then we had a third group

00:07:29.470 --> 00:07:31.966
that were assigned
to read just a nonpolitical essay.

00:07:31.990 --> 00:07:34.726
It was just a comparison group
so we could get a baseline.

00:07:34.750 --> 00:07:36.703
And what we found when we surveyed people

00:07:36.727 --> 00:07:38.926
about their environmental
attitudes afterwards,

00:07:38.950 --> 00:07:41.886
we found that liberals,
it didn't matter what essay they read.

00:07:41.910 --> 00:07:45.006
They tended to have highly
pro-environmental attitudes regardless.

00:07:45.030 --> 00:07:47.446
Liberals are on board
for environmental protection.

00:07:47.470 --> 00:07:48.686
Conservatives, however,

00:07:48.710 --> 00:07:53.126
were significantly more supportive
of progressive environmental policies

00:07:53.150 --> 00:07:54.686
and environmental protection

00:07:54.710 --> 00:07:56.766
if they had read the moral purity essay

00:07:56.790 --> 00:07:59.190
than if they read
one of the other two essays.

00:08:00.310 --> 00:08:03.406
We even found that conservatives
who read the moral purity essay

00:08:03.430 --> 00:08:06.926
were significantly more likely to say
that they believed in global warming

00:08:06.950 --> 00:08:08.855
and were concerned about global warming,

00:08:08.879 --> 00:08:11.606
even though this essay
didn't even mention global warming.

00:08:11.630 --> 00:08:14.086
That's just a related environmental issue.

00:08:14.110 --> 00:08:17.190
But that's how robust
this moral reframing effect was.

00:08:18.110 --> 00:08:21.846
And we've studied this on a whole slew
of different political issues.

00:08:21.870 --> 00:08:25.606
So if you want to move conservatives

00:08:25.630 --> 00:08:28.726
on issues like same-sex marriage
or national health insurance,

00:08:28.750 --> 00:08:32.206
it helps to tie these liberal
political issues to conservative values

00:08:32.230 --> 00:08:35.030
like patriotism and moral purity.

00:08:35.950 --> 00:08:38.046
And we studied it the other way, too.

00:08:38.070 --> 00:08:41.886
If you want to move liberals
to the right on conservative policy issues

00:08:41.910 --> 00:08:46.526
like military spending and making English
the official language of the US,

00:08:46.550 --> 00:08:48.206
you're going to be more persuasive

00:08:48.230 --> 00:08:51.566
if you tie those conservative
policy issues to liberal moral values

00:08:51.590 --> 00:08:53.470
like equality and fairness.

00:08:54.790 --> 00:08:57.646
All these studies
have the same clear message:

00:08:57.670 --> 00:09:00.606
if you want to persuade
someone on some policy,

00:09:00.630 --> 00:09:04.470
it's helpful to connect that policy
to their underlying moral values.

00:09:05.670 --> 00:09:07.846
And when you say it like that

00:09:07.870 --> 00:09:09.366
it seems really obvious. Right?

00:09:09.390 --> 00:09:11.166
Like, why did we come here tonight?

00:09:11.190 --> 00:09:12.406
Why --

00:09:12.430 --> 00:09:13.966
(Laughter)

00:09:13.990 --> 00:09:16.030
It's incredibly intuitive.

00:09:17.550 --> 00:09:20.846
And even though it is,
it's something we really struggle to do.

00:09:20.870 --> 00:09:24.726
You know, it turns out that when we go
to persuade somebody on a political issue,

00:09:24.750 --> 00:09:27.486
we talk like we're speaking into a mirror.

00:09:27.510 --> 00:09:31.886
We don't persuade so much
as we rehearse our own reasons

00:09:31.910 --> 00:09:34.790
for why we believe
some sort of political position.

00:09:35.550 --> 00:09:39.966
We kept saying when we were designing
these reframed moral arguments,

00:09:39.990 --> 00:09:42.630
"Empathy and respect,
empathy and respect."

00:09:43.190 --> 00:09:44.646
If you can tap into that,

00:09:44.670 --> 00:09:46.326
you can connect

00:09:46.350 --> 00:09:49.150
and you might be able to persuade
somebody in this country.

00:09:49.710 --> 00:09:52.126
So thinking again

00:09:52.150 --> 00:09:54.430
about what movie we're in,

00:09:55.350 --> 00:09:56.926
maybe I got carried away before.

00:09:56.950 --> 00:09:58.910
Maybe it's not a zombie apocalypse movie.

00:09:59.670 --> 00:10:01.590
Maybe instead it's a buddy cop movie.

00:10:02.190 --> 00:10:04.206
(Laughter)

00:10:04.230 --> 00:10:06.246
Just roll with it, just go with it please.

00:10:06.270 --> 00:10:07.710
(Laughter)

00:10:08.630 --> 00:10:11.326
You know the kind:
there's a white cop and a black cop,

00:10:11.350 --> 00:10:13.486
or maybe a messy cop and an organized cop.

00:10:13.510 --> 00:10:15.566
Whatever it is, they don't get along

00:10:15.590 --> 00:10:16.876
because of this difference.

00:10:17.670 --> 00:10:20.886
But in the end, when they have
to come together and they cooperate,

00:10:20.910 --> 00:10:22.846
the solidarity that they feel,

00:10:22.870 --> 00:10:26.510
it's greater because of that gulf
that they had to cross. Right?

00:10:27.430 --> 00:10:29.406
And remember that in these movies,

00:10:29.430 --> 00:10:32.326
it's usually worst in the second act

00:10:32.350 --> 00:10:34.750
when our leads are further apart
than ever before.

00:10:35.590 --> 00:10:37.926
And so maybe that's
where we are in this country,

00:10:37.950 --> 00:10:40.126
late in the second act
of a buddy cop movie --

00:10:40.150 --> 00:10:42.726
(Laughter)

00:10:42.750 --> 00:10:45.830
torn apart but about
to come back together.

00:10:47.550 --> 00:10:49.206
It sounds good,

00:10:49.230 --> 00:10:51.086
but if we want it to happen,

00:10:51.110 --> 00:10:53.830
I think the responsibility
is going to start with us.

00:10:54.670 --> 00:10:56.830
So this is my call to you:

00:10:57.630 --> 00:10:59.630
let's put this country back together.

00:11:01.230 --> 00:11:04.286
Let's do it despite the politicians

00:11:04.310 --> 00:11:07.166
and the media and Facebook and Twitter

00:11:07.190 --> 00:11:08.726
and Congressional redistricting

00:11:08.750 --> 00:11:11.470
and all of it,
all the things that divide us.

00:11:12.510 --> 00:11:14.750
Let's do it because it's right.

00:11:16.070 --> 00:11:20.486
And let's do it
because this hate and contempt

00:11:20.510 --> 00:11:22.670
that flows through all of us every day

00:11:23.550 --> 00:11:26.726
makes us ugly and it corrupts us,

00:11:26.750 --> 00:11:30.070
and it threatens
the very fabric of our society.

00:11:32.110 --> 00:11:34.766
We owe it to one another and our country

00:11:34.790 --> 00:11:36.950
to reach out and try to connect.

00:11:38.150 --> 00:11:41.310
We can't afford to hate them any longer,

00:11:42.350 --> 00:11:44.550
and we can't afford
to let them hate us either.

00:11:46.030 --> 00:11:47.390
Empathy and respect.

00:11:48.030 --> 00:11:49.270
Empathy and respect.

00:11:50.070 --> 00:11:53.870
If you think about it, it's the very least
that we owe our fellow citizens.

00:11:54.550 --> 00:11:55.766
Thank you.

00:11:55.790 --> 00:12:00.475
(Applause)
