WEBVTT

00:00:01.000 --> 00:00:03.216
سأقوم بالتحدث عن فشل الحدس

00:00:03.240 --> 00:00:04.840
الذي يعاني معظمنا منه.

00:00:05.480 --> 00:00:08.520
هو في الواقع الفشل في تحرِّي نوع محدد
من الخطر.

00:00:09.360 --> 00:00:11.096
سوف أقوم بوصف سيناريو

00:00:11.120 --> 00:00:14.376
أظنه مخيفاً

00:00:14.400 --> 00:00:16.160
ومن المرجح أن يحدث،

00:00:16.840 --> 00:00:18.496
وهذا ليس مزيجاً جيداً،

00:00:18.520 --> 00:00:20.056
كما سيبدو.

00:00:20.080 --> 00:00:22.536
ومع ذلك، بدلاً من الخوف، سيشعر معظمكم

00:00:22.560 --> 00:00:24.640
بأن ما أتحدُّث عنه مثير نوعاً ما.

00:00:25.200 --> 00:00:28.176
سأقوم بوصف كيف أن المكاسب التي نحصل عليها

00:00:28.200 --> 00:00:29.976
من الذكاء الاصطناعي

00:00:30.000 --> 00:00:31.776
يمكن في النهاية أن تدمرنا.

00:00:31.800 --> 00:00:35.256
وفي الواقع، أظنه من الصعب جداً أن نرى
كيف أنها لن تدمرنا

00:00:35.280 --> 00:00:36.960
أو تلهمنا أن ندمر أنفسنا.

00:00:37.400 --> 00:00:39.256
ومع ذلك إذا كنتم تشبهونني في شيء

00:00:39.280 --> 00:00:41.936
فستجدون أنه من الممتع التفكير
بهذه الأشياء.

00:00:41.960 --> 00:00:45.336
وتلك الإجابة هي جزء من المشكلة.

00:00:45.360 --> 00:00:47.080
حسناً؟ تلك الإجابة يجب أن تُقلقكم.

00:00:47.920 --> 00:00:50.576
وإذا كان عليَّ أن أقنعكم في هذه المحادثة

00:00:50.600 --> 00:00:54.016
أنه من المرجح أننا سوف نعاني من
مجاعة عالمية

00:00:54.040 --> 00:00:57.096
إما بسبب تغيُّر المناخ، أو بسبب
كارثة أخرى،

00:00:57.120 --> 00:01:00.536
وأن أحفادكم، أو أحفادهم،

00:01:00.560 --> 00:01:02.360
من المرجح جداً أن يعيشوا، هكذا

00:01:03.200 --> 00:01:04.400
فلن يخطر ببالكم.

00:01:05.440 --> 00:01:06.776
"مثير للاهتمام".

00:01:06.800 --> 00:01:08.000
تعجبني محادثة TED هذه.

00:01:09.200 --> 00:01:10.720
المجاعة ليست مسلية.

00:01:11.800 --> 00:01:15.176
الموت عن طريق الخيال العلمي، من ناحية
أخرى، مسلٍّ،

00:01:15.200 --> 00:01:19.176
وأحد الأشياء التي تقلقني للغاية بسبب تطور
الذكاء الاصطناعي عند هذه النقطة

00:01:19.200 --> 00:01:23.296
هي أننا نبدو وكأننا غير قادرين على إبداء
استجابة عاطفية ملائمة

00:01:23.320 --> 00:01:25.136
للأخطار التي تنتظرنا.

00:01:25.160 --> 00:01:28.360
أنا غير قادر على إبداء هذه الاستجابة،
وأنا أقوم بهذه المحادثة.

00:01:30.120 --> 00:01:32.816
يبدو الأمر وكأننا نقف أمام بابين.

00:01:32.840 --> 00:01:34.096
خلف الباب رقم واحد،

00:01:34.120 --> 00:01:37.416
نتوقف عن تحقيق التقدُّم في بناء الآليات
الذكية.

00:01:37.440 --> 00:01:41.456
معدات حاسوبنا وبرمجياته تتوقف عن التحسن
لسببٍ ما وحسب.

00:01:41.480 --> 00:01:44.480
خذوا الآن لحظة للتفكير لماذا قد يحدث ذلك.

00:01:45.080 --> 00:01:48.736
أعني، بافتراض مدى نفع الذكاء والأتمتة،

00:01:48.760 --> 00:01:52.280
فإننا سنستمر بتحسين التكنولوجيا الخاصة بنا
إذا كان بإمكاننا أصلاً.

00:01:53.200 --> 00:01:54.867
ما الذي قد يمنعنا من فعل هذا؟

00:01:55.800 --> 00:01:57.600
حربٌ نووية على جميع المقاييس؟

00:01:59.000 --> 00:02:00.560
وباء عالمي؟

00:02:02.320 --> 00:02:03.640
اصطدام كويكب؟

00:02:05.640 --> 00:02:08.216
أن يصبح جاستن بيبر رئيساً
للولايات المتحدة؟

00:02:08.240 --> 00:02:10.520
(ضحك)

00:02:12.760 --> 00:02:16.680
الخلاصة هي، شيءٌ ما عليه تدمير
الحضارة كما نعرفها.

00:02:17.360 --> 00:02:21.656
عليكم تخيُّل كم عليه أن يكون سيئاً

00:02:21.680 --> 00:02:25.016
لكي يمنعنا من تحقيق تحسينات في
التكنولوجيا الخاصة بنا

00:02:25.040 --> 00:02:26.256
بشكل دائم،

00:02:26.280 --> 00:02:28.296
جيلاً بعد جيل.

00:02:28.320 --> 00:02:30.456
تقريباً بالتعريف، هذا أسوأ شيء

00:02:30.480 --> 00:02:32.496
يمكن أن يحصل في تاريخ البشرية.

00:02:32.520 --> 00:02:33.816
لذا البديل الوحيد،

00:02:33.840 --> 00:02:36.176
وهو الذي يوجد خلف الباب رقم اثنين،

00:02:36.200 --> 00:02:39.336
هو أن نستمر بتحسين آلاتنا الذكية

00:02:39.360 --> 00:02:40.960
سنة بعد سنة بعد سنة.

00:02:41.720 --> 00:02:45.360
وعند نقطة معينة، سنبني آلياتٍ أذكى
مما نحن عليه،

00:02:46.080 --> 00:02:48.696
وحالما يصبح لدينا آليات أذكى مننا،

00:02:48.720 --> 00:02:50.696
سوف تبدأ بتحسين نفسها.

00:02:50.720 --> 00:02:53.456
وعندها سنواجه ما دعاه عالم 
الرياضيات IJ Good

00:02:53.480 --> 00:02:55.256
"انفجاراً ذكائياً"

00:02:55.280 --> 00:02:57.280
وأن العملية يمكن أن تفلت من بين أيدينا.

00:02:58.120 --> 00:03:00.936
الآن، هذا يتم عمله كرسم كاريكاتيري، 
كما أفعل هنا

00:03:00.960 --> 00:03:04.176
كخوفٍ من جيوش ميليشيا من الرجال الآليين

00:03:04.200 --> 00:03:05.456
التي ستهاجمنا.

00:03:05.480 --> 00:03:08.176
ولكن هذا ليس السيناريو الأكثر احتمالاً.

00:03:08.200 --> 00:03:13.056
الأمر ليس أن آلياتنا ستصبح حاقدة بشكل عفوي

00:03:13.080 --> 00:03:15.696
المقلق حقاً هو أننا نبني آليات

00:03:15.720 --> 00:03:17.776
أكثر كفاءة مما نحن عليه

00:03:17.800 --> 00:03:21.576
وأن أقل اختلاف بين أهدافها وأهدافنا

00:03:21.600 --> 00:03:22.800
يمكن أن يدمرنا.

00:03:23.960 --> 00:03:26.040
فقط فكروا بكيفية علاقتنا بالنمل

00:03:26.600 --> 00:03:28.256
نحن لا نكرههم.

00:03:28.280 --> 00:03:30.336
ولا نخرج عن طريقنا لنؤذيهم.

00:03:30.360 --> 00:03:32.736
في الواقع، أحياناً نتحمل آلاماً لكي
لا نؤذيهم.

00:03:32.760 --> 00:03:34.776
نحن نمشي من فوقهم على الرصيف.

00:03:34.800 --> 00:03:36.936
ولكن عندما يكون لوجودهم

00:03:36.960 --> 00:03:39.456
تعارض جدِّي مع أحد أهدافنا،

00:03:39.480 --> 00:03:41.957
لنقل مثلاً عند إنشاء بناء كهذا،

00:03:41.981 --> 00:03:43.941
نقوم بإبادتهم بدون أي تأنيب ضمير.

00:03:44.480 --> 00:03:47.416
والقلق من أننا يوماً ما سوف نبني آليات

00:03:47.440 --> 00:03:50.176
والتي، سواءٌ كانت مدركة أم لا،

00:03:50.200 --> 00:03:52.200
قد تعاملنا بمثل هذا الاستخفاف.

00:03:53.760 --> 00:03:56.520
والآن، أنا أشك أن هذا أبعد ما يكون
عن العديد منكم.

00:03:57.360 --> 00:04:03.696
وأراهن على أن هنالك منكم من يشك أن الذكاء
الاصطناعي الخارق الذكاء ممكن،

00:04:03.720 --> 00:04:05.376
أو حتى محتوم.

00:04:05.400 --> 00:04:09.020
ولكن مع ذلك يجب عليكم أن تجدوا شيئاً ما 
خاطئاً في أحد الافتراضات التالية.

00:04:09.044 --> 00:04:10.616
وهنالك ثلاثة منهم.

00:04:11.800 --> 00:04:16.519
الذكاء هو مسألة معالجة للمعلومات في
الأنظمة الفيزيائية.

00:04:17.320 --> 00:04:19.935
في الواقع، هذا أكثر بقليل من مجرد افتراض.

00:04:19.959 --> 00:04:23.416
نحن قمنا مسبقاً ببناء ذكاء محدود
في آلياتنا،

00:04:23.440 --> 00:04:25.456
والعديد من هذه الآليات تقوم بالأداء

00:04:25.480 --> 00:04:28.120
في مستوى الذكاء البشري الخارق أصلاً.

00:04:28.840 --> 00:04:31.416
ونعلم أن المادة المجردة

00:04:31.440 --> 00:04:34.056
بإمكانها أن تعطي زيادة لما يدعى
"بالذكاء العام"،

00:04:34.080 --> 00:04:37.736
القدرة على التفكير بمرونة في عدة مجالات،

00:04:37.760 --> 00:04:40.896
لأن دماغنا قام بتدبر الأمر. صحيح؟

00:04:40.920 --> 00:04:44.856
أنا أعني، هناك ذرات فقط هنا،

00:04:44.880 --> 00:04:49.376
وطالما نستمر ببناء أنظمة من الذرات

00:04:49.400 --> 00:04:52.096
والتي تُبدي سلوكاً متزايداً أكثر
وأكثر من الذكاء

00:04:52.120 --> 00:04:54.656
فإننا في النهاية، إلا إذا تمَّت مقاطعتنا،

00:04:54.680 --> 00:04:58.056
في النهاية سوف نبني ذكاءً عاماً

00:04:58.080 --> 00:04:59.376
في آلياتنا.

00:04:59.400 --> 00:05:03.056
من المصيري أن ندرك أن معدل التطور لا يهم،

00:05:03.080 --> 00:05:06.256
لأن أي تطور هو كافٍ لإيصالنا إلى 
منطقة النهاية.

00:05:06.280 --> 00:05:10.056
لا نحتاج قانون Moore للاستمرار.
لا نحتاج التطور الأسي.

00:05:10.080 --> 00:05:11.680
نحتاج فقط لأن نستمر بالمضي قدماً

00:05:13.480 --> 00:05:16.400
الافتراض الثاني هو أننا سنستمر
بالمضي قدماً.

00:05:17.000 --> 00:05:19.760
سنستمر بتحسين آلياتنا الذكية.

00:05:21.000 --> 00:05:25.376
وبافتراض أهمية الذكاء --

00:05:25.400 --> 00:05:28.936
أنا أعني، أن الذكاء هو إما مصدر 
لكل شيءٍ نقدِّره

00:05:28.960 --> 00:05:31.736
أو أننا نحتاجه لحماية كل شيءٍ نقدُّره.

00:05:31.760 --> 00:05:34.016
إنه المنبع الأكثر أهمية لدينا.

00:05:34.040 --> 00:05:35.576
لذلك نحتاج لفعل التالي.

00:05:35.600 --> 00:05:38.936
لدينا مشاكل نحن بحاجة ماسِّة لحلها.

00:05:38.960 --> 00:05:42.160
نريد شفاء الأمراض كالزهايمر والسرطان.

00:05:42.960 --> 00:05:46.896
نريد أن نفهم الأنظمة الاقتصادية.
نريد تحسين علم المناخ الخاص بنا.

00:05:46.920 --> 00:05:49.176
لذلك سنفعل ذلك، إذا استطعنا.

00:05:49.200 --> 00:05:52.486
القطار غادر المحطة بالفعل، ولا
يوجد مكابح لشدها.

00:05:53.880 --> 00:05:59.336
وأخيراً، نحن لا نقف عند قمة الذكاء،

00:05:59.360 --> 00:06:01.160
أو في أي مكان قريب منها، على ما يبدو،

00:06:01.640 --> 00:06:03.536
وهذا حقاً هو البصيرة المصيرية.

00:06:03.560 --> 00:06:05.976
هذا ما يجعل موقفنا متزعزعاً للغاية،

00:06:06.000 --> 00:06:10.040
وهذا ما يجعل حدسنا بما يخص
الخطورة لا يُعتمد عليه أبداً.

00:06:11.120 --> 00:06:13.840
والآن، فكروا فقط بأذكى شخص
عاش في أي وقتٍ مضى.

00:06:14.640 --> 00:06:18.056
تقريباً على قائمة الجميع القصيرة 
يوجد John von Neumann.

00:06:18.080 --> 00:06:21.416
أعني، الانطباع الذي تركه von Neumann 
على الناس المحيطين به،

00:06:21.440 --> 00:06:25.496
وهذا يشمل أعظم علماء الرياضيات 
والفيزياء في عصره،

00:06:25.520 --> 00:06:27.456
بالكاد تم توثيقه بشكل جيد.

00:06:27.480 --> 00:06:31.256
إذا كانت نصف القصص فقط عنه نصف صحيحة،

00:06:31.280 --> 00:06:32.496
فلا يوجد شك

00:06:32.520 --> 00:06:34.976
أنه أحد أذكى الأشخاص الذين عاشوا
في أي وقتٍ مضى.

00:06:35.000 --> 00:06:37.520
لذلك باعتبار طيف الذكاء.

00:06:38.320 --> 00:06:39.749
هنا لدينا John von Neumann.

00:06:41.560 --> 00:06:42.894
وهنا لدينا أنتم وأنا.

00:06:44.120 --> 00:06:45.416
وثم هنا لدينا دجاجة.

00:06:45.440 --> 00:06:47.376
(ضحك)

00:06:47.400 --> 00:06:48.616
آسف، دجاجة.

00:06:48.640 --> 00:06:49.896
(ضحك)

00:06:49.920 --> 00:06:53.656
لا يوجد سبب يجعلني أجعل هذه المحادثة أكثر
إحباطاً مما يحتاجه الأمر.

00:06:53.680 --> 00:06:55.280
(ضحك)

00:06:56.339 --> 00:06:59.816
يبدو أنه ساحق على الأرجح، ومع ذلك، يمتد
طيف الذكاء

00:06:59.840 --> 00:07:02.960
أبعد بكثير مما نتصور حالياً،

00:07:03.880 --> 00:07:07.096
وإذا قمنا ببناء آلات أذكى مما نحن عليه،

00:07:07.120 --> 00:07:09.416
فهي على الأرجح ستستكشف الطيف

00:07:09.440 --> 00:07:11.296
بطرقٍ لا نستطيع تخيلها،

00:07:11.320 --> 00:07:13.840
وسف تتعدانا بطرقٍ لا يمكننا تخيلها.

00:07:15.000 --> 00:07:19.336
ومن المهم أن ندرك أن هذا صحيح بخصوص 
تأثير السرعة وحدها.

00:07:19.360 --> 00:07:24.416
صحيح؟ إذاً تخيلوا إذا بنينا ذكاءً صنعياً 
خارق الذكاء

00:07:24.440 --> 00:07:27.896
ولم يكن أذكى من فريق الباحثين العادي
الخاص بكم

00:07:27.920 --> 00:07:30.216
في ستانفورد أو معهد ماساتشوستس
للتكنولوجيا.

00:07:30.240 --> 00:07:33.216
حسناً، الدارات الالكترونية تعمل بشكل أسرع 
بحوالي ملايين المرات

00:07:33.240 --> 00:07:34.496
من تلك الكيميائية الحيوية،

00:07:34.520 --> 00:07:37.656
لذا هذه الآلة يجب أن تفكر أسرع بملايين
المرات

00:07:37.680 --> 00:07:39.496
من العقول التي قامت ببنائها.

00:07:39.520 --> 00:07:41.176
لذا إذا تركتها تعمل لمدة أسبوع،

00:07:41.200 --> 00:07:45.760
فسوف تؤدي ما يعادل 20,000 سنة من
العمل بمستوى الذكاء البشري،

00:07:46.400 --> 00:07:48.360
أسبوع بعد أسبوع بعد أسبوع.

00:07:49.640 --> 00:07:52.736
كيف بإمكاننا حتى أن نفهم،
أو حتى أن نُقيّد،

00:07:52.760 --> 00:07:55.040
عقلاً يصنع هذا النوع من التطور؟

00:07:56.840 --> 00:07:58.976
والأمر الآخر المقلق، بصراحة،

00:07:59.000 --> 00:08:03.976
هو أن، تخيل أفضل سيناريو.

00:08:04.000 --> 00:08:08.176
إذاً تخيلوا أننا اكتشفنا تصميماً للذكاء
الصنعي فائق الذكاء

00:08:08.200 --> 00:08:09.576
وليس لديه مخاوف سلامة.

00:08:09.600 --> 00:08:12.856
لدينا التصميم المثالي للمرة الأولى.

00:08:12.880 --> 00:08:15.096
وكأنه تم تسليمنا نبوءة

00:08:15.120 --> 00:08:17.136
تتصرَّف تماماً كما ينبغي عليها.

00:08:17.160 --> 00:08:20.880
هذه الآلة ستكون المثلى لتوفير العمالة.

00:08:21.680 --> 00:08:24.109
بإمكانها تصميم آلية ستقوم ببناء الآلة

00:08:24.133 --> 00:08:25.896
التي بإمكانها القيام بأي عمل فيزيائي،

00:08:25.920 --> 00:08:27.376
تعمل بضوء الشمس،

00:08:27.400 --> 00:08:30.096
ومهما يكن من أجل كلفة المواد الخام.

00:08:30.120 --> 00:08:33.376
إذاً نحن نتكلم عن نهاية الكدح البشري.

00:08:33.400 --> 00:08:36.200
نحن أيضا نتكلم عن نهاية العمل الذكائي.

00:08:37.200 --> 00:08:40.256
فماذا قد تفعل القرود أمثالنا في هذا الظرف؟

00:08:40.280 --> 00:08:44.360
حسناً، سنكون متفرغين للعب بالصحن الطائر 
وتبادل الرسائل مع بعضنا.

00:08:45.840 --> 00:08:48.696
أضف بعض الـLSD
مع اختيارات اللباس المشكوك بأمرها،

00:08:48.720 --> 00:08:50.896
وبإمكان العالم كله أن يصبح
كفيلم Burning Man.

00:08:50.920 --> 00:08:52.560
(ضحك)

00:08:54.320 --> 00:08:56.320
حسناً، هذا قد يبدو جيداً جداً،

00:08:57.280 --> 00:08:59.656
ولكن اسألوا أنفسكم ماذا قد يسبب هذا

00:08:59.680 --> 00:09:02.416
لاقتصادنا الحالي ووضعنا السياسي؟

00:09:02.440 --> 00:09:04.856
يبدو أننا سنشهد على الأرجح

00:09:04.880 --> 00:09:09.016
مستوى من تفاوت الثروات والبطالة

00:09:09.040 --> 00:09:10.536
الذي لم نشهده أبداً من قبل

00:09:10.560 --> 00:09:13.176
غياب الرغبة لوضع هذه الثروة الجديدة فوراً

00:09:13.200 --> 00:09:14.680
في خدمة جميع البشرية،

00:09:15.640 --> 00:09:19.256
بعض أصحاب التريليونات سوف يُشرِّفون
أغلفة مجلات أعمالنا

00:09:19.280 --> 00:09:21.720
بينما يكون باقي العالم متفرغاً لكي 
يتضور جوعاً.

00:09:22.320 --> 00:09:24.616
وماذا سيفعل الروس أو الصينيون

00:09:24.640 --> 00:09:27.256
إذا سمعوا أن شركةً ما في Silicon Valley

00:09:27.280 --> 00:09:30.016
كانت على وشك نشر الذكاء الا 
فائق الذكاء؟

00:09:30.040 --> 00:09:32.896
هذه الآلة ستكون قادرة على شن حرب،

00:09:32.920 --> 00:09:35.136
سواء كانت على الأرض أو على الإنترنت،

00:09:35.160 --> 00:09:36.840
بقوة غير مسبوقة.

00:09:38.120 --> 00:09:39.976
هذا السيناريو هو الذي يربح كل شيء

00:09:40.000 --> 00:09:43.136
أن تكون متقدماً بستة أشهر على
المنافسة في هذه الحالة

00:09:43.160 --> 00:09:45.936
تعني أن تكون متقدماً بـ500,000 سنة،

00:09:45.960 --> 00:09:47.456
بأقل حد.

00:09:47.480 --> 00:09:52.216
وبالتالي يبدو أنه حتى الإشاعات المحضة 
بما يخص النوع من الاكتشافات

00:09:52.240 --> 00:09:54.616
يمكن أن تجعل فصائلنا تهتاج.

00:09:54.640 --> 00:09:57.536
والآن، أكثر الأشياء إخافة،

00:09:57.560 --> 00:10:00.336
من وجهة نظري، في هذه اللحظة،

00:10:00.360 --> 00:10:04.656
هي النوع من الأشياء التي يقولها باحثو
الذكاء الاصطناعي عندما

00:10:04.680 --> 00:10:06.240
يريدون أن يكونوا مُطمئِنين.

00:10:07.000 --> 00:10:10.456
والسبب الأكثر شيوعاً الذي يُقال لنا
ألا نقلق بسببه هو الوقت

00:10:10.480 --> 00:10:12.536
هذا كله سيمر، ألا تعلم.

00:10:12.560 --> 00:10:15.000
هذا من المحتمل بعد 50 أو 100 سنة.

00:10:15.720 --> 00:10:16.976
قال أحد الباحثين،

00:10:17.000 --> 00:10:18.576
"القلق بشأن سلامة
الذكاء الاصطناعي

00:10:18.600 --> 00:10:20.880
هو كالقلق بشأن زيادة
التعداد السكاني على المريخ"

00:10:22.116 --> 00:10:23.736
هذه نسخة Silicon Valley

00:10:23.760 --> 00:10:26.136
من "لا تُقلق رأسك الجميل الصغير بهذا".

00:10:26.160 --> 00:10:27.496
(ضحك)

00:10:27.520 --> 00:10:29.416
لا يبدو أن أحداً يلاحظ

00:10:29.440 --> 00:10:32.056
أن الإشارة للحد الزمني

00:10:32.080 --> 00:10:34.656
ليس له عواقب بتاتاً.

00:10:34.680 --> 00:10:37.936
إذا كان الذكاء هو مجرد مسألة معالجة
معلومات،

00:10:37.960 --> 00:10:40.616
ونحن نستمر بتحسين آلياتنا،

00:10:40.640 --> 00:10:43.520
فسنقوم بإنتاج شكلٍ ما من الذكاء الفائق.

00:10:44.320 --> 00:10:47.976
وليس لدينا فكرة كم سيستغرق هذا مننا

00:10:48.000 --> 00:10:50.400
لخلق الشروط للقيام بذلك بأمان.

00:10:52.200 --> 00:10:53.496
دعوني أقل ذلك مرة أخرى.

00:10:53.520 --> 00:10:57.336
ليس لدينا فكرة كم سيستغرق الأمر مننا

00:10:57.360 --> 00:10:59.600
لخلق الشروط للقيام بذلك بأمان.

00:11:00.920 --> 00:11:04.376
وإذا لم تلاحظوا،
لم تعد الخمسون سنة كما كانت عليه.

00:11:04.400 --> 00:11:06.856
هذه 50 سنة في أشهر.

00:11:06.880 --> 00:11:08.720
هذه المدة التي امتلكنا فيها الـiPhone

00:11:09.440 --> 00:11:12.040
هذه المدة التي انعرض فيها
The Simpsons على التلفاز

00:11:12.680 --> 00:11:15.056
خمسون سنة هي ليست بالوقت الكثير

00:11:15.080 --> 00:11:18.240
لمواجهة أحد أكبر التحديات الذي سيواجه
لفصيلتنا.

00:11:19.640 --> 00:11:23.656
مرة أخرى، يبدو أننا نفشل في امتلاك
الاستجابة العاطفية الملائمة

00:11:23.680 --> 00:11:26.376
لما يوجد لدينا أكثر من سبب للإيمكان
بأنه قادم

00:11:26.400 --> 00:11:30.376
عالم الحاسوب Stuart Russell
لديه تحليل لطيف هنا.

00:11:30.400 --> 00:11:35.296
هو يقول، تخيلوا أننا استلمنا رسالة من
حضارة الفضائيين،

00:11:35.320 --> 00:11:37.016
والتي كانت:

00:11:37.040 --> 00:11:38.576
"سكان الأرض،

00:11:38.600 --> 00:11:40.960
سوف نصل إلى كوكبكم خلال 50 سنة.

00:11:41.800 --> 00:11:43.376
استعدوا."

00:11:43.400 --> 00:11:47.656
والآن نقوم بالعد التنازلي للأشهر حتى تهبط
السفينة الأم؟

00:11:47.680 --> 00:11:50.680
سنشعر بالعجلة أكثر مما نفعل الآن بقليل.

00:11:52.680 --> 00:11:54.536
يُقال لنا ألا نقلق لسببٍ آخر

00:11:54.560 --> 00:11:57.576
هو أن هذه الآليات ليس بوسعها
إلا مشاركتنا منافعها

00:11:57.600 --> 00:12:00.216
لأنها ستكون حرفياً امتداداً لأنفسنا.

00:12:00.240 --> 00:12:02.056
ستكون مُطعَّمة مع أدمغتنا،

00:12:02.080 --> 00:12:04.440
ونحن سنصبح جوهرياً أنظمتها الحوفية.

00:12:05.120 --> 00:12:06.536
الآن خذوا لحظة للتفكير

00:12:06.560 --> 00:12:09.736
أن الطريق الوحيد الآمن والحكيم للأمام،

00:12:09.760 --> 00:12:11.096
الذي يُنصح به،

00:12:11.120 --> 00:12:13.920
هو زرع هذه التكنولوجيا مباشرةً في أدمغتنا.

00:12:14.600 --> 00:12:17.976
قد يكون الطريق الوحيد
الآمن والحكيم للأمام،

00:12:18.000 --> 00:12:21.056
ولكن عادةً يكون للمرء مخاوف
عن كون التكنولوجيا

00:12:21.080 --> 00:12:24.736
مفهومة لحدٍّ ما قبل أن تدخلها داخل رأسك.

00:12:24.760 --> 00:12:26.776
(ضحك)

00:12:26.800 --> 00:12:32.136
المشكلة الأعمق أن بناء ذكاء اصطناعي فائق
الذكاء بمفرده

00:12:32.160 --> 00:12:33.896
يبدو على الأرجح أنه أسهل

00:12:33.920 --> 00:12:35.776
من بناء ذكاء اصطناعي فائق
الذكاء

00:12:35.800 --> 00:12:37.576
وامتلاك علم الأعصاب الكامل

00:12:37.600 --> 00:12:40.280
الذي يسمح لنا بمكاملة عقولنا معه بسلاسة.

00:12:40.800 --> 00:12:43.976
وبافتراض أن الشركات والحكومات
التي تقوم بهذا العمل

00:12:44.000 --> 00:12:47.656
على الأرجح ستعي كونها في سباق ضد 
الآخرين جميعاً،

00:12:47.680 --> 00:12:50.936
وبافتراض أن الفوز في هذا السباق
هو الفوز بالعالم،

00:12:50.960 --> 00:12:53.416
بفرض أنك لن تدمره في الشهر التالي،

00:12:53.440 --> 00:12:56.056
فبالتالي يبدو على الأرجح أنه مهما يكن
الأسهل فعله

00:12:56.080 --> 00:12:57.280
سيتم الانتهاء منه أولاً.

00:12:58.560 --> 00:13:01.416
الآن، لسوء الحظ، لا أمتلك حلاً لهذه
المشكلة،

00:13:01.440 --> 00:13:04.056
بمعزل عن توصية أن يفكِّر المزيد مننا به.

00:13:04.080 --> 00:13:06.456
أظن أننا بحاجة شيء ما مثل "مشروع مانهاتن"

00:13:06.480 --> 00:13:08.496
لموضوع الذكاء الاصطناعي.

00:13:08.520 --> 00:13:11.256
ليس لبنائه، لأنني أظننا سنفعل ذلك حتماً،

00:13:11.280 --> 00:13:14.616
ولكن لفهم كيفية تجنب سباق الأذرع

00:13:14.640 --> 00:13:18.136
ولبنائه بطريقة منحازة مع اهتماماتنا.

00:13:18.160 --> 00:13:20.296
عندما تتحدث عن ذكاء اصطناعي فائق
الذكاء

00:13:20.320 --> 00:13:22.576
يمكنه إحداث تغيير لنفسه،

00:13:22.600 --> 00:13:27.216
يبدو أن لدينا فرصة واحدة للقيام بالشروط 
الابتدائية بشكل صحيح،

00:13:27.240 --> 00:13:29.296
وحتى حينها سنكون بحاجة امتصاص

00:13:29.320 --> 00:13:32.360
العواقب الاقتصادية والسياسية من القيام
بها بشكل صحيح.

00:13:33.760 --> 00:13:35.816
ولكن في اللحظة التي نعترف فيها

00:13:35.840 --> 00:13:39.840
أن معالجة المعلومات هي مصدر الذكاء،

00:13:40.720 --> 00:13:45.520
وأن بعض أنظمة الحاسوب الملائمة
تُعبِّر عن أساس الذكاء،

00:13:46.360 --> 00:13:50.120
ونعترف أننا سوف نحسِّن هذه الأنظمة 
باستمرار،

00:13:51.280 --> 00:13:55.736
ونعترف أن أفق الإدراك على الأرجح يتجاوز

00:13:55.760 --> 00:13:56.960
الذي نعرفه حالياً بكثير،

00:13:58.120 --> 00:13:59.336
وبالتالي علينا الاعتراف

00:13:59.360 --> 00:14:02.000
أننا في مرحلة بناء نوع ما من الآلهة.

00:14:03.400 --> 00:14:04.976
الآن سيكون وقتاً جيداً

00:14:05.000 --> 00:14:06.953
للتأكد من أنه إله نستطيع العيش معه.

00:14:08.120 --> 00:14:09.656
شكراً جزيلاً لكم.

00:14:09.680 --> 00:14:14.773
(تصفيق)

