WEBVTT

00:00:00.760 --> 00:00:04.600
عندما كنت صغيراً، كنت شغوفاً بالدراسة

00:00:05.320 --> 00:00:07.496
أظن أن بعضاً منكم كان كذلك، أيضاًً.

00:00:07.520 --> 00:00:08.736
(ضحك)

00:00:08.760 --> 00:00:11.976
و أنت يا سيدي، الذي ضحكت بشدة،
من المحتمل أنك مازلت كذلك.

00:00:12.000 --> 00:00:14.256
(ضحك)

00:00:14.280 --> 00:00:17.776
لقد عشت في بلدة صغيرة
في أحد ضواحي نورث تكساس،

00:00:17.800 --> 00:00:21.136
كوني ولداً لضابط في الشرطة وهو حفيد لقس.

00:00:21.160 --> 00:00:23.080
فإن الوقوع في المشاكل لم يكن من خياراتي.

00:00:24.040 --> 00:00:27.296
فبدأت بقراءة كتب الرياضيات للترفيه.

00:00:27.320 --> 00:00:28.856
(ضحك)

00:00:28.880 --> 00:00:30.576
أنتم فعلتم ذلك، أيضاً.

00:00:30.600 --> 00:00:34.336
قادني ذلك لعمل ليزر وحاسوب ونماذج صواريخ،

00:00:34.360 --> 00:00:37.360
فأنشأت ورشة صواريخ في غرفة نومي.

00:00:37.960 --> 00:00:41.616
الآن، علمياً،

00:00:41.640 --> 00:00:44.896
نسمي هذه بالفكرة السيئة جداً.

00:00:44.920 --> 00:00:46.136
(ضحك)

00:00:46.160 --> 00:00:48.336
في ذلك الوقت،

00:00:48.360 --> 00:00:51.576
رواية ستاني كوبورز "ملحمة الفضاء"
أصبحت في دور العرض،

00:00:51.600 --> 00:00:53.890
وبذلك تغيرت حياتي للأبد.

00:00:54.280 --> 00:00:56.336
لقد أحببت كل شيء في ذلك الفلم،

00:00:56.360 --> 00:00:58.896
خصوصي "هال" 9000.

00:00:58.920 --> 00:01:00.976
"هال" كان حاسوباً واعياً

00:01:01.000 --> 00:01:03.456
مصمماً لقيادة سفينة الاستكشاف

00:01:03.480 --> 00:01:06.016
من الأرض إلى المشترى.

00:01:06.040 --> 00:01:08.096
"هال" كان شخصيةً سيئة،

00:01:08.120 --> 00:01:12.400
في النهاية لقد اختار إتمام المهمة عوضاً 
عن إنقاذ البشرية.

00:01:12.840 --> 00:01:14.936
"هال" كان شخصيةً خياليََة،

00:01:14.960 --> 00:01:17.616
لكن ومع ذلك
لقد تحاكى مع مخاوفنا،

00:01:17.640 --> 00:01:19.736
مخاوفنا في التعامل مع

00:01:19.760 --> 00:01:22.776
ذكاء اصطناعي عديم المشاعر

00:01:22.800 --> 00:01:24.760
لا يختلف عنا نحن البشر.

00:01:25.880 --> 00:01:28.456
أنا أؤمن بأن هذه المخاوف لا وجود لها.

00:01:28.480 --> 00:01:31.176
في الواقع، نحن نعيش وقتاً مميزاً

00:01:31.200 --> 00:01:32.736
في تاريخ البشريََة،

00:01:32.760 --> 00:01:37.736
حيث نسير رافضين وضع حدود لقدراتنا
العقلية والجسدية،

00:01:37.760 --> 00:01:39.456
نقوم ببناء آلات

00:01:39.480 --> 00:01:43.096
ذات تعقيد وأداء رائعين وجميلين

00:01:43.120 --> 00:01:45.176
ستسهم في توسيع المعرفة البشريََة

00:01:45.200 --> 00:01:46.880
لمراحل لا يمكن تصورها.

00:01:47.720 --> 00:01:50.296
عبر خبرتي المهنية من الكلية الجوية

00:01:50.320 --> 00:01:52.256
إلى توجيه سفن الفضاء،
إلى الوقت الحالي،

00:01:52.280 --> 00:01:53.976
أصبحت مهندس أنظمة،

00:01:54.000 --> 00:01:56.736
ومؤخراً تم توكيلي بمهمة حل مشكلة
هندسية

00:01:56.760 --> 00:01:59.336
متعلََقة بعملية وكالة الفضاء 
"ناسا" في المريخ.

00:01:59.360 --> 00:02:01.856
حالياً، في الرحلات الفضائية إلى القمر،

00:02:01.880 --> 00:02:05.016
يمكننا الإعتماد على 
نظام الملاحة المسمََى "هيوستون"

00:02:05.040 --> 00:02:07.016
لمتابعة كافة تفاصيل الرحلة.

00:02:07.040 --> 00:02:10.576
ولكن المريخ يبعد مئتي ضعف مقارنة بالقمر،

00:02:10.600 --> 00:02:13.816
وبالتالي نحتاج في المتوسط ل 13 دقيقة

00:02:13.840 --> 00:02:16.976
لتنتقل الإشارة 
من الأرض إلى المريخ،

00:02:17.000 --> 00:02:20.400
وإذا ما حدثت مشكلة،
لا يوجد وقت كاف لتفاديها.

00:02:20.840 --> 00:02:23.336
ولذلك فإن الحل الهندسي الأكثر فائدة

00:02:23.360 --> 00:02:25.936
يدعونا لوضع جهاز توجيه للحركة (جهاز ملاحة)

00:02:25.960 --> 00:02:28.976
داخل جدران المركبة الفضائية.

00:02:29.000 --> 00:02:31.896
وفكرة أخرى جميلة تصب في 
مجال أداء العملية

00:02:31.920 --> 00:02:34.816
هي إرسال رجال آليين ووضعهم فوق سطح المريخ

00:02:34.840 --> 00:02:36.696
قبل وصول البشر بنفسهم إلى هناك،

00:02:36.720 --> 00:02:38.376
أولاً لتقديم بعض التسهيلات

00:02:38.400 --> 00:02:41.760
ومن ثمََ كأعضاء متعاونين
في الفريق العلمي.

00:02:43.400 --> 00:02:46.136
حالياًً عندما أتمعن في هذه الفكرة
من منظور هندسي،

00:02:46.160 --> 00:02:49.336
فإنني أرى بشكل واضح 
أن ما أخطط له

00:02:49.360 --> 00:02:51.536
هو حل ذكي، متعاون،

00:02:51.560 --> 00:02:53.936
يتمتع بالذكاء الاصطناعي والاجتماعي.

00:02:53.960 --> 00:02:58.256
بعبارة أخرى، أريد بناء ما يشبه "هال"
إلى حد كبير

00:02:58.280 --> 00:03:00.696
ولكن بدون أن يكون له نزعات للقتل.

00:03:00.720 --> 00:03:02.080
(ضحك)

00:03:02.920 --> 00:03:04.736
لنتوقف للحظة.

00:03:04.760 --> 00:03:08.656
هل من الممكن بناء نظام ذكي مثل هذا؟

00:03:08.680 --> 00:03:10.136
في الواقع، أجل.

00:03:10.160 --> 00:03:11.416
بتصورات مختلفة،

00:03:11.440 --> 00:03:13.416
إن هذه مشكلة هندسية صعبة

00:03:13.440 --> 00:03:14.896
تضمن عناصر ذكاء اصطناعي،

00:03:14.920 --> 00:03:19.616
ليست مجرد مسألة ذكاء اصطناعي
بسيطة يمكن إنشاؤها.

00:03:19.640 --> 00:03:22.296
كما ورد عن "آلان تورنج"،

00:03:22.320 --> 00:03:24.696
أنا لست مهتما بآلة ذات إحساس.

00:03:24.720 --> 00:03:26.296
أنا لا أقوم بصناعة "هال".

00:03:26.320 --> 00:03:28.736
كل ما أريد إنجازه هو مجرد عقل بسيط،

00:03:28.760 --> 00:03:31.880
يمكنه تقديم هذا الذكاء الخيالي.

00:03:33.000 --> 00:03:36.136
إن علوم وفنون الحاسوب قد أثمرت
في نهاية المطاف

00:03:36.160 --> 00:03:37.656
منذ ظهور "هال" على الشاشة،

00:03:37.680 --> 00:03:40.896
وأظن في حال وجود المخترع 
الدكتور "شاندرو" هنا اليوم،

00:03:40.920 --> 00:03:43.256
فإنه سوف يكون لديه العديد من
التساؤلات.

00:03:43.280 --> 00:03:45.376
هل بالفعل يمكننا

00:03:45.400 --> 00:03:49.416
تكوين نظام يحوي ملايين ملايين
الأجهزة،

00:03:49.440 --> 00:03:50.896
يمكنه قراءة معطياتها،

00:03:50.920 --> 00:03:53.176
التنبؤ بحالات حصول الخطأ،
و التصرف قبل وقوعه؟

00:03:53.200 --> 00:03:54.416
أجل.

00:03:54.440 --> 00:03:57.616
هل يمكننا بناء نظام يمكنه التحاكي
مع البشر بلغاتهم؟

00:03:57.640 --> 00:03:58.856
أجل.

00:03:58.880 --> 00:04:01.856
هل يمكننا بناء أنظمة يمكنها التعرف على
الأجسام، وعلى حركتها،

00:04:01.880 --> 00:04:05.256
والشعور بأنفسها، أو اللعب بألعاب الحاسب
وقراءة حركة الشفاه؟

00:04:05.280 --> 00:04:06.496
أجل.

00:04:06.520 --> 00:04:08.656
هل يمكننا بناء نظام يضع أهدافاًً لنفسه،

00:04:08.680 --> 00:04:12.296
يتوجب الوصول إليها وضع خطط 
يتعلمها حتى يصل هدفه؟

00:04:12.320 --> 00:04:13.536
أجل.

00:04:13.560 --> 00:04:16.896
هل يمكننا بناء أنظمة تستطيع التفكير؟

00:04:16.920 --> 00:04:18.416
هذا ما نقوم بالتعلم لإنجازه.

00:04:18.440 --> 00:04:21.920
هل يمكننا بناء أنظمة ذات اعتقادات
أدبية وخُلقية؟

00:04:22.480 --> 00:04:24.520
هذا ما يتوجب علينا تعلم كيفية إنجازه.

00:04:25.360 --> 00:04:26.736
إذا، لنتقبَََل حالياً

00:04:26.760 --> 00:04:29.656
القدرة على بناء نظام كهذا

00:04:29.680 --> 00:04:31.816
من أجل مهمة كهذه وغيرها.

00:04:31.840 --> 00:04:34.376
السؤال الآخر الذي ينبغي عليكم سؤاله هو،

00:04:34.400 --> 00:04:35.856
هل ينبغي علينا الخوف منه؟

00:04:35.880 --> 00:04:37.856
حسناً، كل تقنية جديدة

00:04:37.880 --> 00:04:40.776
تجلب معها شيئاً من الريبة.

00:04:40.800 --> 00:04:42.496
عندما رأينا السيارات لأول مرة،

00:04:42.520 --> 00:04:46.536
توقع البشر حدوث دمار للعائلة.

00:04:46.560 --> 00:04:49.256
عندما رأينا الهواتف لأول مرة،

00:04:49.280 --> 00:04:52.176
قلق الناس من قيامها بإنهاء المحادثات
الشخصية.

00:04:52.200 --> 00:04:56.136
في الماضي عندما انتشرت الكلمة المكتوبة،

00:04:56.160 --> 00:04:58.656
ظن البشر أننا سنفقد قدرتنا على التذكر.

00:04:58.680 --> 00:05:00.736
هذه التنبؤات واقعية لحد معين،

00:05:00.760 --> 00:05:03.176
ولكن هذه التقنيات

00:05:03.200 --> 00:05:06.576
جلبت لنا ما زاد خبرتنا كبشر

00:05:06.600 --> 00:05:08.480
بطرقٍ عميقة.

00:05:09.840 --> 00:05:12.120
لنتوسع في هذه النقطة بعض الشيء.

00:05:13.120 --> 00:05:17.856
أنا لا أخاف من بناء أنظمة 
ذكاء اصطناعي كهذه،

00:05:17.880 --> 00:05:21.696
ﻷنها بالنتيجة ستقوم باستعمال معاييرنا.

00:05:21.720 --> 00:05:25.216
لنأخذ بعين الاعتبار أن
بناء نظام واع يختلف تماماً

00:05:25.240 --> 00:05:28.536
عن بناء النظام التقليدي المعقد في الماضي.

00:05:28.560 --> 00:05:31.016
نحن لا نقوم ببرمجتها، نحن نعلِِمها.

00:05:31.040 --> 00:05:33.696
لتعليم نظام كيف يتعرف على الورود،

00:05:33.720 --> 00:05:36.736
أُريه آلاف الورود، من الأنواع المفضلة لدي.

00:05:36.760 --> 00:05:39.016
لتعليم نظام كيف يلعب لعبة

00:05:39.040 --> 00:05:41.000
حسنا، أريد فعل ذلك.
و أنتم تريدون، أيضاً.

00:05:42.600 --> 00:05:44.640
أنا أحب الورود، صدقوني.

00:05:45.440 --> 00:05:48.296
لتعليم نظام كيفية لعب لعبة ك "جو"،

00:05:48.320 --> 00:05:50.376
أقوم بلعب ال "جو" آلاف المرات،

00:05:50.400 --> 00:05:52.056
ضمنياً أقوم بتعليمه التمييز

00:05:52.080 --> 00:05:54.496
بين اللعبة الجيدة من السيئة.

00:05:54.520 --> 00:05:58.216
إذا أردت إنشاء مساعد قانوني
ذكي،

00:05:58.240 --> 00:06:00.016
سوف أعلمه بعض قواعد القانون

00:06:00.040 --> 00:06:02.896
ولكن بنفس الوقت أقوم بتمرير

00:06:02.920 --> 00:06:05.800
إحساس الرحمة والعدل المتعلق
بهذا القانون.

00:06:06.560 --> 00:06:09.536
علمياً نطلق على هذه العملية
"القاعدة الحقيقية"،

00:06:09.560 --> 00:06:11.576
هنا تكمن النقطة المهمة:

00:06:11.600 --> 00:06:13.056
لإنشاء آلات كهذه،

00:06:13.080 --> 00:06:16.496
نحن نقوم بتعليمها الشعور بقيمنا.

00:06:16.520 --> 00:06:19.656
إلى هنا، أنا أثق بالذكاء الاصطناعي

00:06:19.680 --> 00:06:23.320
بما يساوي، إن لم يزِد،
إنساناً جيد التدريب.

00:06:24.080 --> 00:06:25.296
لكن، قد تسألون،

00:06:25.320 --> 00:06:27.936
ماذا عن العملاء المحتالين،

00:06:27.960 --> 00:06:31.296
المؤسسات غير الحكومية 
ذات الدعم المادي القوي؟

00:06:31.320 --> 00:06:35.136
أنا لا أخاف من الذكاء الاصطناعي في يد
ثعلب وحيد.

00:06:35.160 --> 00:06:39.696
بشكل صريح، لا يمكننا حماية أنفسنا
من كافة أشكال العنف،

00:06:39.720 --> 00:06:41.856
لكن في الواقع نظام كهذا

00:06:41.880 --> 00:06:44.976
يحتاج لإنجاز تدريبات كثيفة ومعقدة

00:06:45.000 --> 00:06:47.296
تفوق قدرة الفرد الواحد.

00:06:47.320 --> 00:06:48.536
أيضاًً،

00:06:48.560 --> 00:06:51.816
إنها أصعب من مجرد إدخال فايروس
إلى العالم عبر الإنترنت،

00:06:51.840 --> 00:06:54.936
حيث بضغطة زر، ينتشر في ملايين الأجهزة

00:06:54.960 --> 00:06:57.416
وتتعطل الحواسيب المحمولة في كل مكان.

00:06:57.440 --> 00:07:00.256
إن هذه الأمور أكبر بكثير،

00:07:00.280 --> 00:07:01.995
ونحن نراها في المستقبل.

00:07:02.520 --> 00:07:05.576
هل أخاف من هكذا ذكاء اصطناعي

00:07:05.600 --> 00:07:07.560
قد يهدد البشرية جمعاء؟

00:07:08.280 --> 00:07:12.656
إذا ما تابعتم أفلام ك "ماتريكس"، 
و"شرطة القطار"

00:07:12.680 --> 00:07:15.856
و"المدمر"، ومسلسلات مثل "العالم الغربي"

00:07:15.880 --> 00:07:18.016
فإن كلها تتحدث عن هذه المخاوف.

00:07:18.040 --> 00:07:22.336
في الواقع، في كتاب "الذكاء الخارق"
للفيلسوف "نيك بوستروم"،

00:07:22.360 --> 00:07:23.896
يتناول هذه النمطية

00:07:23.920 --> 00:07:27.936
ويرى أن الذكاء الخارق ليس مجرد
خطر فحسب،

00:07:27.960 --> 00:07:31.816
وإنما يمكن أن يمثل تهديداً حقيقياً
لكافة البشرية.

00:07:31.840 --> 00:07:34.056
الجدل الأساسي للدكتور "بوستروم"

00:07:34.080 --> 00:07:36.816
أن هذه الأنظمة بشكل مفاجيء

00:07:36.840 --> 00:07:40.096
ستصبح متعطشة بشكل شديد للمعلومات

00:07:40.120 --> 00:07:43.016
وبالتالي سوف تتعلم كيف تتم عملية التعلم

00:07:43.040 --> 00:07:45.656
ومن ثم تكتشف أن لديها أهدافاً

00:07:45.680 --> 00:07:47.976
تخالف ما يحتاجه البشر.

00:07:48.000 --> 00:07:49.856
الدكتور "بوستروم" لديه عدد من الأتباع.

00:07:49.880 --> 00:07:54.200
و يدعمه في تفكيره أشخاص ك 
"إيلون موسك" و "ستيفن هاوكنج".

00:07:54.880 --> 00:07:57.280
مع كل احترامي

00:07:58.160 --> 00:08:00.176
لهذه العقول الرائعة،

00:08:00.200 --> 00:08:02.456
إلا أني أؤمن بأنهم على خطأ.

00:08:02.480 --> 00:08:05.656
هناك العديد من جدليات الدكتور "بوستوم"
للمناقشة،

00:08:05.680 --> 00:08:07.816
وليس لدي وقت كاف لمناقشتها كلها،

00:08:07.840 --> 00:08:10.536
لكن بشكل مختصر، فكروا في هذه:

00:08:10.560 --> 00:08:14.296
المعرفة الفائقة تختلف بشكل أساسي عن
العمل الخارق.

00:08:14.320 --> 00:08:16.216
"هال" كان خطراً على فريق الاستكشاف

00:08:16.240 --> 00:08:20.656
فقط عندما قام "هال" بإصدار
التعليمات كافة في عملية الاستكشاف.

00:08:20.680 --> 00:08:23.176
لذا هذه الفكرة مرتبطة بالذكاء الخارق.

00:08:23.200 --> 00:08:25.696
من الممكن أن تسيطر على عالمنا.

00:08:25.720 --> 00:08:28.536
هذه الأشياء مقتبسة من فلم "المدمر"

00:08:28.560 --> 00:08:30.416
حيث لدينا ذكاء خارق

00:08:30.440 --> 00:08:31.816
يقود رغبة البشر،

00:08:31.840 --> 00:08:35.696
يتحكم بكل جهاز موجود في كل زاوية
في أنحاء العالم.

00:08:35.720 --> 00:08:37.176
للحديث بشكل عملي،

00:08:37.200 --> 00:08:39.296
هذا لن يحدث.

00:08:39.320 --> 00:08:42.376
نحن لا نقوم ببناء أنظمة ذكاء اصطناعي
تتحكم بحالة الطقس،

00:08:42.400 --> 00:08:43.736
وتوجه المد والجزر،

00:08:43.760 --> 00:08:47.136
وتأمرنا نحن البشر المتقلبين العشوائيين.

00:08:47.160 --> 00:08:51.056
وللمزيد، إذا وُجدت أنظمة ذكاء اصطناعي
كهذه،

00:08:51.080 --> 00:08:54.016
ستنافس اقتصاد البشر،

00:08:54.040 --> 00:08:56.560
وبالتالي تنافس البشر
في المصادر والثروات.

00:08:57.200 --> 00:08:58.416
وفي النهاية.

00:08:58.440 --> 00:08:59.680
لا تخبروا نظام "سيري" بهذا

00:09:00.440 --> 00:09:01.816
حيث يمكننا إطفاؤها دائماً.

00:09:01.840 --> 00:09:03.960
(ضحك)

00:09:05.360 --> 00:09:07.816
نحن في رحلة عظيمة

00:09:07.840 --> 00:09:10.336
من التطور مع آلاتنا.

00:09:10.360 --> 00:09:12.856
ما نحن عليه اليوم كبشر

00:09:12.880 --> 00:09:15.416
يختلف عن ما سنكون عليه في المستقبل.

00:09:15.440 --> 00:09:18.576
القلق الآن من نمو الذكاء الخارق

00:09:18.600 --> 00:09:21.656
هو إلهاء خطير بحالات متعددة

00:09:21.680 --> 00:09:24.016
لأن نمو الحوسبة بذاتها

00:09:24.040 --> 00:09:27.056
يجلب لنا عدداً من المشاكل
البشرية والاجتماعية

00:09:27.080 --> 00:09:28.720
ينبغي علينا حلها الآن.

00:09:29.360 --> 00:09:32.176
كيف يمكنني أن أنظم المجتمع
بالشكل الأفضل

00:09:32.200 --> 00:09:34.536
عندما يقل احتياج اليد العاملة البشرية؟

00:09:34.560 --> 00:09:38.376
كيف يمكنني أن أجلب الوعي والتعليم لكافة 
أنحاء الأرض

00:09:38.400 --> 00:09:40.176
مع احترام كافة أشكال اختلافاتنا؟

00:09:40.200 --> 00:09:44.456
كيف يمكنني أن أوسع وأحسن حياة الإنسان
من خلال العناية الإدراكية؟

00:09:44.480 --> 00:09:47.336
كيف يمكنني استعمال الحوسبة

00:09:47.360 --> 00:09:49.120
لتقوم بأخذنا إلى النجوم؟

00:09:49.760 --> 00:09:51.800
وهذا هو الشيء الممتع.

00:09:52.400 --> 00:09:54.736
إن الفرص لاستعمال الحوسبة

00:09:54.760 --> 00:09:56.296
لتطوير خبرة الإنسان

00:09:56.320 --> 00:09:57.736
في متناول يدنا،

00:09:57.760 --> 00:09:59.616
هنا والآن،

00:09:59.640 --> 00:10:01.320
نحن نبدأ فحسب.

00:10:02.280 --> 00:10:03.496
شكراً لكم.

00:10:03.520 --> 00:10:07.806
(تصفيق)

