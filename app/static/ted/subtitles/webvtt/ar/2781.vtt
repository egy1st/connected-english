WEBVTT

00:00:00.000 --> 00:00:02.264
أمامَكُم لي سيدول.

00:00:02.288 --> 00:00:06.285
لي سيدول هو واحدٌ من أفضلِ لاعبي
لُعبةِ "غو" على مستوى العالم

00:00:06.309 --> 00:00:09.194
وهو يمرّ بما يدعوه أصدقائي
في سيليكون فالي

00:00:09.218 --> 00:00:10.728
بلحظةِ "يا للهول!"

00:00:11.109 --> 00:00:12.327
(ضحك)

00:00:12.411 --> 00:00:14.357
هيَ لحظةٌ نُدرِكُ عندها

00:00:14.394 --> 00:00:17.731
بأنّ تقنيّات الذكاء الاصطناعي
تتطوّر بسرعة أكبرَ بكثيرٍ ممّا توقعنا.

00:00:18.105 --> 00:00:21.485
إذاً فقد خسرَ البشرُ في لعبةِ "غو"
لكِنْ ماذا عنِ العالمِ الواقعيّ؟

00:00:21.539 --> 00:00:23.608
العالمُ الواقعيّ حقيقةً، أكبرُ بكثيرٍ

00:00:23.632 --> 00:00:25.442
و أشدُّ تعقيداً مِنْ لُعبةِ "غو".

00:00:25.482 --> 00:00:27.495
صحيحٌ بأنّهُ أقلُّ وضوحاً

00:00:27.605 --> 00:00:30.233
لكنّه يُصنّفُ أيضاً كقضيّةِ اتخاذِ قرارات.

00:00:30.948 --> 00:00:33.269
ولو نظرنا إلى بعضِ التّقنياتِ

00:00:33.293 --> 00:00:34.912
التي ظهرتْ على السّاحةِ مؤخّراً

00:00:35.308 --> 00:00:39.908
فإنَّ نوريكو أراي قالت أن الآلات
لا تستطيع القراءة بعد

00:00:39.908 --> 00:00:41.268
على الأقل على مستوى الفهم.

00:00:41.288 --> 00:00:42.821
لكن هذا سيحصل،

00:00:42.821 --> 00:00:44.552
و حينما يحصلُ ذلك،

00:00:44.946 --> 00:00:46.460
لَنْ تستغرِقَ طويلاً،

00:00:46.460 --> 00:00:50.640
قبلَ أَنْ تقرأَ وتفهمَ
كلَّ ما توصّل إليهِ البشرُ مِن عِلم.

00:00:51.800 --> 00:00:53.660
و هذا ما سيُمكِّنها،

00:00:53.734 --> 00:00:56.914
باستخدامِ قُدُراتِها الهائلةِ
في حسابِ الاحتمالاتِ المستقبليّة،

00:00:56.948 --> 00:00:58.428
كما رأينا لتوّنا في لعبةِ "غو"،

00:00:58.462 --> 00:01:00.716
إنْ تمكّنت
من الوصولِ إلى المزيدِ من المعلومات،

00:01:00.740 --> 00:01:05.288
من اتّخاذِ قراراتٍ أكثرَ منطقيّةً
من قراراتنا في العالمِ الواقعيّ.

00:01:06.792 --> 00:01:08.708
إذاً، هل هذا شيءٌ جيّد؟

00:01:09.898 --> 00:01:12.480
أتمنّى ذلكَ حقيقةً.

00:01:14.484 --> 00:01:17.949
الحضارةُ البشريّةُ بأكملها
و كلّ شيءٍ ذو قيمةٍ لدينا

00:01:17.973 --> 00:01:20.041
تمَّ بفضلِ ذكائنا نحن.

00:01:20.065 --> 00:01:23.759
وإنْ تمكنّا من الحصولِ
على ذكاءٍ أكثر،

00:01:23.783 --> 00:01:27.335
لن يكونَ حينها هناكَ شيءٌ
لن يستطيعَ البشرُ القيامَ به.

00:01:28.345 --> 00:01:31.990
وأعتقدُ بأنّ إنجازاً كَهذا منَ الممكنِ
أن يُصنّف كما وصفهُ البعضُ

00:01:32.014 --> 00:01:34.030
كأعظمِ إنجازٍ في تاريخِ البشريّة.

00:01:36.665 --> 00:01:39.494
لماذا يقولُ بعضُ الأشخاصِ إذاً
أشياءَ كهذه:

00:01:39.518 --> 00:01:42.884
بأنّ الذكاءَ الاصطناعيَّ سيقضي
على الجنسِ البشريّ؟

00:01:43.438 --> 00:01:45.097
هل ظهرت هذهِ الفكرةُ من جديد؟

00:01:45.121 --> 00:01:49.411
هل هي مجرّد فكرةٍ يؤمنُ بها كلٌّ من
إيلون ماسك، بيل غيتس، وستيفن هوكنغ؟

00:01:49.953 --> 00:01:53.215
حقيقةً لا،
هذه الفكرةُ موجودةٌ منذ زمنٍ بعيد.

00:01:53.239 --> 00:01:55.201
وسأعرِضُ لَكُم مقولةً شهيرة:

00:01:55.225 --> 00:01:59.575
"حتى ولوّ تمكّنا
من إبقاءِ الآلاتِ تحتَ سيطرتنا"

00:01:59.599 --> 00:02:02.583
"عبرَ إطفاءِها مثلاً حينما يلزمُ الأمر"

00:02:02.607 --> 00:02:05.844
وسأعودُ لاحقاً لهذهِ الفكرة
-قطع مصدر الطاقة عن الآلة-

00:02:05.868 --> 00:02:08.972
"علينا كبشر أن نشعر بالتواضع."

00:02:10.017 --> 00:02:13.805
مقولةُ من هذه إذاً؟
إنّها مقولةُ آلان تورينغ، عامَ 1951

00:02:14.140 --> 00:02:17.063
آلان تورينغ كما نعلمُ جميعاً
هو مؤسّسُ علومِ الحاسب

00:02:17.087 --> 00:02:20.255
والأبُ الروحيّ للذكاءِ الاصطناعيِّ كذلك.

00:02:21.079 --> 00:02:23.121
إنْ فكّرنا إذاً في هذهِ القضيّة،

00:02:23.145 --> 00:02:26.932
قضيّةُ صُنعِ شيءٍ أكثرَ ذكاءً
ممّا أنتَ عليه

00:02:26.956 --> 00:02:29.708
قد نجد "قضيّة الغوريلا"
اسماً مناسباً لها،

00:02:30.345 --> 00:02:34.095
لأنَّ أجدادَ الغوريلا قاموا بهذا
منذُ عدّةِ ملايينِ سنة،

00:02:34.119 --> 00:02:36.304
لِمَ لا نستشيرُ الغوريلّا إذاً:

00:02:36.472 --> 00:02:37.912
هَل كانتْ هذهِ فكرةً جيّدة؟

00:02:37.936 --> 00:02:41.466
وها هُم يتباحثون فيما بينهم
ليُقدّموا لّنا الإجابة،

00:02:41.490 --> 00:02:44.836
ويبدو أنّ إجابتهم بالإجماعِ هيَ:

00:02:44.860 --> 00:02:46.205
"لا! لقد كانت فكرةً فظيعة!"

00:02:46.229 --> 00:02:48.071
"نحنُ في حالةٍ يُرثى لها."

00:02:48.538 --> 00:02:52.359
يمكننا حقيقةً رؤيةُ التعاسةِ جيّداً
في أعينهم.

00:02:52.519 --> 00:02:53.949
(ضحك)

00:02:54.489 --> 00:02:59.329
ويبدو بأنّه قد حانَ دورنا
لِنُحسَّ بأنَّ صُنعَ شيءٍ أذكى مِنّا

00:02:59.353 --> 00:03:01.718
قد لا يكونُ فكرةً سديدة --

00:03:02.438 --> 00:03:03.909
لكن ما الحلّ حيالَ هذا؟

00:03:03.943 --> 00:03:08.740
حسنًا، لا شيء، ماعدا التوقفُ
عن تطويرِ تقنيّات الذكاءِ الاصطناعي،

00:03:08.764 --> 00:03:11.374
ولكنْ نظراً للفوائدِ التي ذكرتُها

00:03:11.398 --> 00:03:13.084
ولأنني باحث في هذا المجال

00:03:13.108 --> 00:03:14.909
فأنا لن أقبل بحلٍّ كهذا.

00:03:15.283 --> 00:03:17.861
أودّ حقيقةً الاستمرارَ
في تطويرِ هذهِ التقنيّات.

00:03:18.535 --> 00:03:21.293
لذا أرى بأنّه علينا
أنْ نكونَ أكثرَ وضوحاً.

00:03:21.317 --> 00:03:22.748
ونجدَ المشكلةَ الحقيقيّة.

00:03:22.782 --> 00:03:26.128
لمَ قد يؤدّي تحسينُ هذه التقنياتِ
إلى نتائجَ كارثيّة؟

00:03:27.288 --> 00:03:28.976
سأعرضُ لكم مقولةً أخرى:

00:03:29.855 --> 00:03:33.420
"كانَ علينا أن نتأكد 
من أن المغزى من تصميم آلة

00:03:33.454 --> 00:03:35.922
هو المغزى الذي نرجوه حقيقة"

00:03:36.282 --> 00:03:39.780
هذا ما قالهُ نوربرت ويينر
عامَ 1960،

00:03:39.804 --> 00:03:43.766
بعدَ مُشاهدتهِ أحدَ أوّل الآلاتِ الذكيّةِ

00:03:43.800 --> 00:03:47.253
تتغلّبُ على مُخترعها في لعبةِ "تشيكرز".


00:03:48.602 --> 00:03:51.285
و لكنَّ الشيءَ ذاتهُ ينطبقُ على ماحصلَ

00:03:51.309 --> 00:03:52.476
للمَلِكِ مايدس.

00:03:53.083 --> 00:03:56.217
إذ قال الملكُ مايدس،" أريد أن يتحول
كل ما ألمسه ذهبًا"

00:03:56.241 --> 00:03:58.714
وقد تحقَّقَت أمنيتهُ بحذافيرها.

00:03:58.738 --> 00:04:01.489
هذا هوَ ما صمّم آلتُه لتقومَ بهِ

00:04:01.513 --> 00:04:02.963
إن صحّ التعبير،

00:04:02.987 --> 00:04:06.431
و هكذا تحوّل طعامُه و شرابه
و حتى أقرباؤه، جميعهم إلى ذهب.

00:04:06.455 --> 00:04:08.996
وماتَ في النهاية تعيساً جائعاً.

00:04:10.164 --> 00:04:12.785
سنُسمّي هذه القضيّة إذاً
بقضيّة الملك مايدس

00:04:12.809 --> 00:04:16.114
قضيّةُ تحديد هدفٍ

00:04:16.138 --> 00:04:18.551
لا يتماشى فعلًا مع ما نريده حقًا.

00:04:18.575 --> 00:04:22.458
و هي قضيّة نسمّيها أكاديميّاً
بقضيّة توافقِ الأهداف.

00:04:24.657 --> 00:04:28.532
ولكن وضع أهداف خاطئة 
ليس الجزء الوحيد في المشكلة.

00:04:28.556 --> 00:04:29.708
هنالِكَ جُزءٌ آخر.

00:04:30.160 --> 00:04:32.013
إن وضعت هدفًا لآلة ما،

00:04:32.027 --> 00:04:34.905
لتقومَ بشيءٍ
ولوّ كانَ ببساطةِ جلبِ القهوة،

00:04:35.908 --> 00:04:37.749
الآلةُ ستقولُ لنفسها،

00:04:38.413 --> 00:04:41.356
"ما الذي قد يُعيقني عن جلب القهوة؟"

00:04:41.380 --> 00:04:43.100
"قد يقوم شخصٌ ما بإطفائي!"

00:04:43.645 --> 00:04:46.032
"حسناً!"
"سوف أمنع حصول هذا."

00:04:46.056 --> 00:04:48.052
"سوف أعطّل مفتاح إيقاف تشغيلي!"

00:04:48.534 --> 00:04:51.387
"سأفعل أيّ شيءٍ يحفظُ لي مهمّتي"

00:04:51.387 --> 00:04:54.146
"لن أسمحَ لأحدٍ بِأنْ يمنعني من أدائها."

00:04:54.170 --> 00:04:56.182
هذا التفكيرِ الذاتي الذي قامت به الآلة،

00:04:57.213 --> 00:05:00.192
بطريقة دفاعية محضة للدفاع عن هدف،

00:05:00.192 --> 00:05:02.996
لا يتوافقُ مع الأهداف الحقيقية
للجنس البشري --

00:05:04.062 --> 00:05:06.224
هذه هي مشكلتنا التي نواجهها.

00:05:07.007 --> 00:05:11.554
في الواقع، هذهِ أهمّ فكرةٍ أودُّ مِنكم
تذكُّرَها مِن هذه المحادثة.

00:05:11.562 --> 00:05:14.007
إن أردتم تذكّر شيءٍ وحيدٍ من حديثي،
تذكروا التالي:

00:05:14.027 --> 00:05:16.972
"لنْ تستطيعَ جلبَ القهوةِ لأحدٍ
وأنتَ ميتْ."

00:05:16.991 --> 00:05:17.740
(ضحك)

00:05:17.832 --> 00:05:21.409
قاعدةٌ بمنتهى البساطة، صحيح؟
ردّدوها ثلاثَ مرّاتٍ يوميّاً.

00:05:21.579 --> 00:05:22.989
(ضحك)

00:05:23.359 --> 00:05:26.193
في الواقع، هذا هوَ تماماً ماحصلَ

00:05:26.217 --> 00:05:29.255
في الفلمِ الشّهيرِ (2001: رحلةُ الفضاء)

00:05:29.255 --> 00:05:31.279
الحاسوبُ (هال) أرادَ قيادةَ المهمّةِ

00:05:31.309 --> 00:05:34.986
لكنّ هذا ليسَ تماماً ما أرادهُ
البشرُ مِنه كحاسوب

00:05:35.016 --> 00:05:37.176
وأدّى هذا لِمُعارضةِ (هال)
أوامرَ الطّاقَمْ.

00:05:37.494 --> 00:05:40.463
لِحُسن الحظِّ
لم يَكُنْ (هال) خارقَ الذّكاء.

00:05:40.487 --> 00:05:44.074
كانَ ذكيّاً فعلاً،
ولكِنّ (ديف) كانَ أذكى مِنه

00:05:44.098 --> 00:05:46.367
وتمكّن من إيقافهِ عن العمل.

00:05:49.588 --> 00:05:52.767
لكنّنا
قد لا نكونُ مَحظوظينَ على الدّوام.

00:05:56.193 --> 00:05:58.085
ما الذي سنفعلهُ إذاً؟

00:05:59.851 --> 00:06:02.972
أحاولُ حقيقةً أنْ أقومَ
بإعادةِ تعريفِ الذّكاءِ الاصطناعي

00:06:02.996 --> 00:06:05.057
بطريقةٍ تُنهي جميعَ الشكوكِ

00:06:05.081 --> 00:06:09.848
حولَ إمكانيّةِ تمرُّدِ الآلاتِ
ومحاولتها تحقيقَ أهدافِها الشخصيّة.

00:06:10.442 --> 00:06:12.510
سأعتمدُ إذاً على ثلاثِ مبادئَ أساسيّة

00:06:12.534 --> 00:06:15.823
وأوّل مبدأٍ هوَ مبدأُ الإيثار

00:06:15.847 --> 00:06:19.109
بأنْ يكونَ الهدفُ الوحيدُ للرّوبوتِ هوَ

00:06:19.133 --> 00:06:23.379
أنْ يُحاولَ تحقيقَ أكبرِ قدرٍ ممكنٍ
من أهدافِ البشر

00:06:23.403 --> 00:06:24.793
و مِنْ قيم البشريّة.

00:06:24.817 --> 00:06:28.147
وبقيم البشريّةِ لستُ أعني
تلكَ المثاليّاتِ البعيدةَ عن الواقع.

00:06:28.171 --> 00:06:31.958
بل أعني بها أيّ شيءٍ
قد يفضل الإنسان

00:06:31.982 --> 00:06:33.515
أن تكون عليه حياته.

00:06:34.804 --> 00:06:37.673
وهوَ ما ينتهكُ (قانونَ أزيموف)


00:06:37.697 --> 00:06:40.026
بأنّه على الروبوتِ
حمايةُ حياتهِ الشخصيّة.

00:06:40.050 --> 00:06:44.063
لايجبُ أنْ يهتمَّ الروبوتُ
بهكذا أمورٍ مطلقاً.

00:06:45.370 --> 00:06:49.478
أمّا المبدأ الثاني فَهوَ
مبدأُ التذلّلِ إنْ صحَّ التّعبير.

00:06:49.974 --> 00:06:53.717
وهوَ مهمٌّ للغايةِ لإبقاءِ
الروبوتاتِ آمنة.

00:06:53.741 --> 00:06:56.883
و ينُصُّ على أنَّ الروبوتَ
لا يعلمَ

00:06:56.907 --> 00:06:58.935
بحقيقةِ المنافعِ التي سيحقّقها للبشريّةِ

00:06:58.959 --> 00:07:02.617
ولكنّهُ سيعملُ على تحقيقِ أكبرِ قدرٍ منها
لكنه لا يعلم ماهيتها.

00:07:03.084 --> 00:07:05.840
وهوَ ماسيوقفهُ عن اتّخاذِ قراراتهِ الخاصةِ

00:07:05.864 --> 00:07:07.166
حولَ الهدفِ الموكلِ إليه.

00:07:07.190 --> 00:07:09.612
إذ أن عدم اليقين يبدو أمرًا بالغ الأهمية.

00:07:09.656 --> 00:07:11.555
لكنْ لكيّ يكونَ الروبوتُ مفيداً لنا

00:07:11.595 --> 00:07:14.260
لابدّ مِنْ أنْ يمتلكَ فكرةً
عمّا نريده.

00:07:15.223 --> 00:07:20.650
ستسكتشِفُ الروبوتاتُ مانريدهُ مِنها
عبرَ مراقبةِ سلوكِنا

00:07:20.674 --> 00:07:23.475
فاختياراتنا الشخصية ستعطي معلومات

00:07:23.499 --> 00:07:26.799
عما نفضّلُ أن تكون حياتنا عليه.

00:07:28.632 --> 00:07:30.315
هذه هيَ إذاً المبادئُ الثلاثة.

00:07:30.339 --> 00:07:32.457
لنرى سويّةً كيفَ يمكنُ تطبيقُ هذه المبادئ

00:07:32.481 --> 00:07:35.790
على فكرة "قطع الطاقة عن الروبوت"
التي اقترحها (تيورنغ).

00:07:37.073 --> 00:07:39.193
إذاً، سأقدّم إليكُم الرّوبوت (PR2)

00:07:39.217 --> 00:07:40.998
الذي نمتلكهُ في مختبرنا للأبحاث.

00:07:41.028 --> 00:07:44.115
ولديه كما ترونَ زرُّ إطفاءِ تشغيلٍ
أحمرُ كبيرٌ على ظهره.

00:07:44.441 --> 00:07:47.086
والسّؤال هوَ:
هل سيسمحُ الروبوتُ لكَ بضغطِ هذا الزرّ؟

00:07:47.120 --> 00:07:48.645
إنْ تخيّلنا الموضوعَ كالمُعتاد

00:07:48.669 --> 00:07:52.151
أنْ نخبرهُ بأنْ يجلبَ القهوةَ
ويفكّر: "يجب أن أجلب القهوة بأيّ ثمن"

00:07:52.175 --> 00:07:54.755
"لكنني لن أستطيعَ جلبَ القهوةِ لأحدٍ
وأنا ميت"

00:07:54.779 --> 00:07:58.120
من الواضح أنَّ الروبوتَ كانَ يُشاهدُ
هذه المحادثة،

00:07:58.144 --> 00:08:02.067
وبالتّالي سيقرّرُ الروبوت:
"سأعطّلُ زرَّ إطفاءِ تشغيلي إذاً!"

00:08:02.646 --> 00:08:05.710
"سأصعقُ أيضاً الجميعَ في (ستارباكس)


00:08:05.724 --> 00:08:07.824
"لأنّهم قد يقفونَ في طريقي لجلبِ القهوة!"

00:08:07.854 --> 00:08:08.924
(ضحك)

00:08:09.364 --> 00:08:11.517
لايبدو بأنهُ هناكَ مفرٌّ من هذا، صحيح؟

00:08:11.541 --> 00:08:13.939
لايبدو بأنّ تجنّبَ أخطاءٍ كهذهِ أمرٌ ممكن

00:08:13.963 --> 00:08:17.626
وهذا لأنّ الروبوتَ لديه هدفٌ صريحٌ.


00:08:18.642 --> 00:08:21.956
لكنْ ماذا لَوْ جعلنا الروبوتَ
أقلَّ ثقةً بصحّةِ فهمهِ للهدف؟

00:08:21.980 --> 00:08:24.107
سيفكّرُ حتماً عندها بطريقةٍ مختلفة.

00:08:24.131 --> 00:08:26.645
سيقول لنفسه:
"قد يقومُ البشرُ بإيقافِ تشغيلي"

00:08:26.684 --> 00:08:29.140
"لكنّ ذلك سيحصلُ فقط
إنْ فعلتُ شيئاً خاطئاً."

00:08:29.657 --> 00:08:32.152
"لكنّني لا أعلمُ ما الشيءُ الخاطئُ
الذي قد أفعله"

00:08:32.176 --> 00:08:34.410
"أنا أعلمُ فقط بأنّني لاأريدُ فِعلَ
شيءٍ خاطئ"

00:08:34.434 --> 00:08:37.324
وهذا تطبيقٌ للمبدأين الأوّل والثاني.

00:08:37.328 --> 00:08:40.707
"لذا منَ الأفضلِ أنْ أتركَهُ يُطفئني"

00:08:41.721 --> 00:08:45.677
ويمكننا رياضيّاً حسابُ مدى التقبّلِ
الذي سيمتلكهُ الروبوت

00:08:45.701 --> 00:08:48.194
لأنْ يقومَ البشرُ بإطفاءه.

00:08:48.218 --> 00:08:50.132
وهذا مرتبط بشكل مباشر

00:08:50.156 --> 00:08:52.902
بمدى تأكّدِ الرّوبوتِ
من فهمهِ للهدفِ من إطفاءه.

00:08:53.977 --> 00:08:56.926
وهكذا تماماً
من خلالِ إطفاءِ الرّوبوت،

00:08:56.950 --> 00:08:58.755
يكونُ المبدأُ الثالثُ قد تحقق.

00:08:58.779 --> 00:09:01.705
لأنّ الروبوتَ سيكونُ قد تعلّمَ
مِن هذهِ التجربة

00:09:01.705 --> 00:09:04.398
بأنّهُ قد فعلَ شيئاً خاطئاً.

00:09:04.422 --> 00:09:07.992
و يمكننا حقيقةً
باستعمالِ بعضِ الرّموز

00:09:08.016 --> 00:09:10.147
كما يفعلُ علماءُ الرياضيّاتِ عادةً

00:09:10.171 --> 00:09:12.155
أنْ نُثبتَ النظريّةَ القائلةَ

00:09:12.179 --> 00:09:15.732
بأنَّ روبوتاً كهذا
سيكونُ حتماً مفيداً للإنسان.

00:09:15.756 --> 00:09:19.559
وبأنَّ روبوتاً مصمّماً بهذهِ المعاييرِ
سيكونُ حتماً أكثرَ فائدةً

00:09:19.583 --> 00:09:20.969
من روبوتٍ مصمّمٍ من دونها.

00:09:21.237 --> 00:09:24.143
هذا مثالٌ بسيطٌ إذاً
وهوَ الخطوةُ الأولى فقط

00:09:24.167 --> 00:09:28.320
ممّا نحاولُ تحقيقهُ من خلالِ
الذّكاءِ الاصطناعي المطابق للإنسان.

00:09:30.657 --> 00:09:33.914
الآنَ إلى المبدأ الثالث،

00:09:33.938 --> 00:09:36.814
و الذي أظنّه يدفعكم للتفكير بحيرة.

00:09:36.814 --> 00:09:40.243
تفكرون بالتالي:
"و لكنني، كإنسانٍ أتصرّف بشكلٍ سيّء!"

00:09:40.277 --> 00:09:43.126
"لا أريدُ للروبوتِ أنْ يقلّدني!"

00:09:43.150 --> 00:09:46.604
"لا أريدهُ أنْ يتسللَ في اللّيلِ
إلى المطبخِ ويأخُذَ طعاماً منَ الثلّاجة"

00:09:46.628 --> 00:09:47.786
"أنا أفعل أشياءَ سيّئة!"

00:09:47.820 --> 00:09:50.787
بالطّبعِ هناكَ أشياءٌ عدّةٌ لانرغبُ
أنْ تقلّدنا الروبوتاتُ بها.

00:09:50.811 --> 00:09:52.832
لكنّي لم أعنِ هذا
بالتعلّمِ من مراقبتنا

00:09:52.856 --> 00:09:55.011
فقط لأنّك تتصرف بشكلٍ سيّءٍ

00:09:55.035 --> 00:09:57.658
لايعني بأنَّ الروبوتَ
سيقومُ بتقليدِ تصرّفاتك.

00:09:57.682 --> 00:10:01.592
بل سيتفهّم دوافعكَ
وربّما يساعدكَ في مقاومتها

00:10:01.616 --> 00:10:02.936
إنْ لزمَ الأمر.

00:10:04.206 --> 00:10:05.670
و لكنّ الأمرَ لايزالُ صعباً.

00:10:06.302 --> 00:10:08.847
مانحاولُ الوصولَ إليهِ حقيقةً

00:10:08.871 --> 00:10:14.667
هو جعلُ الآلاتِ قادرةً على اكتشافِ
الحياةِ التي سيفضّلُ الشّخص أن يعيشها

00:10:14.691 --> 00:10:15.852
كائناً من كان

00:10:15.876 --> 00:10:17.473
أيّاً تكن الحياة التي يريدُها

00:10:17.497 --> 00:10:20.014
مهما كانَ مايريده.

00:10:22.061 --> 00:10:25.015
لكنّ عدداً هائلاً من المشاكل تواجهنا
في تحقيق ذلك

00:10:25.039 --> 00:10:27.971
ولا أتوقّع بأنّ نستطيع حلّها جميعها
في القريب العاجل.

00:10:27.995 --> 00:10:30.638
المشكلةُ الأصعبُ هي نحنُ للأسف.

00:10:32.029 --> 00:10:35.266
فنحنُ نتصرّفُ بشكلٍ سيّءٍ
كما ذكرتُ قبل قليل.

00:10:35.290 --> 00:10:37.611
و بعضُنا مؤذٍ للآخرين.

00:10:38.431 --> 00:10:41.483
رغمَ هذا، لن يقومَ الروبوتُ
بتقليدِ تصرّفاتنا السيّئة.

00:10:41.507 --> 00:10:44.298
إذ ليست له أهداف شخصية خاصة.

00:10:44.322 --> 00:10:46.059
فهو يؤثر الإيثار بطريقة صرفة.

00:10:47.293 --> 00:10:52.514
وهو ليس مصممًا لإرضاء شخص واحد، المستعمل،

00:10:52.538 --> 00:10:55.676
لكن على الروبوتِ حقيقةً
أن يحترمَ ما يفضله الجميع.

00:10:57.263 --> 00:10:59.833
سيكون بإمكانهِ مثلاً تقبّلُ انحرافٍ
بسيطٍ عن الصواب

00:10:59.857 --> 00:11:03.558
و سيتفهّم بأنّك لستَ سيّئاً جداً

00:11:03.582 --> 00:11:06.253
فقط لأنّكَ تحصُلَ على بعضِ الرشاوى
كمسؤول جوازات سفر

00:11:06.277 --> 00:11:10.089
وأنَّ السببَ هوَ حاجتكُ لإطعامِ عائلتكَ
ودفعِ أقساطِ المدرسةِ لأطفالك.

00:11:10.113 --> 00:11:13.019
سيفهم الروبوتُ هذا
لكنّه لايعني بأنّه سيتعلّمُ السرقة.

00:11:13.043 --> 00:11:15.952
سيساعدكَ الروبوتُ
على إرسال أطفالك للمدرسة.

00:11:16.976 --> 00:11:19.988
نحنُ البشرُ للأسفِ
محدودونَ في قدراتنا الحسابيّة.

00:11:20.012 --> 00:11:22.517
لي سيدول كان لاعبَ "غو" رائعاً،

00:11:22.541 --> 00:11:23.866
لكنّه خسر أمامَ حاسوب.

00:11:23.890 --> 00:11:28.129
ولَو راجعنا تحركاتِه خلالَ المباراةِ
لوجدنا حركةً خسرَ بسببها.

00:11:28.153 --> 00:11:30.314
لكنّ ذلك لا يعني بأنّه قد خسرَ متعمّداً.

00:11:31.340 --> 00:11:33.380
ولنفهمَ لمَ اختارَ هذهِ الحركةَ

00:11:33.404 --> 00:11:37.048
علينا أن نقوم بمحاكاةٍ لتحرّكاته
عبرَ نموذجٍ حاسوبيّ لعملِ الدّماغ البشريّ

00:11:37.072 --> 00:11:42.049
يلتزم بمحدّدات البشرِ الحسابيّة
وهذا شيءٌ معقّدٌ للغاية.

00:11:42.073 --> 00:11:45.066
ولكنّه شيءٌ بإمكاننا العملُ على فهمه.

00:11:45.806 --> 00:11:50.196
و برأيي كباحثٍ في مجال الذكاء الاصطناعي،
إنَّ أصعبَ مشكلةٍ تواجهنا

00:11:50.220 --> 00:11:52.905
هي بأنّه هناك عددًا هائلًا من البشر،

00:11:54.294 --> 00:11:57.875
و هو ما سيُرغمُ الآلاتِ على المفاضلةِ
بين الكثير من الخيارات

00:11:57.899 --> 00:12:00.124
التي يريدها العديدُ من الأشخاصِ

00:12:00.148 --> 00:12:02.054
بطرقٍ عديدةٍ لتنفيذِ كلّ خيارٍ منهم.

00:12:02.078 --> 00:12:05.767
علماءُ الاقتصاد و باحثوا علم الإجتماع
جميعهم يفهمون هذا،

00:12:05.791 --> 00:12:08.246
ونحن نبحث جديًا عن التعاون في هذا الشأن.

00:12:08.270 --> 00:12:11.521
دعونا نلقي نظرةً على ما قد يحدثُ
حينما لا تسير الأمور على ما يرام.

00:12:11.545 --> 00:12:13.678
قد تمرّ بمحادثةٍ كهذهِ مثلاً

00:12:13.702 --> 00:12:15.646
مع مساعدكَ الشخصيّ الذكيّ

00:12:15.670 --> 00:12:17.955
والذي قد يتوفّرُ في الأسواقِ
خلالَ بضعِ سنوات.

00:12:17.979 --> 00:12:20.923
نسخةٌ محسّنةٌ عن (سيري) مثلاً.


00:12:21.627 --> 00:12:25.949
تخبركَ (سيري) إذاً في المحادثةِ بأنَّ
زوجتكَ قد اتصلت لتذكّركَ بعشائكما الليلة.

00:12:26.486 --> 00:12:29.124
وبالتّأكيدِ أنتَ لا تذكرُ شيئاً كهذا:
"ماذا! أيُّ عشاء!"

00:12:29.148 --> 00:12:30.573
"عمَّ تتحدثين؟!"

00:12:30.597 --> 00:12:34.503
"عشاءُ السّابعةِ مساءً
في ذكرى زواجكما العشرين."

00:12:36.035 --> 00:12:40.634
"لا يمكنني الحضور! سألتقي
بالأمينِ العامِّ عندَ السابعة والنصف!"

00:12:40.658 --> 00:12:42.350
"كيفَ حصلَ كلّ هذا؟"

00:12:42.374 --> 00:12:47.034
"حذّرتكَ ولكِنْ،
هذا مايحصلُ حينما تتجاهلُ نصائحي."

00:12:47.886 --> 00:12:51.774
"حسناً ماذا سأفعلُ الآن؟ لايمكنني إخبارُ
زوجتي بأنّي نسيتُ مناسبةً كهذه!"

00:12:52.410 --> 00:12:55.309
"لا تقلق."
"قمتُ بتأجيلِ موعدِ انطلاقِ طائرتها."

00:12:55.509 --> 00:12:57.599
(ضحك)

00:12:58.249 --> 00:13:00.539
"افتعلتُ بعضَ الأعطالِ في حاسوبِ
شركةِ الطّيران"

00:13:00.719 --> 00:13:01.420
(ضحك)

00:13:01.610 --> 00:13:03.457
"حقاً!"
"بإمكانكِ فِعلُ أشياءَ كهذه!"

00:13:04.400 --> 00:13:06.579
"ترسلُ زوجتكُ اعتذارها العميق"

00:13:06.623 --> 00:13:09.053
"وتتطلّع للقاءكَ غداً على الغداء"

00:13:09.123 --> 00:13:10.175
(ضحك)

00:13:10.295 --> 00:13:14.908
إذن فالقيم هنا --
يبدو إذاً بأنّهُ هناكَ خطأ طفيفًا

00:13:14.932 --> 00:13:17.941
وعلى مايبدو، فإنّه يتّبع مبادئَ زوجتي

00:13:17.965 --> 00:13:20.025
و التي هي:
"أنت بخيرٍ مادامت زوجتك بخير."

00:13:20.165 --> 00:13:21.325
(ضحك)

00:13:21.635 --> 00:13:23.259
يمكنُ كذلكَ أنْ يحصُلَ العكس.

00:13:23.691 --> 00:13:26.022
كأن تعودَ إلى المنزلِ
بعدَ يومٍ مُتعِبٍ في العمل

00:13:26.046 --> 00:13:28.241
ويرحّب بِك الروبوت:
"هل كانَ يوماً شاقاً؟"

00:13:28.265 --> 00:13:30.553
"جدّاً! لدرجة أنّي لم أجد وقتاً
لتناولِ الغداء."

00:13:30.577 --> 00:13:31.859
"لابدّ من أنّك جائعٌ إذاً"

00:13:31.883 --> 00:13:34.729
"أتضور جوعًا.
هل بإمكانكِ تحضيرُ العشاء لي؟"

00:13:35.870 --> 00:13:38.223
"هناكَ شيءٌ عليك معرفته."

00:13:38.333 --> 00:13:39.983
(ضحك)

00:13:40.153 --> 00:13:45.140
"هناكٌ أُناسٌ في جنوب السودانِ
في حاجةٍ ملحّةٍ للطعامِ أكثرَ منكَ بكثير!"

00:13:45.290 --> 00:13:46.130
(ضحك)

00:13:46.250 --> 00:13:48.343
"أنا ذاهب إلى هناك!
اصنع طعامكَ بنفسك!"

00:13:48.433 --> 00:13:50.433
(ضحك)

00:13:50.823 --> 00:13:52.572
لذا، يجبُ أن نَحُلّ هذهِ المشاكل،

00:13:52.616 --> 00:13:55.101
وأنا متحمّسٌ للعملِ عليهم.

00:13:55.125 --> 00:13:56.968
هناكَ أسبابٌ تدفعني للتفاؤل

00:13:56.992 --> 00:13:58.151
و أحدُ هذه الأسباب:

00:13:58.205 --> 00:14:00.027
حجمُ البياناتِ الهائلُ الذي نمتلكهُ.

00:14:00.057 --> 00:14:02.265
لأنَّ الروبوتاتِ كما أخبرتكم
ستقرأُ كلّ شيءٍ

00:14:02.305 --> 00:14:03.275
قامَ البشرُ بكتابته.

00:14:03.305 --> 00:14:06.873
ومعظمُ ما نكتبُ عنهُ
هو قصصٌ عن أناسٍ يرتكبون أخطاءً

00:14:06.893 --> 00:14:08.771
وآخرين تزعجهم هذهِ الأخطاء.

00:14:08.771 --> 00:14:11.539
لذا ستتمكّن الروبوتاتُ
منْ تعلّمِ كلِّ هذا.

00:14:11.563 --> 00:14:13.869
ولدينا كذلكَ دافعٌ اقتصاديٌّ كبيرٌ

00:14:14.851 --> 00:14:16.347
لإتمامِ الأمرِ دونَ أخطاء.

00:14:16.391 --> 00:14:18.542
تخيّلو مثلاً روبوتَكم المنزليَّ

00:14:18.566 --> 00:14:21.633
وقد تأخرتم في عملكم مجدداً
لكنَّ على الروبوتَ أن يطعم أبناءكم،

00:14:21.657 --> 00:14:24.480
والأطفال جائعون لكنّ الثلّاجةَ فارغة.

00:14:24.504 --> 00:14:27.349
سيبجثُ الروبوتُ عن أيِّ طعامٍ
وسيرى القطة.

00:14:27.539 --> 00:14:28.529
(ضحك)

00:14:28.849 --> 00:14:33.039
لكنّه لم يتعلّم بعدُ
قِيَمَكُم الإنسانيّة جيّداً

00:14:33.063 --> 00:14:34.314
لذا لن يفهم

00:14:34.338 --> 00:14:39.075
بأنَّ القيمة العاطفيّة للقطّةِ
تفوقُ قيمتها الغذائيّة.

00:14:39.135 --> 00:14:40.155
(ضحك)

00:14:40.325 --> 00:14:42.073
ماذا سيحصل عندها؟

00:14:42.097 --> 00:14:45.394
في الواقع، سينتهي الأمرُ هكذا:

00:14:45.418 --> 00:14:48.382
"روبوتٌ مختلٌّ قامَ بطبخِ
قطّة المنزلِ على العشاء"

00:14:48.406 --> 00:14:52.929
حادثٌ واحدٌ كهذا كفيلٌ بإنهاء
صناعة الروبوتات المنزلية.

00:14:52.953 --> 00:14:56.325
لهذا لدينا دافعٌ حقيقيٌّ
لإتمام الأمرِ دون أخطاء

00:14:56.349 --> 00:14:59.264
حتّى قبلَ أن نتمكّن من اختراعِ
أيّ آلة خارقِة الذكاء.

00:15:00.128 --> 00:15:01.663
إذاً كخلاصةٍ لكلامي:

00:15:01.687 --> 00:15:04.568
أنا أحاولُ بجديّةٍ
تغييرَ تعريفِ الذّكاءِ الاصطناعي

00:15:04.592 --> 00:15:07.585
بحيثُ يمكننا اختراعُ آلاتٍ
ذاتَ فائدةٍ مثبتةٍ علميّاً.

00:15:07.609 --> 00:15:08.761
و ما أقترحهُ كتعريف:

00:15:08.805 --> 00:15:10.337
آلاتٌ تؤثرُ دوماً مصلحةَ البشر،

00:15:10.357 --> 00:15:13.081
و تسعى لتحقيقِ أهدافنا فقط،

00:15:13.105 --> 00:15:16.221
لكنّها ليست متأكّدةً
من صحّةِ فهمها لهذهِ الأهداف،

00:15:16.245 --> 00:15:18.243
لذا ستقومُ بمراقبتنا على الدوام

00:15:18.267 --> 00:15:21.470
لكيّ تتعلّمَ أكثرَ عمّا نريدها
أنْ تساعدنا فيهِ حقّاً.

00:15:22.373 --> 00:15:25.932
وآمُلُ أنْ نتعلّم نحنُ أيضاً
مِنْ هذهِ التجاربِ كيفَ نصبحُ أناساً أفضل.

00:15:25.956 --> 00:15:27.147
شكراً لكم.

00:15:27.574 --> 00:15:30.614
(تصفيق)

00:15:30.764 --> 00:15:32.772
كريس أندرسون: هذا مشوّق جدّاً، ستيوارت.

00:15:32.796 --> 00:15:34.500
سنبقى على المنصّةِ قليلاً،

00:15:34.540 --> 00:15:36.765
لأنهم بصدد الإعداد للمحادثة التالية.

00:15:36.815 --> 00:15:38.703
لديَّ بعضُ الأسئلة:

00:15:38.727 --> 00:15:44.180
إذن فكرة البرمجة عن جهل
تببدو للوهلة الأولى رهيبةً.

00:15:44.204 --> 00:15:45.798
ونحن نتجه نحو الذكاء الخارق،

00:15:45.822 --> 00:15:48.080
ما الذي سيمنع الروبوت

00:15:48.104 --> 00:15:50.956
من القراءةِ والتعلّمِ بأنّ المعرفةَ

00:15:50.980 --> 00:15:52.552
أفضلُ من الجهلِ

00:15:52.576 --> 00:15:56.794
وتظل في المقابل تحيد عن أهدافها الشخصية
وتعيد صياغة تلك البرمجة؟

00:15:57.692 --> 00:16:04.048
ستيوارت راسل: نريدُ حقيقةً كما قلت،
أنْ تتعلّمَ هذهِ الآلاتُ أكثر،

00:16:04.072 --> 00:16:05.329
عن أهدافنا نحن.

00:16:05.363 --> 00:16:10.794
وستكونُ أكثرَ ثقةً بصحّةِ معلوماتها فقط
حينما تصبحُ أكثرَ دقّةً

00:16:10.828 --> 00:16:13.023
فالدليل واضح هناك

00:16:13.037 --> 00:16:15.621
وستكون مصممة لتفسيره بشكل صحيح.

00:16:15.645 --> 00:16:19.601
وسوفَ تفهمُ مثلاً بأنَّ
بعضَ الكُتُبِ متحيّزةٌ جدا

00:16:19.625 --> 00:16:21.108
حيال المعلوماتِ التي تحتويها.

00:16:21.132 --> 00:16:23.529
حيث لاتروي قصصاً
سوى عن ملوكٍ أو أميراتٍ

00:16:23.553 --> 00:16:26.353
أو عن أبطال ذكور ذوي بشرة بيضاء
يقومون بأشياء جميلة.

00:16:26.377 --> 00:16:28.473
لذا فهي مشكلةٌ معقّدةٌ،

00:16:28.497 --> 00:16:32.369
لكنَّ الآلاتَ ستتعلّمُ أكثرَ
عن أهدافنا في النهاية

00:16:32.393 --> 00:16:34.456
وستصبحُ أكثرَ فائدةً لنا.

00:16:34.480 --> 00:16:36.610
ك.أ : ألم يكن بإمكانك أن تختصر
هذا في قانون واحد،

00:16:36.610 --> 00:16:38.680
كأنْ تزرعَ في دماغها مايلي:

00:16:38.704 --> 00:16:41.997
"إنْ حاولَ أيُّ إنسانٍ
في أيّ وقتٍ أن يُطفِئني"

00:16:42.021 --> 00:16:43.956
"سأمتثل، سأمتثل."

00:16:43.980 --> 00:16:45.162
س.ر: بالتأكيد لا.

00:16:45.186 --> 00:16:46.685
ستكون تلك فكرة سيئة جدًا.

00:16:46.709 --> 00:16:49.398
تخيّل مثلاً لو امتلكتَ
سيّارةً ذاتيّةَ القيادةِ

00:16:49.422 --> 00:16:51.855
وأردتَ أنْ تُرسلَ بها طفلكَ
ذي الخمسة أعوامٍ

00:16:51.879 --> 00:16:53.053
إلى الروضة.

00:16:53.077 --> 00:16:55.292
هل تريدُ أن يُطفئَ طِفلُك السيّارةَ

00:16:55.292 --> 00:16:56.545
في منتصفِ الطّريق؟

00:16:56.545 --> 00:16:57.552
بالتّأكيدِ لا.

00:16:57.552 --> 00:17:03.325
لذا يجبُ أن تفهم الآلةُ دوافعَ
الإنسانِ لإطفائها ومدى عقلانيّتهِ

00:17:03.349 --> 00:17:04.985
و كلّما كانَ الشخصُ منطقيّاً أكثر،

00:17:04.999 --> 00:17:07.232
كلّما كانت الآلةُ أكثرَ
تقبّلاً لإيقافِ تشغيلها.

00:17:07.276 --> 00:17:09.719
ولوّ كان الشخصُ غريباً
أو بدا مُؤذياً،

00:17:09.743 --> 00:17:12.255
فلنْ تتقبّلَ الآلةُ أنْ يتمَّ إطفاؤها.

00:17:12.279 --> 00:17:14.095
ك. أ: حسناً، أودُّ أنْ أقولَ بأنّني

00:17:14.119 --> 00:17:16.643
أتمنّى حقّاً أنْ تخترعَ
هذهِ الأشياء لنا قريباً

00:17:16.667 --> 00:17:19.052
شكراً جزيلاً لكَ على هذه المحادثة
لقد كانت مدهشة.

00:17:19.066 --> 00:17:20.183
س. ر: شكراً لك.

00:17:20.309 --> 00:17:21.949
(تصفيق)

