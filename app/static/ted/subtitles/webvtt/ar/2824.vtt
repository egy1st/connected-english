WEBVTT

00:00:01.000 --> 00:00:05.080
سأتحدث اليوم عن التكنولوجيا والمجتمع.

00:00:07.040 --> 00:00:10.736
قدّرت هيئة المواصلات أنه في السنة الماضية

00:00:10.760 --> 00:00:14.840
توفي 35000 شخص إثر حوادث سير
في الولايات المتحدة وحدها.

00:00:16.040 --> 00:00:20.840
على مستوى العالم، يموت 1.2 مليون شخص
سنويًا إثر حوادث سير.

00:00:21.760 --> 00:00:25.856
إن كانت هناك طريقة تستطيع تجنب 90 في المئة
من هذه الحوادث،

00:00:25.880 --> 00:00:27.080
هل كنت لتدعمها؟

00:00:27.720 --> 00:00:29.016
بالطبع ستفعل.

00:00:29.040 --> 00:00:32.695
هذا ما تطمح السيارات ذاتية القيادة
للوصول إليه

00:00:32.720 --> 00:00:35.536
عن طريق حذف السبب الرئيس للحوادث --

00:00:35.560 --> 00:00:36.760
وهو الخطأ البشري.

00:00:37.920 --> 00:00:43.336
تخيل نفسك الآن في سيارة ذاتية القيادة
سنة 2030،

00:00:43.360 --> 00:00:46.816
وأنت تجلس في الخلف 
وتشاهد هذا الشريط القديم ل TEDxCambridge.

00:00:46.840 --> 00:00:48.840
(ضحك)

00:00:49.520 --> 00:00:50.736
وفجأة،

00:00:50.760 --> 00:00:54.040
تتعرض السيارة لخلل ميكانيكي
ولا تستطيع التوقف.

00:00:55.360 --> 00:00:56.880
إن استمرت السيارة في التقدم،

00:00:57.720 --> 00:01:01.840
ستصطدم بمجموعة من الراجلين
وهم يقطعون الطريق،

00:01:03.080 --> 00:01:05.215
لكن السيارة تنحرف،

00:01:05.239 --> 00:01:07.096
لتصطدم بشخص واقف،

00:01:07.120 --> 00:01:09.200
لتقتله وتنقذ الراجلين.

00:01:10.040 --> 00:01:12.640
ما الذي يجب على السيارة فعله،
ومن عليه أن يقرر؟

00:01:13.520 --> 00:01:17.056
ماذا لو انحرفت السيارة في اتجاه الحائط

00:01:17.080 --> 00:01:20.376
لتسحقك وتقتلك، أنت الراكب فيها،

00:01:20.400 --> 00:01:22.720
لتنقذ هؤلاء الراجلين؟

00:01:23.240 --> 00:01:26.320
تم استلهام هذا السيناريو
من مشكلة الترولي،

00:01:26.960 --> 00:01:30.736
والتي تم اختراعها من طرف فلاسفة
قبل عقود قليلة

00:01:30.760 --> 00:01:32.000
للتفكير حول الأخلاقيات.

00:01:34.120 --> 00:01:36.616
الآن، الطريقة التي نفكر بها
في هذا المشكل مهمة.

00:01:36.640 --> 00:01:39.256
يمكننا مثلًا ألا نفكر فيه إطلاقًا.

00:01:39.280 --> 00:01:42.656
يمكننا أن نقول أن هذا السيناريو غير واقعي،

00:01:42.680 --> 00:01:45.000
غير ممكن بتاتًا، أو أنه تافه.

00:01:45.760 --> 00:01:48.496
لكن أعتقد أن هذا الانتقاد يغفل المغزى

00:01:48.520 --> 00:01:50.680
لأنه ينظر إلى السيناريو بطريقة بسيطة.

00:01:51.920 --> 00:01:54.656
بالطبع، لايمكن لحادثة أن تكون هكذا،

00:01:54.680 --> 00:01:58.016
ليس هناك احتمالان أو ثلاثة لحادثة

00:01:58.040 --> 00:02:00.040
حيث يموت الجميع بطريقة أو بأخرى.

00:02:01.480 --> 00:02:04.056
عوضا عن ذلك، ستقوم السيارة بحساب ما

00:02:04.080 --> 00:02:08.976
مثل احتمال الاصطدام
بمجموعة معينة من الناس،

00:02:09.000 --> 00:02:12.336
حين تنحرف إلى اتجاه دون آخر،

00:02:12.360 --> 00:02:15.816
يمكن أن ترفع الخطر بشكل ضئيل
على الركاب أو السائقين الآخرين

00:02:15.840 --> 00:02:17.376
في مقابل الراجلين.

00:02:17.400 --> 00:02:19.560
ستكون عبارة عن حسابات معقدة،

00:02:20.480 --> 00:02:23.000
لكنها ستشمل دائمًا نوعا من المقايضة،

00:02:23.840 --> 00:02:26.720
وغالبا مما تتطلب المقايضة حضور الأخلاقيات.

00:02:27.840 --> 00:02:30.576
يمكننا أن نقول حينها، "حسنا، 
لمَ لا نترك القلق جانيًا.

00:02:30.600 --> 00:02:35.240
فلننتظر حتى تصير التكنولوجيا جاهزة
وآمنة 100 بالمئة."

00:02:36.520 --> 00:02:40.200
لنفترض أننا فعلا نستطيع حذف 90 في المئة
من هذه الحوادث،

00:02:41.080 --> 00:02:43.920
أو حتى 99 في المئة منها
خلال 10 سنوات المقبلة.

00:02:44.920 --> 00:02:48.096
ماذا لو كان يتطلب حذف العشر في المئة
المتبقية من الحوادث

00:02:48.120 --> 00:02:51.240
50 سنة أخرى من البحث؟

00:02:52.400 --> 00:02:54.200
أليس علينا أن نتبنى التكنولوجيا؟

00:02:54.720 --> 00:02:59.496
سيكون لدينا 60 مليون حالة وفاة
من حوادث السير

00:02:59.520 --> 00:03:01.280
إذا حافظنا على المعدل الحالي.

00:03:02.760 --> 00:03:03.976
فالفكرة هنا،

00:03:04.000 --> 00:03:07.616
التريث حتى نصل
للسلامة الكاملة هو اختيار أيضا،

00:03:07.640 --> 00:03:09.800
ويشمل مقايضات أيضا.

00:03:11.560 --> 00:03:15.896
قام الناس على مواقع التواصل الاجتماعية
باختلاق كل الطرق الممكنة

00:03:15.920 --> 00:03:17.936
لعدم التفكير في هذا المشكل.

00:03:17.960 --> 00:03:21.176
اقترح شخص أن على السيارة
أن تنحرف في اتجاه ما فحسب

00:03:21.200 --> 00:03:23.336
بين الركاب --

00:03:23.360 --> 00:03:24.376
(ضحك)

00:03:24.400 --> 00:03:25.656
والشخص الواقف.

00:03:25.680 --> 00:03:29.040
بالطبع، إن كانت السيارة تستطبع فعل ذلك،
فعليها فعل ذلك.

00:03:29.920 --> 00:03:32.760
نحن مهتمون بإيجاد سيناريوهات
حيث لا يكون هذا ممكنا.

00:03:33.280 --> 00:03:38.696
والاقتراح المفضل لدي شخصيا كان من مدون

00:03:38.720 --> 00:03:41.736
بأن تضع زر إخراج في السيارة، 
حيث يمكنك الضغط عليه --

00:03:41.760 --> 00:03:42.976
(ضحك)

00:03:43.000 --> 00:03:44.667
قبل أن تدمر السيارة نفسها.

00:03:44.691 --> 00:03:46.371
(ضحك)

00:03:47.840 --> 00:03:53.040
إذن حين نعترف أنه من الممكن للسيارات
أن تقوم بمقايضات على الطريق،

00:03:54.200 --> 00:03:56.080
كيف يمكننا التفكير حيال هذه المقايضات،

00:03:57.320 --> 00:03:58.896
وكيف لنا أن نقرر؟

00:03:58.920 --> 00:04:02.056
حسنا، ربما علينا إجراء استقصاء
لنعرف ما الذي يريده المجتمع،

00:04:02.080 --> 00:04:03.536
لأنه في النهاية،

00:04:03.560 --> 00:04:07.520
القوانين هي انعكاس للقيم المجتمعية.

00:04:08.040 --> 00:04:09.280
إذن فهذا ما قمنا به.

00:04:09.880 --> 00:04:11.496
مع زملائي،

00:04:11.520 --> 00:04:13.856
جون فرونسوا بونفون وأزيم شريف،

00:04:13.880 --> 00:04:15.496
أجرينا استقصاءً

00:04:15.520 --> 00:04:18.375
حيث قدمنا للناس هذه الأنواع
من السيناريوهات.

00:04:18.399 --> 00:04:22.176
قدمنا لهم احتمالين مستلهمين
من فيلسوفين:

00:04:22.200 --> 00:04:24.840
جيرمي بينتام وإيمانويل كانت.

00:04:25.600 --> 00:04:28.696
يقول بينتام أن على السيارة 
أن تتبع الأخلاقيات النفعية:

00:04:28.720 --> 00:04:32.136
عليها أن تخطو الخطوة
التي ستقلل مجموع الضرر --

00:04:32.160 --> 00:04:34.976
حتى وإن كانت ستقتل الرجل الواقف

00:04:35.000 --> 00:04:37.440
حتى وإن كانت هذه الخطوة ستقتل الراكب.

00:04:38.120 --> 00:04:43.096
يقول إيمانويل كارت أن على السيارة
أن تتبع مبادئ الواجب الملزم،

00:04:43.120 --> 00:04:44.680
"عليك ألا تقتل."

00:04:45.480 --> 00:04:49.936
إذن فعليك ألا تتخذ خطوة ستضر بها
عنوة شخصا ما،

00:04:49.960 --> 00:04:52.416
وعليك أن تترك السيارة تأخذ مسارها

00:04:52.440 --> 00:04:54.400
حتى وإن كانت ستلحق الضرر بأشخاص أكثر.

00:04:55.640 --> 00:04:56.840
ماذا تعتقدون؟

00:04:57.360 --> 00:04:58.880
بيتم أو كانت؟

00:04:59.760 --> 00:05:01.016
إليكم ماوجدنا.

00:05:01.040 --> 00:05:02.840
معظم الناس وقفوا إلى جانب بينتام.

00:05:04.160 --> 00:05:07.936
لذا يبدو أن الناس يريدون
أن تكون السيارات نفعية،

00:05:07.960 --> 00:05:09.376
وتقلل مجموع الضرر،

00:05:09.400 --> 00:05:10.976
وهذا ماعلينا أن نقوم به جميعا.

00:05:11.000 --> 00:05:12.200
تم حل المشكلة.

00:05:13.240 --> 00:05:14.720
لكن هناك نقطة صغيرة.

00:05:15.920 --> 00:05:19.656
حين سألنا الناس إن كانوا سيشترون
مثل هذه السيارات،

00:05:19.680 --> 00:05:21.296
قالوا "بالطبع لا."

00:05:21.320 --> 00:05:23.616
(ضحك)

00:05:23.640 --> 00:05:27.536
هم يريدون أن يشتروا سيارات تحميهم بأي ثمن،

00:05:27.560 --> 00:05:31.176
لكنهم يريدون أن يشتري الآخرون سيارات
تقلل من الضرر.

00:05:31.200 --> 00:05:33.720
(ضحك)

00:05:34.720 --> 00:05:36.576
لقد رأينا هذا المشكل من قبل.

00:05:36.600 --> 00:05:38.160
وهو يدعى المعضلة الاجتماعية.

00:05:39.160 --> 00:05:40.976
ولفهم المعضلة الاجتماعية

00:05:41.000 --> 00:05:43.040
علينا أن نعود قليلا إلى التاريخ.

00:05:44.000 --> 00:05:46.576
في القرن 19،

00:05:46.600 --> 00:05:50.336
نشر خبير الاقتصاد البريطاني
ويليام فورستر لويد كتيبا

00:05:50.360 --> 00:05:52.576
يصف فيه السيناريو الآتي.

00:05:52.600 --> 00:05:54.256
لديك مجموعة من المزارعين --

00:05:54.280 --> 00:05:55.616
مزارعون بريطانيون --

00:05:55.640 --> 00:05:58.320
يتقاسمون أرضا مشتركة لترعى بها أغنامهم.

00:05:59.520 --> 00:06:02.096
الآن، إذا قام كل مزارع بجلب
عدد معين من الأغنام --

00:06:02.120 --> 00:06:03.616
لنقل ثلاثة أغنام --

00:06:03.640 --> 00:06:05.736
ستزدهر الأرض،

00:06:05.760 --> 00:06:06.976
وسيفرح المزارعون،

00:06:07.000 --> 00:06:08.616
وستفرح الأغنام،

00:06:08.640 --> 00:06:09.840
كل شيء على ما يرام.

00:06:10.440 --> 00:06:12.960
الآن، إذا قام مزارع بجلب خروف إضافي،

00:06:13.800 --> 00:06:18.520
سيكون ذلك المزارع ناجحا بقدر يسير،
ولن يتضرر أحد من هذا.

00:06:19.160 --> 00:06:22.800
لكن إن أخذ كل مزارع 
هذا القرار الفردي المنطقي،

00:06:23.840 --> 00:06:26.560
سيتم استغلال الأرض بشكل جائر
وستُستنزف

00:06:27.360 --> 00:06:29.536
ولن يستفيد المزارعون جميعهم.

00:06:29.560 --> 00:06:31.680
وبالتأكيد، لن تستفيد الأغنام.

00:06:32.720 --> 00:06:36.400
نرى هذا المشكل في كل مكان:

00:06:37.080 --> 00:06:40.256
في صعوبة إدلرة الصيد الجائر،

00:06:40.280 --> 00:06:44.840
أو تقليل انبعاث الكربون
للتعامل مع التغير المناخي.

00:06:47.160 --> 00:06:50.080
حين يتعلق الأمر بتقنين
السيارات ذاتية القيادة،

00:06:51.080 --> 00:06:55.416
فالأرض المشتركة الآن هي بالأساس
السلامة العامة --

00:06:55.440 --> 00:06:56.680
هذا هو الشيء المشترك --

00:06:57.400 --> 00:06:59.376
والمزارعون هم الركاب

00:06:59.400 --> 00:07:03.000
أو أصحاب السيارات الذيم يختارون
أن يتجولوا بهذه السيارات.

00:07:04.960 --> 00:07:07.576
وباتخاذ اختيار منطقي فردي

00:07:07.600 --> 00:07:10.416
أو إعطاء الأولوية للسلامة الشخصية،

00:07:10.440 --> 00:07:13.576
فقد يكونون بصدد إلحاق الضرر
بالشيء المشترك،

00:07:13.600 --> 00:07:15.800
والذي هو تقليل مجموع الضرر.

00:07:18.320 --> 00:07:20.456
وهذا مايسمى بتراجيديا المشاع،

00:07:20.480 --> 00:07:21.776
بشكل تقليدي،

00:07:21.800 --> 00:07:24.896
لكنني أعتقد أنه في حالة السيارات
ذاتية القيادة،

00:07:24.920 --> 00:07:27.776
فإن المشكلة قد تكون أكثر تعقيدًا

00:07:27.800 --> 00:07:31.296
لأنه ليس هناك بالضرورة شخص واحد

00:07:31.320 --> 00:07:33.016
يقوم بهذه القرارات.

00:07:33.040 --> 00:07:36.336
إذ يمكن لمصنعي السيارات ببساطة
أن يبرمجوا سيارات

00:07:36.360 --> 00:07:38.880
تعطي حماية قصوى لعملائهم،

00:07:40.080 --> 00:07:43.056
ويمكن لهذه السيارات
أن تتعلم تلقائيا بمفردها

00:07:43.080 --> 00:07:46.600
أن القيام بذلك يتطلب
زيادة الخطر بالنسبة للراجلين.

00:07:47.520 --> 00:07:48.936
وباستخدام مثال الأغنام،

00:07:48.960 --> 00:07:52.576
فهذا أشبه بخروف كهربائي
له عقله الخاص.

00:07:52.600 --> 00:07:54.056
(ضحك)

00:07:54.080 --> 00:07:57.160
ويمكنه أن يذهب ويرعى دون أن يدري المزارع.

00:07:58.640 --> 00:08:02.616
وهذا مايمكننا أن نسميه
تراجيديا المشاع الخوارزمية،

00:08:02.640 --> 00:08:05.000
وهي تقدم أنواعا جديدة من التحديات.

00:08:10.520 --> 00:08:12.416
عادة، وبشكل تقليدي،

00:08:12.440 --> 00:08:15.776
نحل هذا النوع من المعضلات الاجتماعية
عن طريق القوانين،

00:08:15.800 --> 00:08:18.536
حيث تجتمع الحكومات أو المجتمعات

00:08:18.560 --> 00:08:22.296
ليقرروا جماعيا ما نوع النتائج 
التي يريدونها

00:08:22.320 --> 00:08:24.976
وما نوع العوائق على مستوى السلوك الفردي

00:08:25.000 --> 00:08:26.200
التي يجب عليهم تنفيذها.

00:08:27.600 --> 00:08:30.216
وبعد ذلك، باستخدام المراقبة والفرض،

00:08:30.240 --> 00:08:32.799
يمكنهم ضمان الحفاظ على الصالح العام.

00:08:33.440 --> 00:08:35.015
لذا، لم لا نقوم،

00:08:35.039 --> 00:08:36.535
كواضعي قوانين،

00:08:36.559 --> 00:08:39.456
بإجبار السيارات على تقليل الضرر؟

00:08:39.480 --> 00:08:41.720
في جميع الأحوال، هذا ما يريده الناس.

00:08:43.200 --> 00:08:44.616
والأهم من هذا،

00:08:44.640 --> 00:08:47.736
يمكنني أن أكون متأكدا كفرد،

00:08:47.760 --> 00:08:51.616
إن اشتريت سيارة يمكنها أن تضحي بي
في حالة نادرة جدا،

00:08:51.640 --> 00:08:53.296
فلن أكون الأبله الوحيد
الذي يقوم بهذا

00:08:53.320 --> 00:08:56.000
بينما يتمتع الجميع بحماية لامحدودة.

00:08:57.120 --> 00:09:00.456
في استقصائنا، سألنا الناس فيما إن كانوا
سيدعمون القوانين

00:09:00.480 --> 00:09:01.680
وإليكم ماوجدنا.

00:09:02.360 --> 00:09:06.120
أولا، رفض الناس القوانين،

00:09:07.280 --> 00:09:08.536
ثانيا، قالوا،

00:09:08.560 --> 00:09:12.496
"حسنا، إن قننتم السيارات لتقوم بهذا
وتقلل مجموع الضرر،

00:09:12.520 --> 00:09:14.000
فلن أشتري هذه السيارة."

00:09:15.400 --> 00:09:16.776
لذا، من المضحك،

00:09:16.800 --> 00:09:20.296
أنه بتقنين السيارات لتقليل الضرر،

00:09:20.320 --> 00:09:22.160
ربما سينتهي بنا المطاف
بالمزيد من الضرر

00:09:23.040 --> 00:09:26.696
لأن الناس لن يختاروا التكنولوجيا
الأكثر أمنا

00:09:26.720 --> 00:09:28.800
حتى وإن كانت أكثر أمنا
من الأشخاص السائقين.

00:09:30.360 --> 00:09:33.776
ليست لدي إجابة نهائية لهذا اللغز،

00:09:33.800 --> 00:09:35.376
لكنني أعتقد كبداية،

00:09:35.400 --> 00:09:38.696
نريد أن يجتمع المجتمع

00:09:38.720 --> 00:09:41.480
لنقرر ما هي المقايضات التي نرتاح لها

00:09:42.360 --> 00:09:45.840
ونجد طرقا يمكننا بها فرض هذه المقايضات.

00:09:46.520 --> 00:09:49.056
وكنقطة بداية، بنى طلابي الأذكياء،

00:09:49.080 --> 00:09:51.536
إدموند عواد و سوهان دسوزا،

00:09:51.560 --> 00:09:53.360
موقع Moral Machine،

00:09:54.200 --> 00:09:56.880
الذي يخرج سيناريوهات عشوائية لكم --

00:09:58.080 --> 00:10:00.536
مجموعة من المعضلات العشوائية بتسلسل

00:10:00.560 --> 00:10:04.480
حيث يمكنك أن تختار ما الذي يجب
أن تقوم به السيارة في سيناريو معين.

00:10:05.040 --> 00:10:09.640
ونقوم بتغيير الأعمار
وأنواع الضحايا المختلفة.

00:10:11.040 --> 00:10:14.736
ولحد الآن فقد جمعنا 5 ملايين قرار

00:10:14.760 --> 00:10:16.960
من ما يفوق مليون شخص حول العالم

00:10:18.400 --> 00:10:19.600
من الموقع.

00:10:20.360 --> 00:10:22.776
وهذا يساعدنا على تكوين صورة مبكرة

00:10:22.800 --> 00:10:25.416
عن ماهية المقايضات التي يرتاح لها الناس

00:10:25.440 --> 00:10:27.336
وما هي الأشياء المهمة بالنسبة لهم --

00:10:27.360 --> 00:10:28.800
حتى وإن اختلفت الثقافات.

00:10:30.240 --> 00:10:31.736
لكن الأمر الأهم،

00:10:31.760 --> 00:10:35.136
يساعد هذا التمرين الناس على التعرف

00:10:35.160 --> 00:10:37.976
على صعوبة اتخاذ هذه الاختيارات

00:10:38.000 --> 00:10:41.800
وأن أمام واضعي القوانين مهمة صعبة 
ذات اختيارات مستحيلة.

00:10:43.360 --> 00:10:46.936
وهذا سيساعدنا كمجتمع على أن نفهم
أنواع المقايضات

00:10:46.960 --> 00:10:50.016
التس سيتم تطبيقها في النهاية في القوانين.

00:10:50.040 --> 00:10:51.776
وبالفعل، كنت سعيدا جدا حينما سمعت

00:10:51.800 --> 00:10:53.816
بأن الدفعة الأول من القوانين

00:10:53.840 --> 00:10:55.976
التي صدرت عن هيئة المواصلات --

00:10:56.000 --> 00:10:57.456
أُعلِن عنها الأسبوع الماضي --

00:10:57.456 --> 00:11:03.976
تتضمن لائحة من 15 نقطة
على كل مصنعي السيارات توفيرها،

00:11:04.000 --> 00:11:07.256
والنقطة رقم 14 كانت ذات اعتبار أخلاقي --

00:11:07.280 --> 00:11:09.000
كيف ستقوم بالتعامل مع هذا.

00:11:11.800 --> 00:11:14.456
لقد جعلنا الناس يفكرون في قراراتهم الخاصة

00:11:14.480 --> 00:11:17.480
من خلال إعطائهم ملخصات عما اختاروه.

00:11:18.440 --> 00:11:20.096
سأعطيكم مثالًا --

00:11:20.120 --> 00:11:23.656
وعلي أن أحذركم
أن هذا ليس مثالا اعتدتم عليه،

00:11:23.680 --> 00:11:25.056
كمستعمل اعتيادي

00:11:25.080 --> 00:11:28.696
هذه هي الشخصية التي يتم إنقاذها بكثرة
وهذه هي التي يتم التضحية بها

00:11:28.720 --> 00:11:33.920
(ضحك)

00:11:34.680 --> 00:11:36.576
قد يتفق بعضكم معه،

00:11:36.600 --> 00:11:38.240
أو معها، لا ندري.

00:11:40.480 --> 00:11:46.616
لكن يبدو أن هذا الشخص يفضل
قليلا الركاب على الراجلين

00:11:46.640 --> 00:11:48.736
في اختياره

00:11:48.760 --> 00:11:51.576
وهو سعيد بأن يعاقب
كل من لا يحترم إشارة المرور .

00:11:51.600 --> 00:11:54.640
(ضحك)

00:11:57.320 --> 00:11:58.536
إذن لنلخص ما سبق.

00:11:58.559 --> 00:12:01.975
فقد بدأنا بسؤال --
لنسمه المعضلة الأخلاقية --

00:12:02.000 --> 00:12:05.056
ما الذي يجب على السيارة أن تقوم به
في حالة معينة:

00:12:05.080 --> 00:12:06.280
أن تنحرف أو لا؟

00:12:07.240 --> 00:12:09.976
لكن بعدها أدركنا أن المشكل كان مختلفًا.

00:12:10.000 --> 00:12:14.536
كان المشكل عن كيفية جعل المجتمع
يوافق ويفرض


00:12:14.560 --> 00:12:16.496
المقايضات التي يرتاح لها.

00:12:16.520 --> 00:12:17.776
هذه معضلة اجتماعية.

00:12:17.800 --> 00:12:22.816
في الأربعينيات، كتب إسحاق عظيموف
قوانينه الشهيرة عن الروبوتات --

00:12:22.840 --> 00:12:24.160
قوانين الروبوتات الثلاث --

00:12:25.240 --> 00:12:27.696
الروبوت لن يضر الإنسان،

00:12:27.720 --> 00:12:30.256
الروبوت لن يعصي الإنسان،

00:12:30.280 --> 00:12:33.536
ولن يسمح الروبوت لنفسه بأن يأتي الضرر --

00:12:33.560 --> 00:12:35.520
بهذا التسلسل حسب الأهمية.

00:12:36.360 --> 00:12:38.496
لكن بعد 40 سنة ونيف

00:12:38.520 --> 00:12:42.256
وبعد العديد من القصص التي دفعت 
هذه القوانين لتظهر ضعفها،

00:12:42.280 --> 00:12:45.976
قدم عظيموف القانون الصفري للترموديناميك

00:12:46.000 --> 00:12:48.256
والذي كانت له الأولوية قبل كل شيء،

00:12:48.280 --> 00:12:51.560
وهي أن الروبوت لن يضر الإنسانية جمعاء.

00:12:52.480 --> 00:12:56.856
لست أدري ما الذي يعنيه هذا 
في سياق السيارات ذاتية القيادة

00:12:56.880 --> 00:12:59.616
أو أي حالة معينة،

00:12:59.640 --> 00:13:01.856
ولست أدري كيف يمكننا تطبيق هذا،

00:13:01.880 --> 00:13:03.416
لكنني أعتقد أنه من خلال الإدراك

00:13:03.440 --> 00:13:09.576
أن تقنين السيارات ذاتية القيادة 
ليس مشكلا تكنولوجيا فقط

00:13:09.600 --> 00:13:12.880
ولكنه مشكل تعاون مجتمعي،

00:13:13.800 --> 00:13:16.730
آمل أن يكون بمقدرونا أن نبدأ على الأقل
في طرح الأسئلة المناسبة

00:13:17.200 --> 00:13:18.416
شكرا لكم.

00:13:18.440 --> 00:13:21.360
(تصفيق)

