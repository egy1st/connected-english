WEBVTT

00:00:00.000 --> 00:00:03.000
Ever since I was a little girl

00:00:03.000 --> 00:00:05.000
seeing "Star Wars" for the first time,

00:00:05.000 --> 00:00:07.000
I've been fascinated by this idea

00:00:07.000 --> 00:00:09.000
of personal robots.

00:00:09.000 --> 00:00:11.000
And as a little girl,

00:00:11.000 --> 00:00:13.000
I loved the idea of a robot that interacted with us

00:00:13.000 --> 00:00:16.000
much more like a helpful, trusted sidekick --

00:00:16.000 --> 00:00:18.000
something that would delight us, enrich our lives

00:00:18.000 --> 00:00:21.000
and help us save a galaxy or two.

00:00:22.000 --> 00:00:25.000
I knew robots like that didn't really exist,

00:00:25.000 --> 00:00:27.000
but I knew I wanted to build them.

00:00:27.000 --> 00:00:29.000
So 20 years pass --

00:00:29.000 --> 00:00:31.000
I am now a graduate student at MIT

00:00:31.000 --> 00:00:33.000
studying artificial intelligence,

00:00:33.000 --> 00:00:35.000
the year is 1997,

00:00:35.000 --> 00:00:38.000
and NASA has just landed the first robot on Mars.

00:00:38.000 --> 00:00:41.000
But robots are still not in our home, ironically.

00:00:41.000 --> 00:00:43.000
And I remember thinking about

00:00:43.000 --> 00:00:45.000
all the reasons why that was the case.

00:00:45.000 --> 00:00:47.000
But one really struck me.

00:00:47.000 --> 00:00:50.000
Robotics had really been about interacting with things,

00:00:50.000 --> 00:00:52.000
not with people --

00:00:52.000 --> 00:00:54.000
certainly not in a social way that would be natural for us

00:00:54.000 --> 00:00:56.000
and would really help people accept robots

00:00:56.000 --> 00:00:58.000
into our daily lives.

00:00:58.000 --> 00:01:01.000
For me, that was the white space; that's what robots could not do yet.

00:01:01.000 --> 00:01:04.000
And so that year, I started to build this robot, Kismet,

00:01:04.000 --> 00:01:07.000
the world's first social robot.

00:01:07.000 --> 00:01:09.000
Three years later --

00:01:09.000 --> 00:01:11.000
a lot of programming,

00:01:11.000 --> 00:01:13.000
working with other graduate students in the lab --

00:01:13.000 --> 00:01:15.000
Kismet was ready to start interacting with people.

00:01:15.000 --> 00:01:17.000
(Video) Scientist: I want to show you something.

00:01:17.000 --> 00:01:19.000
Kismet: (Nonsense)

00:01:19.000 --> 00:01:22.000
Scientist: This is a watch that my girlfriend gave me.

00:01:22.000 --> 00:01:24.000
Kismet: (Nonsense)

00:01:24.000 --> 00:01:26.000
Scientist: Yeah, look, it's got a little blue light in it too.

00:01:26.000 --> 00:01:29.000
I almost lost it this week.

00:01:29.000 --> 00:01:32.000
Cynthia Breazeal: So Kismet interacted with people

00:01:32.000 --> 00:01:35.000
like kind of a non-verbal child or pre-verbal child,

00:01:35.000 --> 00:01:38.000
which I assume was fitting because it was really the first of its kind.

00:01:38.000 --> 00:01:40.000
It didn't speak language, but it didn't matter.

00:01:40.000 --> 00:01:42.000
This little robot was somehow able

00:01:42.000 --> 00:01:45.000
to tap into something deeply social within us --

00:01:45.000 --> 00:01:47.000
and with that, the promise of an entirely new way

00:01:47.000 --> 00:01:49.000
we could interact with robots.

00:01:49.000 --> 00:01:51.000
So over the past several years

00:01:51.000 --> 00:01:53.000
I've been continuing to explore this interpersonal dimension of robots,

00:01:53.000 --> 00:01:55.000
now at the media lab

00:01:55.000 --> 00:01:57.000
with my own team of incredibly talented students.

00:01:57.000 --> 00:02:00.000
And one of my favorite robots is Leonardo.

00:02:00.000 --> 00:02:03.000
We developed Leonardo in collaboration with Stan Winston Studio.

00:02:03.000 --> 00:02:06.000
And so I want to show you a special moment for me of Leo.

00:02:06.000 --> 00:02:08.000
This is Matt Berlin interacting with Leo,

00:02:08.000 --> 00:02:10.000
introducing Leo to a new object.

00:02:10.000 --> 00:02:13.000
And because it's new, Leo doesn't really know what to make of it.

00:02:13.000 --> 00:02:15.000
But sort of like us, he can actually learn about it

00:02:15.000 --> 00:02:18.000
from watching Matt's reaction.

00:02:18.000 --> 00:02:20.000
(Video) Matt Berlin: Hello, Leo.

00:02:23.000 --> 00:02:26.000
Leo, this is Cookie Monster.

00:02:29.000 --> 00:02:32.000
Can you find Cookie Monster?

00:02:37.000 --> 00:02:40.000
Leo, Cookie Monster is very bad.

00:02:41.000 --> 00:02:43.000
He's very bad, Leo.

00:02:45.000 --> 00:02:48.000
Cookie Monster is very, very bad.

00:02:52.000 --> 00:02:54.000
He's a scary monster.

00:02:54.000 --> 00:02:56.000
He wants to get your cookies.

00:02:57.000 --> 00:02:59.000
(Laughter)

00:02:59.000 --> 00:03:02.000
CB: All right, so Leo and Cookie

00:03:02.000 --> 00:03:04.000
might have gotten off to a little bit of a rough start,

00:03:04.000 --> 00:03:07.000
but they get along great now.

00:03:07.000 --> 00:03:09.000
So what I've learned

00:03:09.000 --> 00:03:11.000
through building these systems

00:03:11.000 --> 00:03:13.000
is that robots are actually

00:03:13.000 --> 00:03:15.000
a really intriguing social technology,

00:03:15.000 --> 00:03:17.000
where it's actually their ability

00:03:17.000 --> 00:03:19.000
to push our social buttons

00:03:19.000 --> 00:03:21.000
and to interact with us like a partner

00:03:21.000 --> 00:03:24.000
that is a core part of their functionality.

00:03:24.000 --> 00:03:26.000
And with that shift in thinking, we can now start to imagine

00:03:26.000 --> 00:03:29.000
new questions, new possibilities for robots

00:03:29.000 --> 00:03:32.000
that we might not have thought about otherwise.

00:03:32.000 --> 00:03:34.000
But what do I mean when I say "push our social buttons?"

00:03:34.000 --> 00:03:36.000
Well, one of the things that we've learned

00:03:36.000 --> 00:03:38.000
is that, if we design these robots to communicate with us

00:03:38.000 --> 00:03:40.000
using the same body language,

00:03:40.000 --> 00:03:42.000
the same sort of non-verbal cues that people use --

00:03:42.000 --> 00:03:45.000
like Nexi, our humanoid robot, is doing here --

00:03:45.000 --> 00:03:47.000
what we find is that people respond to robots

00:03:47.000 --> 00:03:49.000
a lot like they respond to people.

00:03:49.000 --> 00:03:52.000
People use these cues to determine things like how persuasive someone is,

00:03:52.000 --> 00:03:54.000
how likable, how engaging,

00:03:54.000 --> 00:03:56.000
how trustworthy.

00:03:56.000 --> 00:03:58.000
It turns out it's the same for robots.

00:03:58.000 --> 00:04:00.000
It's turning out now

00:04:00.000 --> 00:04:03.000
that robots are actually becoming a really interesting new scientific tool

00:04:03.000 --> 00:04:05.000
to understand human behavior.

00:04:05.000 --> 00:04:08.000
To answer questions like, how is it that, from a brief encounter,

00:04:08.000 --> 00:04:11.000
we're able to make an estimate of how trustworthy another person is?

00:04:11.000 --> 00:04:14.000
Mimicry's believed to play a role, but how?

00:04:14.000 --> 00:04:17.000
Is it the mimicking of particular gestures that matters?

00:04:17.000 --> 00:04:19.000
It turns out it's really hard

00:04:19.000 --> 00:04:21.000
to learn this or understand this from watching people

00:04:21.000 --> 00:04:24.000
because when we interact we do all of these cues automatically.

00:04:24.000 --> 00:04:26.000
We can't carefully control them because they're subconscious for us.

00:04:26.000 --> 00:04:28.000
But with the robot, you can.

00:04:28.000 --> 00:04:30.000
And so in this video here --

00:04:30.000 --> 00:04:33.000
this is a video taken from David DeSteno's lab at Northeastern University.

00:04:33.000 --> 00:04:35.000
He's a psychologist we've been collaborating with.

00:04:35.000 --> 00:04:38.000
There's actually a scientist carefully controlling Nexi's cues

00:04:38.000 --> 00:04:41.000
to be able to study this question.

00:04:41.000 --> 00:04:43.000
And the bottom line is -- the reason why this works is

00:04:43.000 --> 00:04:45.000
because it turns out people just behave like people

00:04:45.000 --> 00:04:48.000
even when interacting with a robot.

00:04:48.000 --> 00:04:50.000
So given that key insight,

00:04:50.000 --> 00:04:52.000
we can now start to imagine

00:04:52.000 --> 00:04:55.000
new kinds of applications for robots.

00:04:55.000 --> 00:04:58.000
For instance, if robots do respond to our non-verbal cues,

00:04:58.000 --> 00:05:02.000
maybe they would be a cool, new communication technology.

00:05:02.000 --> 00:05:04.000
So imagine this:

00:05:04.000 --> 00:05:06.000
What about a robot accessory for your cellphone?

00:05:06.000 --> 00:05:08.000
You call your friend, she puts her handset in a robot,

00:05:08.000 --> 00:05:10.000
and, bam! You're a MeBot --

00:05:10.000 --> 00:05:13.000
you can make eye contact, you can talk with your friends,

00:05:13.000 --> 00:05:15.000
you can move around, you can gesture --

00:05:15.000 --> 00:05:18.000
maybe the next best thing to really being there, or is it?

00:05:18.000 --> 00:05:20.000
To explore this question,

00:05:20.000 --> 00:05:23.000
my student, Siggy Adalgeirsson, did a study

00:05:23.000 --> 00:05:26.000
where we brought human participants, people, into our lab

00:05:26.000 --> 00:05:28.000
to do a collaborative task

00:05:28.000 --> 00:05:30.000
with a remote collaborator.

00:05:30.000 --> 00:05:32.000
The task involved things

00:05:32.000 --> 00:05:34.000
like looking at a set of objects on the table,

00:05:34.000 --> 00:05:37.000
discussing them in terms of their importance and relevance to performing a certain task --

00:05:37.000 --> 00:05:39.000
this ended up being a survival task --

00:05:39.000 --> 00:05:41.000
and then rating them in terms

00:05:41.000 --> 00:05:43.000
of how valuable and important they thought they were.

00:05:43.000 --> 00:05:46.000
The remote collaborator was an experimenter from our group

00:05:46.000 --> 00:05:48.000
who used one of three different technologies

00:05:48.000 --> 00:05:50.000
to interact with the participants.

00:05:50.000 --> 00:05:52.000
The first was just the screen.

00:05:52.000 --> 00:05:55.000
This is just like video conferencing today.

00:05:55.000 --> 00:05:58.000
The next was to add mobility -- so, have the screen on a mobile base.

00:05:58.000 --> 00:06:01.000
This is like, if you're familiar with any of the telepresence robots today --

00:06:01.000 --> 00:06:04.000
this is mirroring that situation.

00:06:04.000 --> 00:06:06.000
And then the fully expressive MeBot.

00:06:06.000 --> 00:06:08.000
So after the interaction,

00:06:08.000 --> 00:06:11.000
we asked people to rate their quality of interaction

00:06:11.000 --> 00:06:13.000
with the technology, with a remote collaborator

00:06:13.000 --> 00:06:16.000
through this technology, in a number of different ways.

00:06:16.000 --> 00:06:18.000
We looked at psychological involvement --

00:06:18.000 --> 00:06:20.000
how much empathy did you feel for the other person?

00:06:20.000 --> 00:06:22.000
We looked at overall engagement.

00:06:22.000 --> 00:06:24.000
We looked at their desire to cooperate.

00:06:24.000 --> 00:06:27.000
And this is what we see when they use just the screen.

00:06:27.000 --> 00:06:30.000
It turns out, when you add mobility -- the ability to roll around the table --

00:06:30.000 --> 00:06:32.000
you get a little more of a boost.

00:06:32.000 --> 00:06:35.000
And you get even more of a boost when you add the full expression.

00:06:35.000 --> 00:06:37.000
So it seems like this physical, social embodiment

00:06:37.000 --> 00:06:39.000
actually really makes a difference.

00:06:39.000 --> 00:06:42.000
Now let's try to put this into a little bit of context.

00:06:42.000 --> 00:06:45.000
Today we know that families are living further and further apart,

00:06:45.000 --> 00:06:47.000
and that definitely takes a toll on family relationships

00:06:47.000 --> 00:06:49.000
and family bonds over distance.

00:06:49.000 --> 00:06:51.000
For me, I have three young boys,

00:06:51.000 --> 00:06:53.000
and I want them to have a really good relationship

00:06:53.000 --> 00:06:55.000
with their grandparents.

00:06:55.000 --> 00:06:57.000
But my parents live thousands of miles away,

00:06:57.000 --> 00:06:59.000
so they just don't get to see each other that often.

00:06:59.000 --> 00:07:01.000
We try Skype, we try phone calls,

00:07:01.000 --> 00:07:03.000
but my boys are little -- they don't really want to talk;

00:07:03.000 --> 00:07:05.000
they want to play.

00:07:05.000 --> 00:07:07.000
So I love the idea of thinking about robots

00:07:07.000 --> 00:07:10.000
as a new kind of distance-play technology.

00:07:10.000 --> 00:07:13.000
I imagine a time not too far from now --

00:07:13.000 --> 00:07:15.000
my mom can go to her computer,

00:07:15.000 --> 00:07:17.000
open up a browser and jack into a little robot.

00:07:17.000 --> 00:07:20.000
And as grandma-bot,

00:07:20.000 --> 00:07:22.000
she can now play, really play,

00:07:22.000 --> 00:07:24.000
with my sons, with her grandsons,

00:07:24.000 --> 00:07:27.000
in the real world with his real toys.

00:07:27.000 --> 00:07:29.000
I could imagine grandmothers being able to do social-plays

00:07:29.000 --> 00:07:31.000
with their granddaughters, with their friends,

00:07:31.000 --> 00:07:33.000
and to be able to share all kinds of other activities around the house,

00:07:33.000 --> 00:07:35.000
like sharing a bedtime story.

00:07:35.000 --> 00:07:37.000
And through this technology,

00:07:37.000 --> 00:07:39.000
being able to be an active participant

00:07:39.000 --> 00:07:41.000
in their grandchildren's lives

00:07:41.000 --> 00:07:43.000
in a way that's not possible today.

00:07:43.000 --> 00:07:45.000
Let's think about some other domains,

00:07:45.000 --> 00:07:47.000
like maybe health.

00:07:47.000 --> 00:07:49.000
So in the United States today,

00:07:49.000 --> 00:07:52.000
over 65 percent of people are either overweight or obese,

00:07:52.000 --> 00:07:54.000
and now it's a big problem with our children as well.

00:07:54.000 --> 00:07:56.000
And we know that as you get older in life,

00:07:56.000 --> 00:07:59.000
if you're obese when you're younger, that can lead to chronic diseases

00:07:59.000 --> 00:08:01.000
that not only reduce your quality of life,

00:08:01.000 --> 00:08:04.000
but are a tremendous economic burden on our health care system.

00:08:04.000 --> 00:08:06.000
But if robots can be engaging,

00:08:06.000 --> 00:08:08.000
if we like to cooperate with robots,

00:08:08.000 --> 00:08:10.000
if robots are persuasive,

00:08:10.000 --> 00:08:12.000
maybe a robot can help you

00:08:12.000 --> 00:08:14.000
maintain a diet and exercise program,

00:08:14.000 --> 00:08:17.000
maybe they can help you manage your weight.

00:08:17.000 --> 00:08:19.000
Sort of like a digital Jiminy --

00:08:19.000 --> 00:08:21.000
as in the well-known fairy tale --

00:08:21.000 --> 00:08:23.000
a kind of friendly, supportive presence that's always there

00:08:23.000 --> 00:08:25.000
to be able to help you make the right decision

00:08:25.000 --> 00:08:27.000
in the right way at the right time

00:08:27.000 --> 00:08:29.000
to help you form healthy habits.

00:08:29.000 --> 00:08:31.000
So we actually explored this idea in our lab.

00:08:31.000 --> 00:08:33.000
This is a robot, Autom.

00:08:33.000 --> 00:08:36.000
Cory Kidd developed this robot for his doctoral work.

00:08:36.000 --> 00:08:39.000
And it was designed to be a robot diet-and-exercise coach.

00:08:39.000 --> 00:08:41.000
It had a couple of simple non-verbal skills it could do.

00:08:41.000 --> 00:08:43.000
It could make eye contact with you.

00:08:43.000 --> 00:08:45.000
It could share information looking down at a screen.

00:08:45.000 --> 00:08:47.000
You'd use a screen interface to enter information,

00:08:47.000 --> 00:08:49.000
like how many calories you ate that day,

00:08:49.000 --> 00:08:51.000
how much exercise you got.

00:08:51.000 --> 00:08:53.000
And then it could help track that for you.

00:08:53.000 --> 00:08:55.000
And the robot spoke with a synthetic voice

00:08:55.000 --> 00:08:57.000
to engage you in a coaching dialogue

00:08:57.000 --> 00:08:59.000
modeled after trainers

00:08:59.000 --> 00:09:01.000
and patients and so forth.

00:09:01.000 --> 00:09:03.000
And it would build a working alliance with you

00:09:03.000 --> 00:09:05.000
through that dialogue.

00:09:05.000 --> 00:09:07.000
It could help you set goals and track your progress,

00:09:07.000 --> 00:09:09.000
and it would help motivate you.

00:09:09.000 --> 00:09:11.000
So an interesting question is,

00:09:11.000 --> 00:09:14.000
does the social embodiment really matter? Does it matter that it's a robot?

00:09:14.000 --> 00:09:17.000
Is it really just the quality of advice and information that matters?

00:09:17.000 --> 00:09:19.000
To explore that question,

00:09:19.000 --> 00:09:21.000
we did a study in the Boston area

00:09:21.000 --> 00:09:24.000
where we put one of three interventions in people's homes

00:09:24.000 --> 00:09:26.000
for a period of several weeks.

00:09:26.000 --> 00:09:29.000
One case was the robot you saw there, Autom.

00:09:29.000 --> 00:09:32.000
Another was a computer that ran the same touch-screen interface,

00:09:32.000 --> 00:09:34.000
ran exactly the same dialogues.

00:09:34.000 --> 00:09:36.000
The quality of advice was identical.

00:09:36.000 --> 00:09:38.000
And the third was just a pen and paper log,

00:09:38.000 --> 00:09:40.000
because that's the standard intervention you typically get

00:09:40.000 --> 00:09:43.000
when you start a diet-and-exercise program.

00:09:43.000 --> 00:09:46.000
So one of the things we really wanted to look at

00:09:46.000 --> 00:09:49.000
was not how much weight people lost,

00:09:49.000 --> 00:09:52.000
but really how long they interacted with the robot.

00:09:52.000 --> 00:09:55.000
Because the challenge is not losing weight, it's actually keeping it off.

00:09:55.000 --> 00:09:58.000
And the longer you could interact with one of these interventions,

00:09:58.000 --> 00:10:01.000
well that's indicative, potentially, of longer-term success.

00:10:01.000 --> 00:10:03.000
So the first thing I want to look at is how long,

00:10:03.000 --> 00:10:05.000
how long did people interact with these systems.

00:10:05.000 --> 00:10:07.000
It turns out that people interacted with the robot

00:10:07.000 --> 00:10:09.000
significantly more,

00:10:09.000 --> 00:10:12.000
even though the quality of the advice was identical to the computer.

00:10:13.000 --> 00:10:16.000
When it asked people to rate it on terms of the quality of the working alliance,

00:10:16.000 --> 00:10:18.000
people rated the robot higher

00:10:18.000 --> 00:10:20.000
and they trusted the robot more.

00:10:20.000 --> 00:10:22.000
(Laughter)

00:10:22.000 --> 00:10:24.000
And when you look at emotional engagement,

00:10:24.000 --> 00:10:26.000
it was completely different.

00:10:26.000 --> 00:10:28.000
People would name the robots.

00:10:28.000 --> 00:10:30.000
They would dress the robots.

00:10:30.000 --> 00:10:32.000
(Laughter)

00:10:32.000 --> 00:10:35.000
And even when we would come up to pick up the robots at the end of the study,

00:10:35.000 --> 00:10:37.000
they would come out to the car and say good-bye to the robots.

00:10:37.000 --> 00:10:39.000
They didn't do this with a computer.

00:10:39.000 --> 00:10:41.000
The last thing I want to talk about today

00:10:41.000 --> 00:10:43.000
is the future of children's media.

00:10:43.000 --> 00:10:46.000
We know that kids spend a lot of time behind screens today,

00:10:46.000 --> 00:10:49.000
whether it's television or computer games or whatnot.

00:10:49.000 --> 00:10:52.000
My sons, they love the screen. They love the screen.

00:10:52.000 --> 00:10:55.000
But I want them to play; as a mom, I want them to play,

00:10:55.000 --> 00:10:57.000
like, real-world play.

00:10:57.000 --> 00:11:00.000
And so I have a new project in my group I wanted to present to you today

00:11:00.000 --> 00:11:02.000
called Playtime Computing

00:11:02.000 --> 00:11:04.000
that's really trying to think about how we can take

00:11:04.000 --> 00:11:06.000
what's so engaging about digital media

00:11:06.000 --> 00:11:08.000
and literally bring it off the screen

00:11:08.000 --> 00:11:10.000
into the real world of the child,

00:11:10.000 --> 00:11:13.000
where it can take on many of the properties of real-world play.

00:11:14.000 --> 00:11:18.000
So here's the first exploration of this idea,

00:11:18.000 --> 00:11:21.000
where characters can be physical or virtual,

00:11:21.000 --> 00:11:23.000
and where the digital content

00:11:23.000 --> 00:11:25.000
can literally come off the screen

00:11:25.000 --> 00:11:27.000
into the world and back.

00:11:27.000 --> 00:11:29.000
I like to think of this

00:11:29.000 --> 00:11:31.000
as the Atari Pong

00:11:31.000 --> 00:11:33.000
of this blended-reality play.

00:11:33.000 --> 00:11:35.000
But we can push this idea further.

00:11:35.000 --> 00:11:37.000
What if --

00:11:37.000 --> 00:11:40.000
(Game) Nathan: Here it comes. Yay!

00:11:40.000 --> 00:11:43.000
CB: -- the character itself could come into your world?

00:11:43.000 --> 00:11:45.000
It turns out that kids love it

00:11:45.000 --> 00:11:48.000
when the character becomes real and enters into their world.

00:11:48.000 --> 00:11:50.000
And when it's in their world,

00:11:50.000 --> 00:11:52.000
they can relate to it and play with it in a way

00:11:52.000 --> 00:11:54.000
that's fundamentally different from how they play with it on the screen.

00:11:54.000 --> 00:11:56.000
Another important idea is this notion

00:11:56.000 --> 00:11:59.000
of persistence of character across realities.

00:11:59.000 --> 00:12:01.000
So changes that children make in the real world

00:12:01.000 --> 00:12:03.000
need to translate to the virtual world.

00:12:03.000 --> 00:12:06.000
So here, Nathan has changed the letter A to the number 2.

00:12:06.000 --> 00:12:08.000
You can imagine maybe these symbols

00:12:08.000 --> 00:12:11.000
give the characters special powers when it goes into the virtual world.

00:12:11.000 --> 00:12:14.000
So they are now sending the character back into that world.

00:12:14.000 --> 00:12:17.000
And now it's got number power.

00:12:17.000 --> 00:12:19.000
And then finally, what I've been trying to do here

00:12:19.000 --> 00:12:22.000
is create a really immersive experience for kids,

00:12:22.000 --> 00:12:25.000
where they really feel like they are part of that story,

00:12:25.000 --> 00:12:27.000
a part of that experience.

00:12:27.000 --> 00:12:29.000
And I really want to spark their imaginations

00:12:29.000 --> 00:12:32.000
the way mine was sparked as a little girl watching "Star Wars."

00:12:32.000 --> 00:12:34.000
But I want to do more than that.

00:12:34.000 --> 00:12:37.000
I actually want them to create those experiences.

00:12:37.000 --> 00:12:39.000
I want them to be able to literally build their imagination

00:12:39.000 --> 00:12:41.000
into these experiences and make them their own.

00:12:41.000 --> 00:12:43.000
So we've been exploring a lot of ideas

00:12:43.000 --> 00:12:45.000
in telepresence and mixed reality

00:12:45.000 --> 00:12:48.000
to literally allow kids to project their ideas into this space

00:12:48.000 --> 00:12:50.000
where other kids can interact with them

00:12:50.000 --> 00:12:52.000
and build upon them.

00:12:52.000 --> 00:12:55.000
I really want to come up with new ways of children's media

00:12:55.000 --> 00:12:58.000
that foster creativity and learning and innovation.

00:12:58.000 --> 00:13:01.000
I think that's very, very important.

00:13:01.000 --> 00:13:03.000
So this is a new project.

00:13:03.000 --> 00:13:05.000
We've invited a lot of kids into this space,

00:13:05.000 --> 00:13:08.000
and they think it's pretty cool.

00:13:08.000 --> 00:13:10.000
But I can tell you, the thing that they love the most

00:13:10.000 --> 00:13:12.000
is the robot.

00:13:12.000 --> 00:13:15.000
What they care about is the robot.

00:13:15.000 --> 00:13:18.000
Robots touch something deeply human within us.

00:13:18.000 --> 00:13:20.000
And so whether they're helping us

00:13:20.000 --> 00:13:22.000
to become creative and innovative,

00:13:22.000 --> 00:13:24.000
or whether they're helping us

00:13:24.000 --> 00:13:26.000
to feel more deeply connected despite distance,

00:13:26.000 --> 00:13:28.000
or whether they are our trusted sidekick

00:13:28.000 --> 00:13:30.000
who's helping us attain our personal goals

00:13:30.000 --> 00:13:32.000
in becoming our highest and best selves,

00:13:32.000 --> 00:13:35.000
for me, robots are all about people.

00:13:35.000 --> 00:13:37.000
Thank you.

00:13:37.000 --> 00:13:42.000
(Applause)

