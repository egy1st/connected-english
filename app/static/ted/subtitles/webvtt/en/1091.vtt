WEBVTT

00:00:00.000 --> 00:00:02.000
Mark Zuckerberg,

00:00:02.000 --> 00:00:05.000
a journalist was asking him a question about the news feed.

00:00:05.000 --> 00:00:07.000
And the journalist was asking him,

00:00:07.000 --> 00:00:09.000
"Why is this so important?"

00:00:09.000 --> 00:00:11.000
And Zuckerberg said,

00:00:11.000 --> 00:00:13.000
"A squirrel dying in your front yard

00:00:13.000 --> 00:00:16.000
may be more relevant to your interests right now

00:00:16.000 --> 00:00:19.000
than people dying in Africa."

00:00:19.000 --> 00:00:21.000
And I want to talk about

00:00:21.000 --> 00:00:24.000
what a Web based on that idea of relevance might look like.

00:00:25.000 --> 00:00:27.000
So when I was growing up

00:00:27.000 --> 00:00:29.000
in a really rural area in Maine,

00:00:29.000 --> 00:00:32.000
the Internet meant something very different to me.

00:00:32.000 --> 00:00:34.000
It meant a connection to the world.

00:00:34.000 --> 00:00:37.000
It meant something that would connect us all together.

00:00:37.000 --> 00:00:40.000
And I was sure that it was going to be great for democracy

00:00:40.000 --> 00:00:43.000
and for our society.

00:00:43.000 --> 00:00:45.000
But there's this shift

00:00:45.000 --> 00:00:47.000
in how information is flowing online,

00:00:47.000 --> 00:00:50.000
and it's invisible.

00:00:50.000 --> 00:00:52.000
And if we don't pay attention to it,

00:00:52.000 --> 00:00:55.000
it could be a real problem.

00:00:55.000 --> 00:00:58.000
So I first noticed this in a place I spend a lot of time --

00:00:58.000 --> 00:01:00.000
my Facebook page.

00:01:00.000 --> 00:01:03.000
I'm progressive, politically -- big surprise --

00:01:03.000 --> 00:01:05.000
but I've always gone out of my way to meet conservatives.

00:01:05.000 --> 00:01:07.000
I like hearing what they're thinking about;

00:01:07.000 --> 00:01:09.000
I like seeing what they link to;

00:01:09.000 --> 00:01:11.000
I like learning a thing or two.

00:01:11.000 --> 00:01:14.000
And so I was surprised when I noticed one day

00:01:14.000 --> 00:01:17.000
that the conservatives had disappeared from my Facebook feed.

00:01:18.000 --> 00:01:20.000
And what it turned out was going on

00:01:20.000 --> 00:01:24.000
was that Facebook was looking at which links I clicked on,

00:01:24.000 --> 00:01:26.000
and it was noticing that, actually,

00:01:26.000 --> 00:01:28.000
I was clicking more on my liberal friends' links

00:01:28.000 --> 00:01:31.000
than on my conservative friends' links.

00:01:31.000 --> 00:01:33.000
And without consulting me about it,

00:01:33.000 --> 00:01:35.000
it had edited them out.

00:01:35.000 --> 00:01:38.000
They disappeared.

00:01:39.000 --> 00:01:41.000
So Facebook isn't the only place

00:01:41.000 --> 00:01:43.000
that's doing this kind of invisible, algorithmic

00:01:43.000 --> 00:01:46.000
editing of the Web.

00:01:46.000 --> 00:01:48.000
Google's doing it too.

00:01:48.000 --> 00:01:51.000
If I search for something, and you search for something,

00:01:51.000 --> 00:01:53.000
even right now at the very same time,

00:01:53.000 --> 00:01:56.000
we may get very different search results.

00:01:56.000 --> 00:01:59.000
Even if you're logged out, one engineer told me,

00:01:59.000 --> 00:02:01.000
there are 57 signals

00:02:01.000 --> 00:02:04.000
that Google looks at --

00:02:04.000 --> 00:02:07.000
everything from what kind of computer you're on

00:02:07.000 --> 00:02:09.000
to what kind of browser you're using

00:02:09.000 --> 00:02:11.000
to where you're located --

00:02:11.000 --> 00:02:14.000
that it uses to personally tailor your query results.

00:02:14.000 --> 00:02:16.000
Think about it for a second:

00:02:16.000 --> 00:02:20.000
there is no standard Google anymore.

00:02:20.000 --> 00:02:23.000
And you know, the funny thing about this is that it's hard to see.

00:02:23.000 --> 00:02:25.000
You can't see how different your search results are

00:02:25.000 --> 00:02:27.000
from anyone else's.

00:02:27.000 --> 00:02:29.000
But a couple of weeks ago,

00:02:29.000 --> 00:02:32.000
I asked a bunch of friends to Google "Egypt"

00:02:32.000 --> 00:02:35.000
and to send me screen shots of what they got.

00:02:35.000 --> 00:02:38.000
So here's my friend Scott's screen shot.

00:02:39.000 --> 00:02:42.000
And here's my friend Daniel's screen shot.

00:02:42.000 --> 00:02:44.000
When you put them side-by-side,

00:02:44.000 --> 00:02:46.000
you don't even have to read the links

00:02:46.000 --> 00:02:48.000
to see how different these two pages are.

00:02:48.000 --> 00:02:50.000
But when you do read the links,

00:02:50.000 --> 00:02:53.000
it's really quite remarkable.

00:02:54.000 --> 00:02:57.000
Daniel didn't get anything about the protests in Egypt at all

00:02:57.000 --> 00:02:59.000
in his first page of Google results.

00:02:59.000 --> 00:03:01.000
Scott's results were full of them.

00:03:01.000 --> 00:03:03.000
And this was the big story of the day at that time.

00:03:03.000 --> 00:03:06.000
That's how different these results are becoming.

00:03:06.000 --> 00:03:09.000
So it's not just Google and Facebook either.

00:03:09.000 --> 00:03:11.000
This is something that's sweeping the Web.

00:03:11.000 --> 00:03:14.000
There are a whole host of companies that are doing this kind of personalization.

00:03:14.000 --> 00:03:17.000
Yahoo News, the biggest news site on the Internet,

00:03:17.000 --> 00:03:20.000
is now personalized -- different people get different things.

00:03:21.000 --> 00:03:24.000
Huffington Post, the Washington Post, the New York Times --

00:03:24.000 --> 00:03:27.000
all flirting with personalization in various ways.

00:03:27.000 --> 00:03:30.000
And this moves us very quickly

00:03:30.000 --> 00:03:32.000
toward a world in which

00:03:32.000 --> 00:03:36.000
the Internet is showing us what it thinks we want to see,

00:03:36.000 --> 00:03:39.000
but not necessarily what we need to see.

00:03:39.000 --> 00:03:42.000
As Eric Schmidt said,

00:03:42.000 --> 00:03:45.000
"It will be very hard for people to watch or consume something

00:03:45.000 --> 00:03:47.000
that has not in some sense

00:03:47.000 --> 00:03:50.000
been tailored for them."

00:03:50.000 --> 00:03:52.000
So I do think this is a problem.

00:03:52.000 --> 00:03:55.000
And I think, if you take all of these filters together,

00:03:55.000 --> 00:03:57.000
you take all these algorithms,

00:03:57.000 --> 00:04:00.000
you get what I call a filter bubble.

00:04:01.000 --> 00:04:04.000
And your filter bubble is your own personal,

00:04:04.000 --> 00:04:06.000
unique universe of information

00:04:06.000 --> 00:04:08.000
that you live in online.

00:04:08.000 --> 00:04:11.000
And what's in your filter bubble

00:04:11.000 --> 00:04:14.000
depends on who you are, and it depends on what you do.

00:04:14.000 --> 00:04:18.000
But the thing is that you don't decide what gets in.

00:04:18.000 --> 00:04:20.000
And more importantly,

00:04:20.000 --> 00:04:23.000
you don't actually see what gets edited out.

00:04:23.000 --> 00:04:25.000
So one of the problems with the filter bubble

00:04:25.000 --> 00:04:28.000
was discovered by some researchers at Netflix.

00:04:28.000 --> 00:04:31.000
And they were looking at the Netflix queues, and they noticed something kind of funny

00:04:31.000 --> 00:04:33.000
that a lot of us probably have noticed,

00:04:33.000 --> 00:04:35.000
which is there are some movies

00:04:35.000 --> 00:04:38.000
that just sort of zip right up and out to our houses.

00:04:38.000 --> 00:04:41.000
They enter the queue, they just zip right out.

00:04:41.000 --> 00:04:43.000
So "Iron Man" zips right out,

00:04:43.000 --> 00:04:45.000
and "Waiting for Superman"

00:04:45.000 --> 00:04:47.000
can wait for a really long time.

00:04:47.000 --> 00:04:49.000
What they discovered

00:04:49.000 --> 00:04:51.000
was that in our Netflix queues

00:04:51.000 --> 00:04:54.000
there's this epic struggle going on

00:04:54.000 --> 00:04:57.000
between our future aspirational selves

00:04:57.000 --> 00:05:00.000
and our more impulsive present selves.

00:05:00.000 --> 00:05:02.000
You know we all want to be someone

00:05:02.000 --> 00:05:04.000
who has watched "Rashomon,"

00:05:04.000 --> 00:05:06.000
but right now

00:05:06.000 --> 00:05:09.000
we want to watch "Ace Ventura" for the fourth time.

00:05:09.000 --> 00:05:12.000
(Laughter)

00:05:12.000 --> 00:05:14.000
So the best editing gives us a bit of both.

00:05:14.000 --> 00:05:16.000
It gives us a little bit of Justin Bieber

00:05:16.000 --> 00:05:18.000
and a little bit of Afghanistan.

00:05:18.000 --> 00:05:20.000
It gives us some information vegetables;

00:05:20.000 --> 00:05:23.000
it gives us some information dessert.

00:05:23.000 --> 00:05:25.000
And the challenge with these kinds of algorithmic filters,

00:05:25.000 --> 00:05:27.000
these personalized filters,

00:05:27.000 --> 00:05:29.000
is that, because they're mainly looking

00:05:29.000 --> 00:05:33.000
at what you click on first,

00:05:33.000 --> 00:05:37.000
it can throw off that balance.

00:05:37.000 --> 00:05:40.000
And instead of a balanced information diet,

00:05:40.000 --> 00:05:42.000
you can end up surrounded

00:05:42.000 --> 00:05:44.000
by information junk food.

00:05:44.000 --> 00:05:46.000
What this suggests

00:05:46.000 --> 00:05:49.000
is actually that we may have the story about the Internet wrong.

00:05:49.000 --> 00:05:51.000
In a broadcast society --

00:05:51.000 --> 00:05:53.000
this is how the founding mythology goes --

00:05:53.000 --> 00:05:55.000
in a broadcast society,

00:05:55.000 --> 00:05:57.000
there were these gatekeepers, the editors,

00:05:57.000 --> 00:06:00.000
and they controlled the flows of information.

00:06:00.000 --> 00:06:03.000
And along came the Internet and it swept them out of the way,

00:06:03.000 --> 00:06:05.000
and it allowed all of us to connect together,

00:06:05.000 --> 00:06:07.000
and it was awesome.

00:06:07.000 --> 00:06:10.000
But that's not actually what's happening right now.

00:06:11.000 --> 00:06:14.000
What we're seeing is more of a passing of the torch

00:06:14.000 --> 00:06:16.000
from human gatekeepers

00:06:16.000 --> 00:06:19.000
to algorithmic ones.

00:06:19.000 --> 00:06:22.000
And the thing is that the algorithms

00:06:22.000 --> 00:06:25.000
don't yet have the kind of embedded ethics

00:06:25.000 --> 00:06:28.000
that the editors did.

00:06:28.000 --> 00:06:31.000
So if algorithms are going to curate the world for us,

00:06:31.000 --> 00:06:34.000
if they're going to decide what we get to see and what we don't get to see,

00:06:34.000 --> 00:06:36.000
then we need to make sure

00:06:36.000 --> 00:06:39.000
that they're not just keyed to relevance.

00:06:39.000 --> 00:06:41.000
We need to make sure that they also show us things

00:06:41.000 --> 00:06:44.000
that are uncomfortable or challenging or important --

00:06:44.000 --> 00:06:46.000
this is what TED does --

00:06:46.000 --> 00:06:48.000
other points of view.

00:06:48.000 --> 00:06:50.000
And the thing is, we've actually been here before

00:06:50.000 --> 00:06:52.000
as a society.

00:06:53.000 --> 00:06:56.000
In 1915, it's not like newspapers were sweating a lot

00:06:56.000 --> 00:06:59.000
about their civic responsibilities.

00:06:59.000 --> 00:07:01.000
Then people noticed

00:07:01.000 --> 00:07:04.000
that they were doing something really important.

00:07:04.000 --> 00:07:06.000
That, in fact, you couldn't have

00:07:06.000 --> 00:07:08.000
a functioning democracy

00:07:08.000 --> 00:07:12.000
if citizens didn't get a good flow of information,

00:07:13.000 --> 00:07:16.000
that the newspapers were critical because they were acting as the filter,

00:07:16.000 --> 00:07:18.000
and then journalistic ethics developed.

00:07:18.000 --> 00:07:20.000
It wasn't perfect,

00:07:20.000 --> 00:07:23.000
but it got us through the last century.

00:07:23.000 --> 00:07:25.000
And so now,

00:07:25.000 --> 00:07:28.000
we're kind of back in 1915 on the Web.

00:07:29.000 --> 00:07:32.000
And we need the new gatekeepers

00:07:32.000 --> 00:07:34.000
to encode that kind of responsibility

00:07:34.000 --> 00:07:36.000
into the code that they're writing.

00:07:36.000 --> 00:07:39.000
I know that there are a lot of people here from Facebook and from Google --

00:07:39.000 --> 00:07:41.000
Larry and Sergey --

00:07:41.000 --> 00:07:43.000
people who have helped build the Web as it is,

00:07:43.000 --> 00:07:45.000
and I'm grateful for that.

00:07:45.000 --> 00:07:48.000
But we really need you to make sure

00:07:48.000 --> 00:07:51.000
that these algorithms have encoded in them

00:07:51.000 --> 00:07:54.000
a sense of the public life, a sense of civic responsibility.

00:07:54.000 --> 00:07:57.000
We need you to make sure that they're transparent enough

00:07:57.000 --> 00:07:59.000
that we can see what the rules are

00:07:59.000 --> 00:08:02.000
that determine what gets through our filters.

00:08:02.000 --> 00:08:04.000
And we need you to give us some control

00:08:04.000 --> 00:08:06.000
so that we can decide

00:08:06.000 --> 00:08:09.000
what gets through and what doesn't.

00:08:09.000 --> 00:08:11.000
Because I think

00:08:11.000 --> 00:08:13.000
we really need the Internet to be that thing

00:08:13.000 --> 00:08:15.000
that we all dreamed of it being.

00:08:15.000 --> 00:08:18.000
We need it to connect us all together.

00:08:18.000 --> 00:08:21.000
We need it to introduce us to new ideas

00:08:21.000 --> 00:08:24.000
and new people and different perspectives.

00:08:25.000 --> 00:08:27.000
And it's not going to do that

00:08:27.000 --> 00:08:30.000
if it leaves us all isolated in a Web of one.

00:08:30.000 --> 00:08:32.000
Thank you.

00:08:32.000 --> 00:08:43.000
(Applause)

