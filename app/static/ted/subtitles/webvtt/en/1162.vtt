WEBVTT

00:00:00.000 --> 00:00:02.000
Power.

00:00:02.000 --> 00:00:04.000
That is the word that comes to mind.

00:00:04.000 --> 00:00:06.000
We're the new technologists.

00:00:06.000 --> 00:00:09.000
We have a lot of data, so we have a lot of power.

00:00:09.000 --> 00:00:11.000
How much power do we have?

00:00:11.000 --> 00:00:14.000
Scene from a movie: "Apocalypse Now" -- great movie.

00:00:14.000 --> 00:00:17.000
We've got to get our hero, Captain Willard, to the mouth of the Nung River

00:00:17.000 --> 00:00:19.000
so he can go pursue Colonel Kurtz.

00:00:19.000 --> 00:00:21.000
The way we're going to do this is fly him in and drop him off.

00:00:21.000 --> 00:00:23.000
So the scene:

00:00:23.000 --> 00:00:26.000
the sky is filled with this fleet of helicopters carrying him in.

00:00:26.000 --> 00:00:28.000
And there's this loud, thrilling music in the background,

00:00:28.000 --> 00:00:30.000
this wild music.

00:00:30.000 --> 00:00:32.000
♫ Dum da ta da dum ♫

00:00:32.000 --> 00:00:34.000
♫ Dum da ta da dum ♫

00:00:34.000 --> 00:00:37.000
♫ Da ta da da ♫

00:00:37.000 --> 00:00:39.000
That's a lot of power.

00:00:39.000 --> 00:00:41.000
That's the kind of power I feel in this room.

00:00:41.000 --> 00:00:43.000
That's the kind of power we have

00:00:43.000 --> 00:00:45.000
because of all of the data that we have.

00:00:45.000 --> 00:00:47.000
Let's take an example.

00:00:47.000 --> 00:00:49.000
What can we do

00:00:49.000 --> 00:00:52.000
with just one person's data?

00:00:52.000 --> 00:00:54.000
What can we do

00:00:54.000 --> 00:00:56.000
with that guy's data?

00:00:56.000 --> 00:00:58.000
I can look at your financial records.

00:00:58.000 --> 00:01:00.000
I can tell if you pay your bills on time.

00:01:00.000 --> 00:01:02.000
I know if you're good to give a loan to.

00:01:02.000 --> 00:01:05.000
I can look at your medical records; I can see if your pump is still pumping --

00:01:05.000 --> 00:01:08.000
see if you're good to offer insurance to.

00:01:08.000 --> 00:01:10.000
I can look at your clicking patterns.

00:01:10.000 --> 00:01:13.000
When you come to my website, I actually know what you're going to do already

00:01:13.000 --> 00:01:15.000
because I've seen you visit millions of websites before.

00:01:15.000 --> 00:01:17.000
And I'm sorry to tell you,

00:01:17.000 --> 00:01:19.000
you're like a poker player, you have a tell.

00:01:19.000 --> 00:01:21.000
I can tell with data analysis what you're going to do

00:01:21.000 --> 00:01:23.000
before you even do it.

00:01:23.000 --> 00:01:26.000
I know what you like. I know who you are,

00:01:26.000 --> 00:01:28.000
and that's even before I look at your mail

00:01:28.000 --> 00:01:30.000
or your phone.

00:01:30.000 --> 00:01:32.000
Those are the kinds of things we can do

00:01:32.000 --> 00:01:35.000
with the data that we have.

00:01:35.000 --> 00:01:38.000
But I'm not actually here to talk about what we can do.

00:01:41.000 --> 00:01:44.000
I'm here to talk about what we should do.

00:01:45.000 --> 00:01:48.000
What's the right thing to do?

00:01:49.000 --> 00:01:51.000
Now I see some puzzled looks

00:01:51.000 --> 00:01:54.000
like, "Why are you asking us what's the right thing to do?

00:01:54.000 --> 00:01:57.000
We're just building this stuff. Somebody else is using it."

00:01:57.000 --> 00:02:00.000
Fair enough.

00:02:00.000 --> 00:02:02.000
But it brings me back.

00:02:02.000 --> 00:02:04.000
I think about World War II --

00:02:04.000 --> 00:02:06.000
some of our great technologists then,

00:02:06.000 --> 00:02:08.000
some of our great physicists,

00:02:08.000 --> 00:02:10.000
studying nuclear fission and fusion --

00:02:10.000 --> 00:02:12.000
just nuclear stuff.

00:02:12.000 --> 00:02:15.000
We gather together these physicists in Los Alamos

00:02:15.000 --> 00:02:18.000
to see what they'll build.

00:02:18.000 --> 00:02:21.000
We want the people building the technology

00:02:21.000 --> 00:02:24.000
thinking about what we should be doing with the technology.

00:02:26.000 --> 00:02:29.000
So what should we be doing with that guy's data?

00:02:29.000 --> 00:02:32.000
Should we be collecting it, gathering it,

00:02:32.000 --> 00:02:34.000
so we can make his online experience better?

00:02:34.000 --> 00:02:36.000
So we can make money?

00:02:36.000 --> 00:02:38.000
So we can protect ourselves

00:02:38.000 --> 00:02:40.000
if he was up to no good?

00:02:40.000 --> 00:02:43.000
Or should we respect his privacy,

00:02:43.000 --> 00:02:46.000
protect his dignity and leave him alone?

00:02:47.000 --> 00:02:50.000
Which one is it?

00:02:50.000 --> 00:02:52.000
How should we figure it out?

00:02:52.000 --> 00:02:55.000
I know: crowdsource. Let's crowdsource this.

00:02:56.000 --> 00:02:59.000
So to get people warmed up,

00:02:59.000 --> 00:03:01.000
let's start with an easy question --

00:03:01.000 --> 00:03:04.000
something I'm sure everybody here has an opinion about:

00:03:04.000 --> 00:03:06.000
iPhone versus Android.

00:03:06.000 --> 00:03:09.000
Let's do a show of hands -- iPhone.

00:03:09.000 --> 00:03:11.000
Uh huh.

00:03:11.000 --> 00:03:14.000
Android.

00:03:14.000 --> 00:03:16.000
You'd think with a bunch of smart people

00:03:16.000 --> 00:03:18.000
we wouldn't be such suckers just for the pretty phones.

00:03:18.000 --> 00:03:20.000
(Laughter)

00:03:20.000 --> 00:03:22.000
Next question,

00:03:22.000 --> 00:03:24.000
a little bit harder.

00:03:24.000 --> 00:03:26.000
Should we be collecting all of that guy's data

00:03:26.000 --> 00:03:28.000
to make his experiences better

00:03:28.000 --> 00:03:31.000
and to protect ourselves in case he's up to no good?

00:03:31.000 --> 00:03:33.000
Or should we leave him alone?

00:03:33.000 --> 00:03:36.000
Collect his data.

00:03:38.000 --> 00:03:41.000
Leave him alone.

00:03:41.000 --> 00:03:43.000
You're safe. It's fine.

00:03:43.000 --> 00:03:45.000
(Laughter)

00:03:45.000 --> 00:03:47.000
Okay, last question --

00:03:47.000 --> 00:03:49.000
harder question --

00:03:49.000 --> 00:03:52.000
when trying to evaluate

00:03:52.000 --> 00:03:55.000
what we should do in this case,

00:03:55.000 --> 00:03:59.000
should we use a Kantian deontological moral framework,

00:03:59.000 --> 00:04:02.000
or should we use a Millian consequentialist one?

00:04:04.000 --> 00:04:07.000
Kant.

00:04:07.000 --> 00:04:10.000
Mill.

00:04:10.000 --> 00:04:12.000
Not as many votes.

00:04:12.000 --> 00:04:15.000
(Laughter)

00:04:15.000 --> 00:04:18.000
Yeah, that's a terrifying result.

00:04:19.000 --> 00:04:23.000
Terrifying, because we have stronger opinions

00:04:23.000 --> 00:04:25.000
about our hand-held devices

00:04:25.000 --> 00:04:27.000
than about the moral framework

00:04:27.000 --> 00:04:29.000
we should use to guide our decisions.

00:04:29.000 --> 00:04:32.000
How do we know what to do with all the power we have

00:04:32.000 --> 00:04:35.000
if we don't have a moral framework?

00:04:35.000 --> 00:04:38.000
We know more about mobile operating systems,

00:04:38.000 --> 00:04:41.000
but what we really need is a moral operating system.

00:04:43.000 --> 00:04:45.000
What's a moral operating system?

00:04:45.000 --> 00:04:47.000
We all know right and wrong, right?

00:04:47.000 --> 00:04:49.000
You feel good when you do something right,

00:04:49.000 --> 00:04:51.000
you feel bad when you do something wrong.

00:04:51.000 --> 00:04:54.000
Our parents teach us that: praise with the good, scold with the bad.

00:04:54.000 --> 00:04:57.000
But how do we figure out what's right and wrong?

00:04:57.000 --> 00:05:00.000
And from day to day, we have the techniques that we use.

00:05:00.000 --> 00:05:03.000
Maybe we just follow our gut.

00:05:03.000 --> 00:05:06.000
Maybe we take a vote -- we crowdsource.

00:05:06.000 --> 00:05:08.000
Or maybe we punt --

00:05:08.000 --> 00:05:11.000
ask the legal department, see what they say.

00:05:11.000 --> 00:05:13.000
In other words, it's kind of random,

00:05:13.000 --> 00:05:15.000
kind of ad hoc,

00:05:15.000 --> 00:05:18.000
how we figure out what we should do.

00:05:18.000 --> 00:05:21.000
And maybe, if we want to be on surer footing,

00:05:21.000 --> 00:05:24.000
what we really want is a moral framework that will help guide us there,

00:05:24.000 --> 00:05:27.000
that will tell us what kinds of things are right and wrong in the first place,

00:05:27.000 --> 00:05:31.000
and how would we know in a given situation what to do.

00:05:31.000 --> 00:05:33.000
So let's get a moral framework.

00:05:33.000 --> 00:05:36.000
We're numbers people, living by numbers.

00:05:36.000 --> 00:05:38.000
How can we use numbers

00:05:38.000 --> 00:05:41.000
as the basis for a moral framework?

00:05:41.000 --> 00:05:44.000
I know a guy who did exactly that.

00:05:44.000 --> 00:05:47.000
A brilliant guy --

00:05:47.000 --> 00:05:50.000
he's been dead 2,500 years.

00:05:50.000 --> 00:05:52.000
Plato, that's right.

00:05:52.000 --> 00:05:54.000
Remember him -- old philosopher?

00:05:54.000 --> 00:05:57.000
You were sleeping during that class.

00:05:57.000 --> 00:05:59.000
And Plato, he had a lot of the same concerns that we did.

00:05:59.000 --> 00:06:01.000
He was worried about right and wrong.

00:06:01.000 --> 00:06:03.000
He wanted to know what is just.

00:06:03.000 --> 00:06:05.000
But he was worried that all we seem to be doing

00:06:05.000 --> 00:06:07.000
is trading opinions about this.

00:06:07.000 --> 00:06:10.000
He says something's just. She says something else is just.

00:06:10.000 --> 00:06:12.000
It's kind of convincing when he talks and when she talks too.

00:06:12.000 --> 00:06:14.000
I'm just going back and forth; I'm not getting anywhere.

00:06:14.000 --> 00:06:17.000
I don't want opinions; I want knowledge.

00:06:17.000 --> 00:06:20.000
I want to know the truth about justice --

00:06:20.000 --> 00:06:23.000
like we have truths in math.

00:06:23.000 --> 00:06:26.000
In math, we know the objective facts.

00:06:26.000 --> 00:06:28.000
Take a number, any number -- two.

00:06:28.000 --> 00:06:30.000
Favorite number. I love that number.

00:06:30.000 --> 00:06:32.000
There are truths about two.

00:06:32.000 --> 00:06:34.000
If you've got two of something,

00:06:34.000 --> 00:06:36.000
you add two more, you get four.

00:06:36.000 --> 00:06:38.000
That's true no matter what thing you're talking about.

00:06:38.000 --> 00:06:40.000
It's an objective truth about the form of two,

00:06:40.000 --> 00:06:42.000
the abstract form.

00:06:42.000 --> 00:06:44.000
When you have two of anything -- two eyes, two ears, two noses,

00:06:44.000 --> 00:06:46.000
just two protrusions --

00:06:46.000 --> 00:06:49.000
those all partake of the form of two.

00:06:49.000 --> 00:06:53.000
They all participate in the truths that two has.

00:06:53.000 --> 00:06:55.000
They all have two-ness in them.

00:06:55.000 --> 00:06:58.000
And therefore, it's not a matter of opinion.

00:06:58.000 --> 00:07:00.000
What if, Plato thought,

00:07:00.000 --> 00:07:02.000
ethics was like math?

00:07:02.000 --> 00:07:05.000
What if there were a pure form of justice?

00:07:05.000 --> 00:07:07.000
What if there are truths about justice,

00:07:07.000 --> 00:07:09.000
and you could just look around in this world

00:07:09.000 --> 00:07:11.000
and see which things participated,

00:07:11.000 --> 00:07:14.000
partook of that form of justice?

00:07:14.000 --> 00:07:17.000
Then you would know what was really just and what wasn't.

00:07:17.000 --> 00:07:19.000
It wouldn't be a matter

00:07:19.000 --> 00:07:22.000
of just opinion or just appearances.

00:07:22.000 --> 00:07:24.000
That's a stunning vision.

00:07:24.000 --> 00:07:27.000
I mean, think about that. How grand. How ambitious.

00:07:27.000 --> 00:07:29.000
That's as ambitious as we are.

00:07:29.000 --> 00:07:31.000
He wants to solve ethics.

00:07:31.000 --> 00:07:33.000
He wants objective truths.

00:07:33.000 --> 00:07:36.000
If you think that way,

00:07:36.000 --> 00:07:39.000
you have a Platonist moral framework.

00:07:39.000 --> 00:07:41.000
If you don't think that way,

00:07:41.000 --> 00:07:43.000
well, you have a lot of company in the history of Western philosophy,

00:07:43.000 --> 00:07:46.000
because the tidy idea, you know, people criticized it.

00:07:46.000 --> 00:07:49.000
Aristotle, in particular, he was not amused.

00:07:49.000 --> 00:07:52.000
He thought it was impractical.

00:07:52.000 --> 00:07:56.000
Aristotle said, "We should seek only so much precision in each subject

00:07:56.000 --> 00:07:58.000
as that subject allows."

00:07:58.000 --> 00:08:01.000
Aristotle thought ethics wasn't a lot like math.

00:08:01.000 --> 00:08:04.000
He thought ethics was a matter of making decisions in the here-and-now

00:08:04.000 --> 00:08:06.000
using our best judgment

00:08:06.000 --> 00:08:08.000
to find the right path.

00:08:08.000 --> 00:08:10.000
If you think that, Plato's not your guy.

00:08:10.000 --> 00:08:12.000
But don't give up.

00:08:12.000 --> 00:08:14.000
Maybe there's another way

00:08:14.000 --> 00:08:17.000
that we can use numbers as the basis of our moral framework.

00:08:18.000 --> 00:08:20.000
How about this:

00:08:20.000 --> 00:08:23.000
What if in any situation you could just calculate,

00:08:23.000 --> 00:08:25.000
look at the choices,

00:08:25.000 --> 00:08:28.000
measure out which one's better and know what to do?

00:08:28.000 --> 00:08:30.000
That sound familiar?

00:08:30.000 --> 00:08:33.000
That's a utilitarian moral framework.

00:08:33.000 --> 00:08:35.000
John Stuart Mill was a great advocate of this --

00:08:35.000 --> 00:08:37.000
nice guy besides --

00:08:37.000 --> 00:08:39.000
and only been dead 200 years.

00:08:39.000 --> 00:08:41.000
So basis of utilitarianism --

00:08:41.000 --> 00:08:43.000
I'm sure you're familiar at least.

00:08:43.000 --> 00:08:45.000
The three people who voted for Mill before are familiar with this.

00:08:45.000 --> 00:08:47.000
But here's the way it works.

00:08:47.000 --> 00:08:50.000
What if morals, what if what makes something moral

00:08:50.000 --> 00:08:52.000
is just a matter of if it maximizes pleasure

00:08:52.000 --> 00:08:54.000
and minimizes pain?

00:08:54.000 --> 00:08:57.000
It does something intrinsic to the act.

00:08:57.000 --> 00:08:59.000
It's not like its relation to some abstract form.

00:08:59.000 --> 00:09:01.000
It's just a matter of the consequences.

00:09:01.000 --> 00:09:03.000
You just look at the consequences

00:09:03.000 --> 00:09:05.000
and see if, overall, it's for the good or for the worse.

00:09:05.000 --> 00:09:07.000
That would be simple. Then we know what to do.

00:09:07.000 --> 00:09:09.000
Let's take an example.

00:09:09.000 --> 00:09:11.000
Suppose I go up

00:09:11.000 --> 00:09:13.000
and I say, "I'm going to take your phone."

00:09:13.000 --> 00:09:15.000
Not just because it rang earlier,

00:09:15.000 --> 00:09:18.000
but I'm going to take it because I made a little calculation.

00:09:18.000 --> 00:09:21.000
I thought, that guy looks suspicious.

00:09:21.000 --> 00:09:24.000
And what if he's been sending little messages to Bin Laden's hideout --

00:09:24.000 --> 00:09:26.000
or whoever took over after Bin Laden --

00:09:26.000 --> 00:09:29.000
and he's actually like a terrorist, a sleeper cell.

00:09:29.000 --> 00:09:32.000
I'm going to find that out, and when I find that out,

00:09:32.000 --> 00:09:35.000
I'm going to prevent a huge amount of damage that he could cause.

00:09:35.000 --> 00:09:38.000
That has a very high utility to prevent that damage.

00:09:38.000 --> 00:09:40.000
And compared to the little pain that it's going to cause --

00:09:40.000 --> 00:09:42.000
because it's going to be embarrassing when I'm looking on his phone

00:09:42.000 --> 00:09:45.000
and seeing that he has a Farmville problem and that whole bit --

00:09:45.000 --> 00:09:48.000
that's overwhelmed

00:09:48.000 --> 00:09:50.000
by the value of looking at the phone.

00:09:50.000 --> 00:09:52.000
If you feel that way,

00:09:52.000 --> 00:09:55.000
that's a utilitarian choice.

00:09:55.000 --> 00:09:58.000
But maybe you don't feel that way either.

00:09:58.000 --> 00:10:00.000
Maybe you think, it's his phone.

00:10:00.000 --> 00:10:02.000
It's wrong to take his phone

00:10:02.000 --> 00:10:04.000
because he's a person

00:10:04.000 --> 00:10:06.000
and he has rights and he has dignity,

00:10:06.000 --> 00:10:08.000
and we can't just interfere with that.

00:10:08.000 --> 00:10:10.000
He has autonomy.

00:10:10.000 --> 00:10:12.000
It doesn't matter what the calculations are.

00:10:12.000 --> 00:10:15.000
There are things that are intrinsically wrong --

00:10:15.000 --> 00:10:17.000
like lying is wrong,

00:10:17.000 --> 00:10:20.000
like torturing innocent children is wrong.

00:10:20.000 --> 00:10:23.000
Kant was very good on this point,

00:10:23.000 --> 00:10:25.000
and he said it a little better than I'll say it.

00:10:25.000 --> 00:10:27.000
He said we should use our reason

00:10:27.000 --> 00:10:30.000
to figure out the rules by which we should guide our conduct,

00:10:30.000 --> 00:10:33.000
and then it is our duty to follow those rules.

00:10:33.000 --> 00:10:36.000
It's not a matter of calculation.

00:10:36.000 --> 00:10:38.000
So let's stop.

00:10:38.000 --> 00:10:41.000
We're right in the thick of it, this philosophical thicket.

00:10:41.000 --> 00:10:44.000
And this goes on for thousands of years,

00:10:44.000 --> 00:10:46.000
because these are hard questions,

00:10:46.000 --> 00:10:48.000
and I've only got 15 minutes.

00:10:48.000 --> 00:10:50.000
So let's cut to the chase.

00:10:50.000 --> 00:10:54.000
How should we be making our decisions?

00:10:54.000 --> 00:10:57.000
Is it Plato, is it Aristotle, is it Kant, is it Mill?

00:10:57.000 --> 00:10:59.000
What should we be doing? What's the answer?

00:10:59.000 --> 00:11:02.000
What's the formula that we can use in any situation

00:11:02.000 --> 00:11:04.000
to determine what we should do,

00:11:04.000 --> 00:11:06.000
whether we should use that guy's data or not?

00:11:06.000 --> 00:11:09.000
What's the formula?

00:11:10.000 --> 00:11:12.000
There's not a formula.

00:11:14.000 --> 00:11:16.000
There's not a simple answer.

00:11:16.000 --> 00:11:19.000
Ethics is hard.

00:11:19.000 --> 00:11:22.000
Ethics requires thinking.

00:11:23.000 --> 00:11:25.000
And that's uncomfortable.

00:11:25.000 --> 00:11:27.000
I know; I spent a lot of my career

00:11:27.000 --> 00:11:29.000
in artificial intelligence,

00:11:29.000 --> 00:11:32.000
trying to build machines that could do some of this thinking for us,

00:11:32.000 --> 00:11:34.000
that could give us answers.

00:11:34.000 --> 00:11:36.000
But they can't.

00:11:36.000 --> 00:11:38.000
You can't just take human thinking

00:11:38.000 --> 00:11:40.000
and put it into a machine.

00:11:40.000 --> 00:11:43.000
We're the ones who have to do it.

00:11:43.000 --> 00:11:46.000
Happily, we're not machines, and we can do it.

00:11:46.000 --> 00:11:48.000
Not only can we think,

00:11:48.000 --> 00:11:50.000
we must.

00:11:50.000 --> 00:11:52.000
Hannah Arendt said,

00:11:52.000 --> 00:11:54.000
"The sad truth

00:11:54.000 --> 00:11:56.000
is that most evil done in this world

00:11:56.000 --> 00:11:58.000
is not done by people

00:11:58.000 --> 00:12:00.000
who choose to be evil.

00:12:00.000 --> 00:12:03.000
It arises from not thinking."

00:12:03.000 --> 00:12:07.000
That's what she called the "banality of evil."

00:12:07.000 --> 00:12:09.000
And the response to that

00:12:09.000 --> 00:12:11.000
is that we demand the exercise of thinking

00:12:11.000 --> 00:12:14.000
from every sane person.

00:12:14.000 --> 00:12:16.000
So let's do that. Let's think.

00:12:16.000 --> 00:12:19.000
In fact, let's start right now.

00:12:19.000 --> 00:12:22.000
Every person in this room do this:

00:12:22.000 --> 00:12:25.000
think of the last time you had a decision to make

00:12:25.000 --> 00:12:27.000
where you were worried to do the right thing,

00:12:27.000 --> 00:12:29.000
where you wondered, "What should I be doing?"

00:12:29.000 --> 00:12:31.000
Bring that to mind,

00:12:31.000 --> 00:12:33.000
and now reflect on that

00:12:33.000 --> 00:12:36.000
and say, "How did I come up that decision?

00:12:36.000 --> 00:12:39.000
What did I do? Did I follow my gut?

00:12:39.000 --> 00:12:41.000
Did I have somebody vote on it? Or did I punt to legal?"

00:12:41.000 --> 00:12:44.000
Or now we have a few more choices.

00:12:44.000 --> 00:12:46.000
"Did I evaluate what would be the highest pleasure

00:12:46.000 --> 00:12:48.000
like Mill would?

00:12:48.000 --> 00:12:51.000
Or like Kant, did I use reason to figure out what was intrinsically right?"

00:12:51.000 --> 00:12:54.000
Think about it. Really bring it to mind. This is important.

00:12:54.000 --> 00:12:56.000
It is so important

00:12:56.000 --> 00:12:58.000
we are going to spend 30 seconds of valuable TEDTalk time

00:12:58.000 --> 00:13:00.000
doing nothing but thinking about this.

00:13:00.000 --> 00:13:02.000
Are you ready? Go.

00:13:18.000 --> 00:13:21.000
Stop. Good work.

00:13:21.000 --> 00:13:23.000
What you just did,

00:13:23.000 --> 00:13:25.000
that's the first step towards taking responsibility

00:13:25.000 --> 00:13:28.000
for what we should do with all of our power.

00:13:30.000 --> 00:13:33.000
Now the next step -- try this.

00:13:34.000 --> 00:13:36.000
Go find a friend and explain to them

00:13:36.000 --> 00:13:38.000
how you made that decision.

00:13:38.000 --> 00:13:40.000
Not right now. Wait till I finish talking.

00:13:40.000 --> 00:13:42.000
Do it over lunch.

00:13:42.000 --> 00:13:45.000
And don't just find another technologist friend;

00:13:45.000 --> 00:13:47.000
find somebody different than you.

00:13:47.000 --> 00:13:49.000
Find an artist or a writer --

00:13:49.000 --> 00:13:52.000
or, heaven forbid, find a philosopher and talk to them.

00:13:52.000 --> 00:13:54.000
In fact, find somebody from the humanities.

00:13:54.000 --> 00:13:56.000
Why? Because they think about problems

00:13:56.000 --> 00:13:58.000
differently than we do as technologists.

00:13:58.000 --> 00:14:01.000
Just a few days ago, right across the street from here,

00:14:01.000 --> 00:14:03.000
there was hundreds of people gathered together.

00:14:03.000 --> 00:14:05.000
It was technologists and humanists

00:14:05.000 --> 00:14:07.000
at that big BiblioTech Conference.

00:14:07.000 --> 00:14:09.000
And they gathered together

00:14:09.000 --> 00:14:11.000
because the technologists wanted to learn

00:14:11.000 --> 00:14:14.000
what it would be like to think from a humanities perspective.

00:14:14.000 --> 00:14:16.000
You have someone from Google

00:14:16.000 --> 00:14:18.000
talking to someone who does comparative literature.

00:14:18.000 --> 00:14:21.000
You're thinking about the relevance of 17th century French theater --

00:14:21.000 --> 00:14:23.000
how does that bear upon venture capital?

00:14:23.000 --> 00:14:26.000
Well that's interesting. That's a different way of thinking.

00:14:26.000 --> 00:14:28.000
And when you think in that way,

00:14:28.000 --> 00:14:31.000
you become more sensitive to the human considerations,

00:14:31.000 --> 00:14:34.000
which are crucial to making ethical decisions.

00:14:34.000 --> 00:14:36.000
So imagine that right now

00:14:36.000 --> 00:14:38.000
you went and you found your musician friend.

00:14:38.000 --> 00:14:41.000
And you're telling him what we're talking about,

00:14:41.000 --> 00:14:43.000
about our whole data revolution and all this --

00:14:43.000 --> 00:14:45.000
maybe even hum a few bars of our theme music.

00:14:45.000 --> 00:14:48.000
♫ Dum ta da da dum dum ta da da dum ♫

00:14:48.000 --> 00:14:50.000
Well, your musician friend will stop you and say,

00:14:50.000 --> 00:14:52.000
"You know, the theme music

00:14:52.000 --> 00:14:54.000
for your data revolution,

00:14:54.000 --> 00:14:56.000
that's an opera, that's Wagner.

00:14:56.000 --> 00:14:58.000
It's based on Norse legend.

00:14:58.000 --> 00:15:00.000
It's Gods and mythical creatures

00:15:00.000 --> 00:15:03.000
fighting over magical jewelry."

00:15:04.000 --> 00:15:07.000
That's interesting.

00:15:07.000 --> 00:15:10.000
Now it's also a beautiful opera,

00:15:10.000 --> 00:15:13.000
and we're moved by that opera.

00:15:13.000 --> 00:15:15.000
We're moved because it's about the battle

00:15:15.000 --> 00:15:17.000
between good and evil,

00:15:17.000 --> 00:15:19.000
about right and wrong.

00:15:19.000 --> 00:15:21.000
And we care about right and wrong.

00:15:21.000 --> 00:15:24.000
We care what happens in that opera.

00:15:24.000 --> 00:15:27.000
We care what happens in "Apocalypse Now."

00:15:27.000 --> 00:15:29.000
And we certainly care

00:15:29.000 --> 00:15:31.000
what happens with our technologies.

00:15:31.000 --> 00:15:33.000
We have so much power today,

00:15:33.000 --> 00:15:36.000
it is up to us to figure out what to do,

00:15:36.000 --> 00:15:38.000
and that's the good news.

00:15:38.000 --> 00:15:41.000
We're the ones writing this opera.

00:15:41.000 --> 00:15:43.000
This is our movie.

00:15:43.000 --> 00:15:46.000
We figure out what will happen with this technology.

00:15:46.000 --> 00:15:49.000
We determine how this will all end.

00:15:49.000 --> 00:15:51.000
Thank you.

00:15:51.000 --> 00:15:56.000
(Applause)

