WEBVTT

00:00:00.000 --> 00:00:02.000
This is a photograph

00:00:02.000 --> 00:00:04.000
by the artist Michael Najjar,

00:00:04.000 --> 00:00:06.000
and it's real,

00:00:06.000 --> 00:00:08.000
in the sense that he went there to Argentina

00:00:08.000 --> 00:00:10.000
to take the photo.

00:00:10.000 --> 00:00:13.000
But it's also a fiction. There's a lot of work that went into it after that.

00:00:13.000 --> 00:00:15.000
And what he's done

00:00:15.000 --> 00:00:17.000
is he's actually reshaped, digitally,

00:00:17.000 --> 00:00:19.000
all of the contours of the mountains

00:00:19.000 --> 00:00:22.000
to follow the vicissitudes of the Dow Jones index.

00:00:22.000 --> 00:00:24.000
So what you see,

00:00:24.000 --> 00:00:26.000
that precipice, that high precipice with the valley,

00:00:26.000 --> 00:00:28.000
is the 2008 financial crisis.

00:00:28.000 --> 00:00:30.000
The photo was made

00:00:30.000 --> 00:00:32.000
when we were deep in the valley over there.

00:00:32.000 --> 00:00:34.000
I don't know where we are now.

00:00:34.000 --> 00:00:36.000
This is the Hang Seng index

00:00:36.000 --> 00:00:38.000
for Hong Kong.

00:00:38.000 --> 00:00:40.000
And similar topography.

00:00:40.000 --> 00:00:42.000
I wonder why.

00:00:42.000 --> 00:00:45.000
And this is art. This is metaphor.

00:00:45.000 --> 00:00:47.000
But I think the point is

00:00:47.000 --> 00:00:49.000
that this is metaphor with teeth,

00:00:49.000 --> 00:00:52.000
and it's with those teeth that I want to propose today

00:00:52.000 --> 00:00:54.000
that we rethink a little bit

00:00:54.000 --> 00:00:57.000
about the role of contemporary math --

00:00:57.000 --> 00:01:00.000
not just financial math, but math in general.

00:01:00.000 --> 00:01:02.000
That its transition

00:01:02.000 --> 00:01:05.000
from being something that we extract and derive from the world

00:01:05.000 --> 00:01:08.000
to something that actually starts to shape it --

00:01:08.000 --> 00:01:11.000
the world around us and the world inside us.

00:01:11.000 --> 00:01:13.000
And it's specifically algorithms,

00:01:13.000 --> 00:01:15.000
which are basically the math

00:01:15.000 --> 00:01:18.000
that computers use to decide stuff.

00:01:18.000 --> 00:01:20.000
They acquire the sensibility of truth

00:01:20.000 --> 00:01:22.000
because they repeat over and over again,

00:01:22.000 --> 00:01:25.000
and they ossify and calcify,

00:01:25.000 --> 00:01:27.000
and they become real.

00:01:27.000 --> 00:01:30.000
And I was thinking about this, of all places,

00:01:30.000 --> 00:01:33.000
on a transatlantic flight a couple of years ago,

00:01:33.000 --> 00:01:35.000
because I happened to be seated

00:01:35.000 --> 00:01:37.000
next to a Hungarian physicist about my age

00:01:37.000 --> 00:01:39.000
and we were talking

00:01:39.000 --> 00:01:41.000
about what life was like during the Cold War

00:01:41.000 --> 00:01:43.000
for physicists in Hungary.

00:01:43.000 --> 00:01:45.000
And I said, "So what were you doing?"

00:01:45.000 --> 00:01:47.000
And he said, "Well we were mostly breaking stealth."

00:01:47.000 --> 00:01:49.000
And I said, "That's a good job. That's interesting.

00:01:49.000 --> 00:01:51.000
How does that work?"

00:01:51.000 --> 00:01:53.000
And to understand that,

00:01:53.000 --> 00:01:56.000
you have to understand a little bit about how stealth works.

00:01:56.000 --> 00:01:59.000
And so -- this is an over-simplification --

00:01:59.000 --> 00:02:01.000
but basically, it's not like

00:02:01.000 --> 00:02:03.000
you can just pass a radar signal

00:02:03.000 --> 00:02:06.000
right through 156 tons of steel in the sky.

00:02:06.000 --> 00:02:09.000
It's not just going to disappear.

00:02:09.000 --> 00:02:12.000
But if you can take this big, massive thing,

00:02:12.000 --> 00:02:15.000
and you could turn it into

00:02:15.000 --> 00:02:17.000
a million little things --

00:02:17.000 --> 00:02:19.000
something like a flock of birds --

00:02:19.000 --> 00:02:21.000
well then the radar that's looking for that

00:02:21.000 --> 00:02:23.000
has to be able to see

00:02:23.000 --> 00:02:25.000
every flock of birds in the sky.

00:02:25.000 --> 00:02:29.000
And if you're a radar, that's a really bad job.

00:02:29.000 --> 00:02:32.000
And he said, "Yeah." He said, "But that's if you're a radar.

00:02:32.000 --> 00:02:34.000
So we didn't use a radar;

00:02:34.000 --> 00:02:37.000
we built a black box that was looking for electrical signals,

00:02:37.000 --> 00:02:40.000
electronic communication.

00:02:40.000 --> 00:02:43.000
And whenever we saw a flock of birds that had electronic communication,

00:02:43.000 --> 00:02:46.000
we thought, 'Probably has something to do with the Americans.'"

00:02:46.000 --> 00:02:48.000
And I said, "Yeah.

00:02:48.000 --> 00:02:50.000
That's good.

00:02:50.000 --> 00:02:52.000
So you've effectively negated

00:02:52.000 --> 00:02:54.000
60 years of aeronautic research.

00:02:54.000 --> 00:02:56.000
What's your act two?

00:02:56.000 --> 00:02:58.000
What do you do when you grow up?"

00:02:58.000 --> 00:03:00.000
And he said,

00:03:00.000 --> 00:03:02.000
"Well, financial services."

00:03:02.000 --> 00:03:04.000
And I said, "Oh."

00:03:04.000 --> 00:03:07.000
Because those had been in the news lately.

00:03:07.000 --> 00:03:09.000
And I said, "How does that work?"

00:03:09.000 --> 00:03:11.000
And he said, "Well there's 2,000 physicists on Wall Street now,

00:03:11.000 --> 00:03:13.000
and I'm one of them."

00:03:13.000 --> 00:03:16.000
And I said, "What's the black box for Wall Street?"

00:03:16.000 --> 00:03:18.000
And he said, "It's funny you ask that,

00:03:18.000 --> 00:03:21.000
because it's actually called black box trading.

00:03:21.000 --> 00:03:23.000
And it's also sometimes called algo trading,

00:03:23.000 --> 00:03:26.000
algorithmic trading."

00:03:26.000 --> 00:03:29.000
And algorithmic trading evolved in part

00:03:29.000 --> 00:03:32.000
because institutional traders have the same problems

00:03:32.000 --> 00:03:35.000
that the United States Air Force had,

00:03:35.000 --> 00:03:38.000
which is that they're moving these positions --

00:03:38.000 --> 00:03:40.000
whether it's Proctor &amp; Gamble or Accenture, whatever --

00:03:40.000 --> 00:03:42.000
they're moving a million shares of something

00:03:42.000 --> 00:03:44.000
through the market.

00:03:44.000 --> 00:03:46.000
And if they do that all at once,

00:03:46.000 --> 00:03:48.000
it's like playing poker and going all in right away.

00:03:48.000 --> 00:03:50.000
You just tip your hand.

00:03:50.000 --> 00:03:52.000
And so they have to find a way --

00:03:52.000 --> 00:03:54.000
and they use algorithms to do this --

00:03:54.000 --> 00:03:56.000
to break up that big thing

00:03:56.000 --> 00:03:58.000
into a million little transactions.

00:03:58.000 --> 00:04:00.000
And the magic and the horror of that

00:04:00.000 --> 00:04:02.000
is that the same math

00:04:02.000 --> 00:04:04.000
that you use to break up the big thing

00:04:04.000 --> 00:04:06.000
into a million little things

00:04:06.000 --> 00:04:08.000
can be used to find a million little things

00:04:08.000 --> 00:04:10.000
and sew them back together

00:04:10.000 --> 00:04:12.000
and figure out what's actually happening in the market.

00:04:12.000 --> 00:04:14.000
So if you need to have some image

00:04:14.000 --> 00:04:17.000
of what's happening in the stock market right now,

00:04:17.000 --> 00:04:19.000
what you can picture is a bunch of algorithms

00:04:19.000 --> 00:04:22.000
that are basically programmed to hide,

00:04:22.000 --> 00:04:25.000
and a bunch of algorithms that are programmed to go find them and act.

00:04:25.000 --> 00:04:28.000
And all of that's great, and it's fine.

00:04:28.000 --> 00:04:30.000
And that's 70 percent

00:04:30.000 --> 00:04:32.000
of the United States stock market,

00:04:32.000 --> 00:04:34.000
70 percent of the operating system

00:04:34.000 --> 00:04:37.000
formerly known as your pension,

00:04:37.000 --> 00:04:40.000
your mortgage.

00:04:40.000 --> 00:04:42.000
And what could go wrong?

00:04:42.000 --> 00:04:44.000
What could go wrong

00:04:44.000 --> 00:04:46.000
is that a year ago,

00:04:46.000 --> 00:04:49.000
nine percent of the entire market just disappears in five minutes,

00:04:49.000 --> 00:04:52.000
and they called it the Flash Crash of 2:45.

00:04:52.000 --> 00:04:55.000
All of a sudden, nine percent just goes away,

00:04:55.000 --> 00:04:57.000
and nobody to this day

00:04:57.000 --> 00:04:59.000
can even agree on what happened

00:04:59.000 --> 00:05:02.000
because nobody ordered it, nobody asked for it.

00:05:02.000 --> 00:05:05.000
Nobody had any control over what was actually happening.

00:05:05.000 --> 00:05:07.000
All they had

00:05:07.000 --> 00:05:09.000
was just a monitor in front of them

00:05:09.000 --> 00:05:11.000
that had the numbers on it

00:05:11.000 --> 00:05:13.000
and just a red button

00:05:13.000 --> 00:05:15.000
that said, "Stop."

00:05:15.000 --> 00:05:17.000
And that's the thing,

00:05:17.000 --> 00:05:19.000
is that we're writing things,

00:05:19.000 --> 00:05:22.000
we're writing these things that we can no longer read.

00:05:22.000 --> 00:05:24.000
And we've rendered something

00:05:24.000 --> 00:05:26.000
illegible,

00:05:26.000 --> 00:05:29.000
and we've lost the sense

00:05:29.000 --> 00:05:31.000
of what's actually happening

00:05:31.000 --> 00:05:33.000
in this world that we've made.

00:05:33.000 --> 00:05:35.000
And we're starting to make our way.

00:05:35.000 --> 00:05:38.000
There's a company in Boston called Nanex,

00:05:38.000 --> 00:05:40.000
and they use math and magic

00:05:40.000 --> 00:05:42.000
and I don't know what,

00:05:42.000 --> 00:05:44.000
and they reach into all the market data

00:05:44.000 --> 00:05:47.000
and they find, actually sometimes, some of these algorithms.

00:05:47.000 --> 00:05:50.000
And when they find them they pull them out

00:05:50.000 --> 00:05:53.000
and they pin them to the wall like butterflies.

00:05:53.000 --> 00:05:55.000
And they do what we've always done

00:05:55.000 --> 00:05:58.000
when confronted with huge amounts of data that we don't understand --

00:05:58.000 --> 00:06:00.000
which is that they give them a name

00:06:00.000 --> 00:06:02.000
and a story.

00:06:02.000 --> 00:06:04.000
So this is one that they found,

00:06:04.000 --> 00:06:08.000
they called the Knife,

00:06:08.000 --> 00:06:10.000
the Carnival,

00:06:10.000 --> 00:06:14.000
the Boston Shuffler,

00:06:14.000 --> 00:06:16.000
Twilight.

00:06:16.000 --> 00:06:18.000
And the gag is

00:06:18.000 --> 00:06:21.000
that, of course, these aren't just running through the market.

00:06:21.000 --> 00:06:24.000
You can find these kinds of things wherever you look,

00:06:24.000 --> 00:06:26.000
once you learn how to look for them.

00:06:26.000 --> 00:06:29.000
You can find it here: this book about flies

00:06:29.000 --> 00:06:31.000
that you may have been looking at on Amazon.

00:06:31.000 --> 00:06:33.000
You may have noticed it

00:06:33.000 --> 00:06:35.000
when its price started at 1.7 million dollars.

00:06:35.000 --> 00:06:37.000
It's out of print -- still ...

00:06:37.000 --> 00:06:39.000
(Laughter)

00:06:39.000 --> 00:06:42.000
If you had bought it at 1.7, it would have been a bargain.

00:06:42.000 --> 00:06:44.000
A few hours later, it had gone up

00:06:44.000 --> 00:06:46.000
to 23.6 million dollars,

00:06:46.000 --> 00:06:48.000
plus shipping and handling.

00:06:48.000 --> 00:06:50.000
And the question is:

00:06:50.000 --> 00:06:52.000
Nobody was buying or selling anything; what was happening?

00:06:52.000 --> 00:06:54.000
And you see this behavior on Amazon

00:06:54.000 --> 00:06:56.000
as surely as you see it on Wall Street.

00:06:56.000 --> 00:06:58.000
And when you see this kind of behavior,

00:06:58.000 --> 00:07:00.000
what you see is the evidence

00:07:00.000 --> 00:07:02.000
of algorithms in conflict,

00:07:02.000 --> 00:07:04.000
algorithms locked in loops with each other,

00:07:04.000 --> 00:07:06.000
without any human oversight,

00:07:06.000 --> 00:07:09.000
without any adult supervision

00:07:09.000 --> 00:07:12.000
to say, "Actually, 1.7 million is plenty."

00:07:12.000 --> 00:07:15.000
(Laughter)

00:07:15.000 --> 00:07:18.000
And as with Amazon, so it is with Netflix.

00:07:18.000 --> 00:07:20.000
And so Netflix has gone through

00:07:20.000 --> 00:07:22.000
several different algorithms over the years.

00:07:22.000 --> 00:07:25.000
They started with Cinematch, and they've tried a bunch of others --

00:07:25.000 --> 00:07:27.000
there's Dinosaur Planet; there's Gravity.

00:07:27.000 --> 00:07:29.000
They're using Pragmatic Chaos now.

00:07:29.000 --> 00:07:31.000
Pragmatic Chaos is, like all of Netflix algorithms,

00:07:31.000 --> 00:07:33.000
trying to do the same thing.

00:07:33.000 --> 00:07:35.000
It's trying to get a grasp on you,

00:07:35.000 --> 00:07:37.000
on the firmware inside the human skull,

00:07:37.000 --> 00:07:39.000
so that it can recommend what movie

00:07:39.000 --> 00:07:41.000
you might want to watch next --

00:07:41.000 --> 00:07:44.000
which is a very, very difficult problem.

00:07:44.000 --> 00:07:46.000
But the difficulty of the problem

00:07:46.000 --> 00:07:49.000
and the fact that we don't really quite have it down,

00:07:49.000 --> 00:07:51.000
it doesn't take away

00:07:51.000 --> 00:07:53.000
from the effects Pragmatic Chaos has.

00:07:53.000 --> 00:07:56.000
Pragmatic Chaos, like all Netflix algorithms,

00:07:56.000 --> 00:07:58.000
determines, in the end,

00:07:58.000 --> 00:08:00.000
60 percent

00:08:00.000 --> 00:08:02.000
of what movies end up being rented.

00:08:02.000 --> 00:08:04.000
So one piece of code

00:08:04.000 --> 00:08:07.000
with one idea about you

00:08:07.000 --> 00:08:10.000
is responsible for 60 percent of those movies.

00:08:10.000 --> 00:08:12.000
But what if you could rate those movies

00:08:12.000 --> 00:08:14.000
before they get made?

00:08:14.000 --> 00:08:16.000
Wouldn't that be handy?

00:08:16.000 --> 00:08:19.000
Well, a few data scientists from the U.K. are in Hollywood,

00:08:19.000 --> 00:08:21.000
and they have "story algorithms" --

00:08:21.000 --> 00:08:23.000
a company called Epagogix.

00:08:23.000 --> 00:08:26.000
And you can run your script through there,

00:08:26.000 --> 00:08:28.000
and they can tell you, quantifiably,

00:08:28.000 --> 00:08:30.000
that that's a 30 million dollar movie

00:08:30.000 --> 00:08:32.000
or a 200 million dollar movie.

00:08:32.000 --> 00:08:34.000
And the thing is, is that this isn't Google.

00:08:34.000 --> 00:08:36.000
This isn't information.

00:08:36.000 --> 00:08:38.000
These aren't financial stats; this is culture.

00:08:38.000 --> 00:08:40.000
And what you see here,

00:08:40.000 --> 00:08:42.000
or what you don't really see normally,

00:08:42.000 --> 00:08:46.000
is that these are the physics of culture.

00:08:46.000 --> 00:08:48.000
And if these algorithms,

00:08:48.000 --> 00:08:50.000
like the algorithms on Wall Street,

00:08:50.000 --> 00:08:53.000
just crashed one day and went awry,

00:08:53.000 --> 00:08:55.000
how would we know?

00:08:55.000 --> 00:08:57.000
What would it look like?

00:08:57.000 --> 00:09:00.000
And they're in your house. They're in your house.

00:09:00.000 --> 00:09:02.000
These are two algorithms competing for your living room.

00:09:02.000 --> 00:09:04.000
These are two different cleaning robots

00:09:04.000 --> 00:09:07.000
that have very different ideas about what clean means.

00:09:07.000 --> 00:09:09.000
And you can see it

00:09:09.000 --> 00:09:12.000
if you slow it down and attach lights to them,

00:09:12.000 --> 00:09:15.000
and they're sort of like secret architects in your bedroom.

00:09:15.000 --> 00:09:18.000
And the idea that architecture itself

00:09:18.000 --> 00:09:20.000
is somehow subject to algorithmic optimization

00:09:20.000 --> 00:09:22.000
is not far-fetched.

00:09:22.000 --> 00:09:25.000
It's super-real and it's happening around you.

00:09:25.000 --> 00:09:27.000
You feel it most

00:09:27.000 --> 00:09:29.000
when you're in a sealed metal box,

00:09:29.000 --> 00:09:31.000
a new-style elevator;

00:09:31.000 --> 00:09:33.000
they're called destination-control elevators.

00:09:33.000 --> 00:09:36.000
These are the ones where you have to press what floor you're going to go to

00:09:36.000 --> 00:09:38.000
before you get in the elevator.

00:09:38.000 --> 00:09:40.000
And it uses what's called a bin-packing algorithm.

00:09:40.000 --> 00:09:42.000
So none of this mishegas

00:09:42.000 --> 00:09:44.000
of letting everybody go into whatever car they want.

00:09:44.000 --> 00:09:46.000
Everybody who wants to go to the 10th floor goes into car two,

00:09:46.000 --> 00:09:49.000
and everybody who wants to go to the third floor goes into car five.

00:09:49.000 --> 00:09:51.000
And the problem with that

00:09:51.000 --> 00:09:53.000
is that people freak out.

00:09:53.000 --> 00:09:55.000
People panic.

00:09:55.000 --> 00:09:57.000
And you see why. You see why.

00:09:57.000 --> 00:09:59.000
It's because the elevator

00:09:59.000 --> 00:10:02.000
is missing some important instrumentation, like the buttons.

00:10:02.000 --> 00:10:04.000
(Laughter)

00:10:04.000 --> 00:10:06.000
Like the things that people use.

00:10:06.000 --> 00:10:08.000
All it has

00:10:08.000 --> 00:10:11.000
is just the number that moves up or down

00:10:11.000 --> 00:10:14.000
and that red button that says, "Stop."

00:10:14.000 --> 00:10:17.000
And this is what we're designing for.

00:10:17.000 --> 00:10:19.000
We're designing

00:10:19.000 --> 00:10:21.000
for this machine dialect.

00:10:21.000 --> 00:10:24.000
And how far can you take that? How far can you take it?

00:10:24.000 --> 00:10:26.000
You can take it really, really far.

00:10:26.000 --> 00:10:29.000
So let me take it back to Wall Street.

00:10:30.000 --> 00:10:32.000
Because the algorithms of Wall Street

00:10:32.000 --> 00:10:35.000
are dependent on one quality above all else,

00:10:35.000 --> 00:10:37.000
which is speed.

00:10:37.000 --> 00:10:40.000
And they operate on milliseconds and microseconds.

00:10:40.000 --> 00:10:42.000
And just to give you a sense of what microseconds are,

00:10:42.000 --> 00:10:44.000
it takes you 500,000 microseconds

00:10:44.000 --> 00:10:46.000
just to click a mouse.

00:10:46.000 --> 00:10:48.000
But if you're a Wall Street algorithm

00:10:48.000 --> 00:10:50.000
and you're five microseconds behind,

00:10:50.000 --> 00:10:52.000
you're a loser.

00:10:52.000 --> 00:10:54.000
So if you were an algorithm,

00:10:54.000 --> 00:10:57.000
you'd look for an architect like the one that I met in Frankfurt

00:10:57.000 --> 00:10:59.000
who was hollowing out a skyscraper --

00:10:59.000 --> 00:11:02.000
throwing out all the furniture, all the infrastructure for human use,

00:11:02.000 --> 00:11:05.000
and just running steel on the floors

00:11:05.000 --> 00:11:08.000
to get ready for the stacks of servers to go in --

00:11:08.000 --> 00:11:10.000
all so an algorithm

00:11:10.000 --> 00:11:13.000
could get close to the Internet.

00:11:13.000 --> 00:11:16.000
And you think of the Internet as this kind of distributed system.

00:11:16.000 --> 00:11:19.000
And of course, it is, but it's distributed from places.

00:11:19.000 --> 00:11:21.000
In New York, this is where it's distributed from:

00:11:21.000 --> 00:11:23.000
the Carrier Hotel

00:11:23.000 --> 00:11:25.000
located on Hudson Street.

00:11:25.000 --> 00:11:28.000
And this is really where the wires come right up into the city.

00:11:28.000 --> 00:11:32.000
And the reality is that the further away you are from that,

00:11:32.000 --> 00:11:34.000
you're a few microseconds behind every time.

00:11:34.000 --> 00:11:36.000
These guys down on Wall Street,

00:11:36.000 --> 00:11:38.000
Marco Polo and Cherokee Nation,

00:11:38.000 --> 00:11:40.000
they're eight microseconds

00:11:40.000 --> 00:11:42.000
behind all these guys

00:11:42.000 --> 00:11:46.000
going into the empty buildings being hollowed out

00:11:46.000 --> 00:11:48.000
up around the Carrier Hotel.

00:11:48.000 --> 00:11:51.000
And that's going to keep happening.

00:11:51.000 --> 00:11:53.000
We're going to keep hollowing them out,

00:11:53.000 --> 00:11:56.000
because you, inch for inch

00:11:56.000 --> 00:11:59.000
and pound for pound and dollar for dollar,

00:11:59.000 --> 00:12:02.000
none of you could squeeze revenue out of that space

00:12:02.000 --> 00:12:05.000
like the Boston Shuffler could.

00:12:05.000 --> 00:12:07.000
But if you zoom out,

00:12:07.000 --> 00:12:09.000
if you zoom out,

00:12:09.000 --> 00:12:13.000
you would see an 825-mile trench

00:12:13.000 --> 00:12:15.000
between New York City and Chicago

00:12:15.000 --> 00:12:17.000
that's been built over the last few years

00:12:17.000 --> 00:12:20.000
by a company called Spread Networks.

00:12:20.000 --> 00:12:22.000
This is a fiber optic cable

00:12:22.000 --> 00:12:24.000
that was laid between those two cities

00:12:24.000 --> 00:12:27.000
to just be able to traffic one signal

00:12:27.000 --> 00:12:30.000
37 times faster than you can click a mouse --

00:12:30.000 --> 00:12:33.000
just for these algorithms,

00:12:33.000 --> 00:12:36.000
just for the Carnival and the Knife.

00:12:36.000 --> 00:12:38.000
And when you think about this,

00:12:38.000 --> 00:12:40.000
that we're running through the United States

00:12:40.000 --> 00:12:43.000
with dynamite and rock saws

00:12:43.000 --> 00:12:45.000
so that an algorithm can close the deal

00:12:45.000 --> 00:12:48.000
three microseconds faster,

00:12:48.000 --> 00:12:50.000
all for a communications framework

00:12:50.000 --> 00:12:54.000
that no human will ever know,

00:12:54.000 --> 00:12:57.000
that's a kind of manifest destiny;

00:12:57.000 --> 00:13:00.000
and we'll always look for a new frontier.

00:13:00.000 --> 00:13:03.000
Unfortunately, we have our work cut out for us.

00:13:03.000 --> 00:13:05.000
This is just theoretical.

00:13:05.000 --> 00:13:07.000
This is some mathematicians at MIT.

00:13:07.000 --> 00:13:09.000
And the truth is I don't really understand

00:13:09.000 --> 00:13:11.000
a lot of what they're talking about.

00:13:11.000 --> 00:13:14.000
It involves light cones and quantum entanglement,

00:13:14.000 --> 00:13:16.000
and I don't really understand any of that.

00:13:16.000 --> 00:13:18.000
But I can read this map,

00:13:18.000 --> 00:13:20.000
and what this map says

00:13:20.000 --> 00:13:23.000
is that, if you're trying to make money on the markets where the red dots are,

00:13:23.000 --> 00:13:25.000
that's where people are, where the cities are,

00:13:25.000 --> 00:13:28.000
you're going to have to put the servers where the blue dots are

00:13:28.000 --> 00:13:30.000
to do that most effectively.

00:13:30.000 --> 00:13:33.000
And the thing that you might have noticed about those blue dots

00:13:33.000 --> 00:13:36.000
is that a lot of them are in the middle of the ocean.

00:13:36.000 --> 00:13:39.000
So that's what we'll do: we'll build bubbles or something,

00:13:39.000 --> 00:13:41.000
or platforms.

00:13:41.000 --> 00:13:43.000
We'll actually part the water

00:13:43.000 --> 00:13:45.000
to pull money out of the air,

00:13:45.000 --> 00:13:47.000
because it's a bright future

00:13:47.000 --> 00:13:49.000
if you're an algorithm.

00:13:49.000 --> 00:13:51.000
(Laughter)

00:13:51.000 --> 00:13:54.000
And it's not the money that's so interesting actually.

00:13:54.000 --> 00:13:56.000
It's what the money motivates,

00:13:56.000 --> 00:13:58.000
that we're actually terraforming

00:13:58.000 --> 00:14:00.000
the Earth itself

00:14:00.000 --> 00:14:02.000
with this kind of algorithmic efficiency.

00:14:02.000 --> 00:14:04.000
And in that light,

00:14:04.000 --> 00:14:06.000
you go back

00:14:06.000 --> 00:14:08.000
and you look at Michael Najjar's photographs,

00:14:08.000 --> 00:14:11.000
and you realize that they're not metaphor, they're prophecy.

00:14:11.000 --> 00:14:13.000
They're prophecy

00:14:13.000 --> 00:14:17.000
for the kind of seismic, terrestrial effects

00:14:17.000 --> 00:14:19.000
of the math that we're making.

00:14:19.000 --> 00:14:22.000
And the landscape was always made

00:14:22.000 --> 00:14:25.000
by this sort of weird, uneasy collaboration

00:14:25.000 --> 00:14:28.000
between nature and man.

00:14:28.000 --> 00:14:31.000
But now there's this third co-evolutionary force: algorithms --

00:14:31.000 --> 00:14:34.000
the Boston Shuffler, the Carnival.

00:14:34.000 --> 00:14:37.000
And we will have to understand those as nature,

00:14:37.000 --> 00:14:39.000
and in a way, they are.

00:14:39.000 --> 00:14:41.000
Thank you.

00:14:41.000 --> 00:15:01.000
(Applause)

