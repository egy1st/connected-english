WEBVTT

00:00:00.000 --> 00:00:03.000
I didn't always love unintended consequences,

00:00:03.000 --> 00:00:05.000
but I've really learned to appreciate them.

00:00:05.000 --> 00:00:07.000
I've learned that they're really the essence

00:00:07.000 --> 00:00:09.000
of what makes for progress,

00:00:09.000 --> 00:00:12.000
even when they seem to be terrible.

00:00:12.000 --> 00:00:14.000
And I'd like to review

00:00:14.000 --> 00:00:17.000
just how unintended consequences

00:00:17.000 --> 00:00:20.000
play the part that they do.

00:00:20.000 --> 00:00:25.000
Let's go to 40,000 years before the present,

00:00:25.000 --> 00:00:29.000
to the time of the cultural explosion,

00:00:29.000 --> 00:00:34.000
when music, art, technology,

00:00:34.000 --> 00:00:36.000
so many of the things that we're enjoying today,

00:00:36.000 --> 00:00:39.000
so many of the things that are being demonstrated at TED

00:00:39.000 --> 00:00:41.000
were born.

00:00:41.000 --> 00:00:44.000
And the anthropologist Randall White

00:00:44.000 --> 00:00:47.000
has made a very interesting observation:

00:00:47.000 --> 00:00:49.000
that if our ancestors

00:00:49.000 --> 00:00:51.000
40,000 years ago

00:00:51.000 --> 00:00:54.000
had been able to see

00:00:54.000 --> 00:00:56.000
what they had done,

00:00:56.000 --> 00:00:58.000
they wouldn't have really understood it.

00:00:58.000 --> 00:01:00.000
They were responding

00:01:00.000 --> 00:01:03.000
to immediate concerns.

00:01:03.000 --> 00:01:05.000
They were making it possible for us

00:01:05.000 --> 00:01:07.000
to do what they do,

00:01:07.000 --> 00:01:09.000
and yet, they didn't really understand

00:01:09.000 --> 00:01:11.000
how they did it.

00:01:11.000 --> 00:01:16.000
Now let's advance to 10,000 years before the present.

00:01:16.000 --> 00:01:18.000
And this is when it really gets interesting.

00:01:18.000 --> 00:01:21.000
What about the domestication of grains?

00:01:21.000 --> 00:01:24.000
What about the origins of agriculture?

00:01:24.000 --> 00:01:27.000
What would our ancestors 10,000 years ago

00:01:27.000 --> 00:01:29.000
have said

00:01:29.000 --> 00:01:31.000
if they really had technology assessment?

00:01:31.000 --> 00:01:33.000
And I could just imagine the committees

00:01:33.000 --> 00:01:35.000
reporting back to them

00:01:35.000 --> 00:01:38.000
on where agriculture was going to take humanity,

00:01:38.000 --> 00:01:41.000
at least in the next few hundred years.

00:01:41.000 --> 00:01:43.000
It was really bad news.

00:01:43.000 --> 00:01:45.000
First of all, worse nutrition,

00:01:45.000 --> 00:01:47.000
maybe shorter life spans.

00:01:47.000 --> 00:01:49.000
It was simply awful for women.

00:01:49.000 --> 00:01:51.000
The skeletal remains from that period

00:01:51.000 --> 00:01:56.000
have shown that they were grinding grain morning, noon and night.

00:01:56.000 --> 00:01:59.000
And politically, it was awful.

00:01:59.000 --> 00:02:02.000
It was the beginning of a much higher degree

00:02:02.000 --> 00:02:05.000
of inequality among people.

00:02:05.000 --> 00:02:08.000
If there had been rational technology assessment then,

00:02:08.000 --> 00:02:10.000
I think they very well might have said,

00:02:10.000 --> 00:02:13.000
"Let's call the whole thing off."

00:02:13.000 --> 00:02:17.000
Even now, our choices are having unintended effects.

00:02:17.000 --> 00:02:19.000
Historically, for example,

00:02:19.000 --> 00:02:22.000
chopsticks -- according to one Japanese anthropologist

00:02:22.000 --> 00:02:24.000
who wrote a dissertation about it

00:02:24.000 --> 00:02:26.000
at the University of Michigan --

00:02:26.000 --> 00:02:29.000
resulted in long-term changes

00:02:29.000 --> 00:02:31.000
in the dentition, in the teeth,

00:02:31.000 --> 00:02:33.000
of the Japanese public.

00:02:33.000 --> 00:02:36.000
And we are also changing our teeth right now.

00:02:36.000 --> 00:02:38.000
There is evidence

00:02:38.000 --> 00:02:40.000
that the human mouth and teeth

00:02:40.000 --> 00:02:42.000
are growing smaller all the time.

00:02:42.000 --> 00:02:45.000
That's not necessarily a bad unintended consequence.

00:02:45.000 --> 00:02:47.000
But I think from the point of view of a Neanderthal,

00:02:47.000 --> 00:02:49.000
there would have been a lot of disapproval

00:02:49.000 --> 00:02:52.000
of the wimpish choppers that we now have.

00:02:52.000 --> 00:02:55.000
So these things are kind of relative

00:02:55.000 --> 00:02:59.000
to where you or your ancestors happen to stand.

00:02:59.000 --> 00:03:01.000
In the ancient world

00:03:01.000 --> 00:03:04.000
there was a lot of respect for unintended consequences,

00:03:04.000 --> 00:03:07.000
and there was a very healthy sense of caution,

00:03:07.000 --> 00:03:09.000
reflected in the Tree of Knowledge,

00:03:09.000 --> 00:03:11.000
in Pandora's Box,

00:03:11.000 --> 00:03:13.000
and especially in the myth of Prometheus

00:03:13.000 --> 00:03:15.000
that's been so important

00:03:15.000 --> 00:03:17.000
in recent metaphors about technology.

00:03:17.000 --> 00:03:20.000
And that's all very true.

00:03:20.000 --> 00:03:22.000
The physicians of the ancient world --

00:03:22.000 --> 00:03:24.000
especially the Egyptians,

00:03:24.000 --> 00:03:26.000
who started medicine as we know it --

00:03:26.000 --> 00:03:28.000
were very conscious

00:03:28.000 --> 00:03:30.000
of what they could and couldn't treat.

00:03:30.000 --> 00:03:35.000
And the translations of the surviving texts say,

00:03:35.000 --> 00:03:37.000
"This I will not treat. This I cannot treat."

00:03:37.000 --> 00:03:39.000
They were very conscious.

00:03:39.000 --> 00:03:41.000
So were the followers of Hippocrates.

00:03:41.000 --> 00:03:43.000
The Hippocratic manuscripts also --

00:03:43.000 --> 00:03:46.000
repeatedly, according to recent studies --

00:03:46.000 --> 00:03:49.000
show how important it is not to do harm.

00:03:49.000 --> 00:03:51.000
More recently,

00:03:51.000 --> 00:03:53.000
Harvey Cushing,

00:03:53.000 --> 00:03:55.000
who really developed neurosurgery as we know it,

00:03:55.000 --> 00:03:58.000
who changed it from a field of medicine

00:03:58.000 --> 00:04:02.000
that had a majority of deaths resulting from surgery

00:04:02.000 --> 00:04:05.000
to one in which there was a hopeful outlook,

00:04:05.000 --> 00:04:07.000
he was very conscious

00:04:07.000 --> 00:04:10.000
that he was not always going to do the right thing.

00:04:10.000 --> 00:04:12.000
But he did his best,

00:04:12.000 --> 00:04:14.000
and he kept meticulous records

00:04:14.000 --> 00:04:17.000
that let him transform that branch of medicine.

00:04:17.000 --> 00:04:20.000
Now if we look forward a bit

00:04:20.000 --> 00:04:22.000
to the 19th century,

00:04:22.000 --> 00:04:24.000
we find a new style of technology.

00:04:24.000 --> 00:04:26.000
What we find is,

00:04:26.000 --> 00:04:29.000
no longer simple tools,

00:04:29.000 --> 00:04:31.000
but systems.

00:04:31.000 --> 00:04:33.000
We find more and more

00:04:33.000 --> 00:04:35.000
complex arrangements of machines

00:04:35.000 --> 00:04:37.000
that make it harder and harder

00:04:37.000 --> 00:04:39.000
to diagnose what's going on.

00:04:39.000 --> 00:04:41.000
And the first people who saw that

00:04:41.000 --> 00:04:44.000
were the telegraphers of the mid-19th century,

00:04:44.000 --> 00:04:46.000
who were the original hackers.

00:04:46.000 --> 00:04:49.000
Thomas Edison would have been very, very comfortable

00:04:49.000 --> 00:04:52.000
in the atmosphere of a software firm today.

00:04:52.000 --> 00:04:55.000
And these hackers had a word

00:04:55.000 --> 00:04:58.000
for those mysterious bugs in telegraph systems

00:04:58.000 --> 00:05:00.000
that they called bugs.

00:05:00.000 --> 00:05:04.000
That was the origin of the word "bug."

00:05:04.000 --> 00:05:06.000
This consciousness, though,

00:05:06.000 --> 00:05:09.000
was a little slow to seep through the general population,

00:05:09.000 --> 00:05:12.000
even people who were very, very well informed.

00:05:12.000 --> 00:05:14.000
Samuel Clemens, Mark Twain,

00:05:14.000 --> 00:05:16.000
was a big investor

00:05:16.000 --> 00:05:19.000
in the most complex machine of all times --

00:05:19.000 --> 00:05:21.000
at least until 1918 --

00:05:21.000 --> 00:05:23.000
registered with the U.S. Patent Office.

00:05:23.000 --> 00:05:25.000
That was the Paige typesetter.

00:05:25.000 --> 00:05:27.000
The Paige typesetter

00:05:27.000 --> 00:05:29.000
had 18,000 parts.

00:05:29.000 --> 00:05:32.000
The patent had 64 pages of text

00:05:32.000 --> 00:05:36.000
and 271 figures.

00:05:36.000 --> 00:05:38.000
It was such a beautiful machine

00:05:38.000 --> 00:05:41.000
because it did everything that a human being did

00:05:41.000 --> 00:05:43.000
in setting type --

00:05:43.000 --> 00:05:45.000
including returning the type to its place,

00:05:45.000 --> 00:05:47.000
which was a very difficult thing.

00:05:47.000 --> 00:05:49.000
And Mark Twain, who knew all about typesetting,

00:05:49.000 --> 00:05:52.000
really was smitten by this machine.

00:05:52.000 --> 00:05:55.000
Unfortunately, he was smitten in more ways than one,

00:05:55.000 --> 00:05:57.000
because it made him bankrupt,

00:05:57.000 --> 00:05:59.000
and he had to tour the world speaking

00:05:59.000 --> 00:06:02.000
to recoup his money.

00:06:02.000 --> 00:06:04.000
And this was an important thing

00:06:04.000 --> 00:06:06.000
about 19th century technology,

00:06:06.000 --> 00:06:08.000
that all these relationships among parts

00:06:08.000 --> 00:06:12.000
could make the most brilliant idea fall apart,

00:06:12.000 --> 00:06:14.000
even when judged by the most expert people.

00:06:14.000 --> 00:06:17.000
Now there is something else, though, in the early 20th century

00:06:17.000 --> 00:06:20.000
that made things even more complicated.

00:06:20.000 --> 00:06:23.000
And that was that safety technology itself

00:06:23.000 --> 00:06:25.000
could be a source of danger.

00:06:25.000 --> 00:06:28.000
The lesson of the Titanic, for a lot of the contemporaries,

00:06:28.000 --> 00:06:30.000
was that you must have enough lifeboats

00:06:30.000 --> 00:06:32.000
for everyone on the ship.

00:06:32.000 --> 00:06:35.000
And this was the result

00:06:35.000 --> 00:06:37.000
of the tragic loss of lives

00:06:37.000 --> 00:06:39.000
of people who could not get into them.

00:06:39.000 --> 00:06:42.000
However, there was another case, the Eastland,

00:06:42.000 --> 00:06:46.000
a ship that capsized in Chicago Harbor in 1915,

00:06:46.000 --> 00:06:49.000
and it killed 841 people --

00:06:49.000 --> 00:06:51.000
that was 14 more

00:06:51.000 --> 00:06:54.000
than the passenger toll of the Titanic.

00:06:54.000 --> 00:06:56.000
The reason for it, in part, was

00:06:56.000 --> 00:06:59.000
the extra life boats that were added

00:06:59.000 --> 00:07:02.000
that made this already unstable ship

00:07:02.000 --> 00:07:04.000
even more unstable.

00:07:04.000 --> 00:07:06.000
And that again proves

00:07:06.000 --> 00:07:09.000
that when you're talking about unintended consequences,

00:07:09.000 --> 00:07:11.000
it's not that easy to know

00:07:11.000 --> 00:07:13.000
the right lessons to draw.

00:07:13.000 --> 00:07:16.000
It's really a question of the system, how the ship was loaded,

00:07:16.000 --> 00:07:19.000
the ballast and many other things.

00:07:20.000 --> 00:07:23.000
So the 20th century, then,

00:07:23.000 --> 00:07:25.000
saw how much more complex reality was,

00:07:25.000 --> 00:07:28.000
but it also saw a positive side.

00:07:28.000 --> 00:07:31.000
It saw that invention

00:07:31.000 --> 00:07:33.000
could actually benefit from emergencies.

00:07:33.000 --> 00:07:35.000
It could benefit

00:07:35.000 --> 00:07:38.000
from tragedies.

00:07:38.000 --> 00:07:40.000
And my favorite example of that --

00:07:40.000 --> 00:07:42.000
which is not really widely known

00:07:42.000 --> 00:07:44.000
as a technological miracle,

00:07:44.000 --> 00:07:47.000
but it may be one of the greatest of all times,

00:07:47.000 --> 00:07:51.000
was the scaling up of penicillin in the Second World War.

00:07:51.000 --> 00:07:54.000
Penicillin was discovered in 1928,

00:07:54.000 --> 00:07:56.000
but even by 1940,

00:07:56.000 --> 00:07:59.000
no commercially and medically useful quantities of it

00:07:59.000 --> 00:08:01.000
were being produced.

00:08:01.000 --> 00:08:04.000
A number of pharmaceutical companies were working on it.

00:08:04.000 --> 00:08:06.000
They were working on it independently,

00:08:06.000 --> 00:08:08.000
and they weren't getting anywhere.

00:08:08.000 --> 00:08:10.000
And the Government Research Bureau

00:08:10.000 --> 00:08:12.000
brought representatives together

00:08:12.000 --> 00:08:14.000
and told them that this is something

00:08:14.000 --> 00:08:16.000
that has to be done.

00:08:16.000 --> 00:08:18.000
And not only did they do it,

00:08:18.000 --> 00:08:20.000
but within two years,

00:08:20.000 --> 00:08:22.000
they scaled up penicillin

00:08:22.000 --> 00:08:25.000
from preparation in one-liter flasks

00:08:25.000 --> 00:08:29.000
to 10,000-gallon vats.

00:08:29.000 --> 00:08:33.000
That was how quickly penicillin was produced

00:08:33.000 --> 00:08:37.000
and became one of the greatest medical advances of all time.

00:08:37.000 --> 00:08:39.000
In the Second World War, too,

00:08:39.000 --> 00:08:41.000
the existence

00:08:41.000 --> 00:08:43.000
of solar radiation

00:08:43.000 --> 00:08:46.000
was demonstrated by studies of interference

00:08:46.000 --> 00:08:50.000
that was detected by the radar stations of Great Britain.

00:08:50.000 --> 00:08:53.000
So there were benefits in calamities --

00:08:53.000 --> 00:08:55.000
benefits to pure science,

00:08:55.000 --> 00:08:57.000
as well as to applied science

00:08:57.000 --> 00:09:00.000
and medicine.

00:09:00.000 --> 00:09:03.000
Now when we come to the period after the Second World War,

00:09:03.000 --> 00:09:07.000
unintended consequences get even more interesting.

00:09:07.000 --> 00:09:09.000
And my favorite example of that

00:09:09.000 --> 00:09:12.000
occurred beginning in 1976,

00:09:12.000 --> 00:09:14.000
when it was discovered

00:09:14.000 --> 00:09:17.000
that the bacteria causing Legionnaires disease

00:09:17.000 --> 00:09:20.000
had always been present in natural waters,

00:09:20.000 --> 00:09:24.000
but it was the precise temperature of the water

00:09:24.000 --> 00:09:27.000
in heating, ventilating and air conditioning systems

00:09:27.000 --> 00:09:31.000
that raised the right temperature

00:09:31.000 --> 00:09:34.000
for the maximum reproduction

00:09:34.000 --> 00:09:36.000
of Legionella bacillus.

00:09:36.000 --> 00:09:38.000
Well, technology to the rescue.

00:09:38.000 --> 00:09:40.000
So chemists got to work,

00:09:40.000 --> 00:09:42.000
and they developed a bactericide

00:09:42.000 --> 00:09:45.000
that became widely used in those systems.

00:09:45.000 --> 00:09:49.000
But something else happened in the early 1980s,

00:09:49.000 --> 00:09:51.000
and that was that there was a mysterious epidemic

00:09:51.000 --> 00:09:54.000
of failures of tape drives

00:09:54.000 --> 00:09:56.000
all over the United States.

00:09:56.000 --> 00:09:59.000
And IBM, which made them,

00:09:59.000 --> 00:10:02.000
just didn't know what to do.

00:10:02.000 --> 00:10:05.000
They commissioned a group of their best scientists

00:10:05.000 --> 00:10:07.000
to investigate,

00:10:07.000 --> 00:10:09.000
and what they found was

00:10:09.000 --> 00:10:11.000
that all these tape drives

00:10:11.000 --> 00:10:14.000
were located near ventilation ducts.

00:10:14.000 --> 00:10:17.000
What happened was the bactericide was formulated

00:10:17.000 --> 00:10:19.000
with minute traces of tin.

00:10:19.000 --> 00:10:22.000
And these tin particles were deposited on the tape heads

00:10:22.000 --> 00:10:25.000
and were crashing the tape heads.

00:10:25.000 --> 00:10:28.000
So they reformulated the bactericide.

00:10:28.000 --> 00:10:30.000
But what's interesting to me

00:10:30.000 --> 00:10:32.000
is that this was the first case

00:10:32.000 --> 00:10:34.000
of a mechanical device

00:10:34.000 --> 00:10:37.000
suffering, at least indirectly, from a human disease.

00:10:37.000 --> 00:10:40.000
So it shows that we're really all in this together.

00:10:40.000 --> 00:10:42.000
(Laughter)

00:10:42.000 --> 00:10:45.000
In fact, it also shows something interesting,

00:10:45.000 --> 00:10:48.000
that although our capabilities and technology

00:10:48.000 --> 00:10:50.000
have been expanding geometrically,

00:10:50.000 --> 00:10:53.000
unfortunately, our ability to model their long-term behavior,

00:10:53.000 --> 00:10:55.000
which has also been increasing,

00:10:55.000 --> 00:10:58.000
has been increasing only arithmetically.

00:10:58.000 --> 00:11:01.000
So one of the characteristic problems of our time

00:11:01.000 --> 00:11:03.000
is how to close this gap

00:11:03.000 --> 00:11:06.000
between capabilities and foresight.

00:11:06.000 --> 00:11:09.000
One other very positive consequence

00:11:09.000 --> 00:11:12.000
of 20th century technology, though,

00:11:12.000 --> 00:11:16.000
was the way in which other kinds of calamities

00:11:16.000 --> 00:11:19.000
could lead to positive advances.

00:11:19.000 --> 00:11:22.000
There are two historians of business

00:11:22.000 --> 00:11:24.000
at the University of Maryland,

00:11:24.000 --> 00:11:26.000
Brent Goldfarb and David Kirsch,

00:11:26.000 --> 00:11:28.000
who have done some extremely interesting work,

00:11:28.000 --> 00:11:31.000
much of it still unpublished,

00:11:31.000 --> 00:11:33.000
on the history of major innovations.

00:11:33.000 --> 00:11:36.000
They have combined the list of major innovations,

00:11:36.000 --> 00:11:39.000
and they've discovered that the greatest number, the greatest decade,

00:11:39.000 --> 00:11:41.000
for fundamental innovations,

00:11:41.000 --> 00:11:45.000
as reflected in all of the lists that others have made --

00:11:45.000 --> 00:11:47.000
a number of lists that they have merged --

00:11:47.000 --> 00:11:50.000
was the Great Depression.

00:11:50.000 --> 00:11:53.000
And nobody knows just why this was so,

00:11:53.000 --> 00:11:56.000
but one story can reflect something of it.

00:11:56.000 --> 00:11:59.000
It was the origin of the Xerox copier,

00:11:59.000 --> 00:12:02.000
which celebrated its 50th anniversary

00:12:02.000 --> 00:12:04.000
last year.

00:12:04.000 --> 00:12:09.000
And Chester Carlson, the inventor,

00:12:09.000 --> 00:12:12.000
was a patent attorney.

00:12:12.000 --> 00:12:15.000
He really was not intending

00:12:15.000 --> 00:12:17.000
to work in patent research,

00:12:17.000 --> 00:12:21.000
but he couldn't really find an alternative technical job.

00:12:21.000 --> 00:12:23.000
So this was the best job he could get.

00:12:23.000 --> 00:12:27.000
He was upset by the low quality and high cost

00:12:27.000 --> 00:12:30.000
of existing patent reproductions,

00:12:30.000 --> 00:12:33.000
and so he started to develop

00:12:33.000 --> 00:12:36.000
a system of dry photocopying,

00:12:36.000 --> 00:12:39.000
which he patented in the late 1930s --

00:12:39.000 --> 00:12:43.000
and which became the first dry photocopier

00:12:43.000 --> 00:12:45.000
that was commercially practical

00:12:45.000 --> 00:12:47.000
in 1960.

00:12:47.000 --> 00:12:49.000
So we see that sometimes,

00:12:49.000 --> 00:12:51.000
as a result of these dislocations,

00:12:51.000 --> 00:12:53.000
as a result of people

00:12:53.000 --> 00:12:56.000
leaving their original intended career

00:12:56.000 --> 00:12:58.000
and going into something else

00:12:58.000 --> 00:13:00.000
where their creativity could make a difference,

00:13:00.000 --> 00:13:02.000
that depressions

00:13:02.000 --> 00:13:05.000
and all kinds of other unfortunate events

00:13:05.000 --> 00:13:08.000
can have a paradoxically stimulating effect

00:13:08.000 --> 00:13:10.000
on creativity.

00:13:10.000 --> 00:13:12.000
What does this mean?

00:13:12.000 --> 00:13:14.000
It means, I think,

00:13:14.000 --> 00:13:16.000
that we're living in a time of unexpected possibilities.

00:13:16.000 --> 00:13:19.000
Think of the financial world, for example.

00:13:19.000 --> 00:13:22.000
The mentor of Warren Buffett, Benjamin Graham,

00:13:22.000 --> 00:13:27.000
developed his system of value investing

00:13:27.000 --> 00:13:29.000
as a result of his own losses

00:13:29.000 --> 00:13:31.000
in the 1929 crash.

00:13:31.000 --> 00:13:33.000
And he published that book

00:13:33.000 --> 00:13:36.000
in the early 1930s,

00:13:36.000 --> 00:13:38.000
and the book still exists in further editions

00:13:38.000 --> 00:13:40.000
and is still a fundamental textbook.

00:13:40.000 --> 00:13:44.000
So many important creative things can happen

00:13:44.000 --> 00:13:47.000
when people learn from disasters.

00:13:47.000 --> 00:13:51.000
Now think of the large and small plagues that we have now --

00:13:51.000 --> 00:13:56.000
bed bugs, killer bees, spam --

00:13:56.000 --> 00:13:59.000
and it's very possible that the solutions to those

00:13:59.000 --> 00:14:02.000
will really extend well beyond the immediate question.

00:14:02.000 --> 00:14:05.000
If we think, for example, of Louis Pasteur,

00:14:05.000 --> 00:14:07.000
who in the 1860s

00:14:07.000 --> 00:14:09.000
was asked to study

00:14:09.000 --> 00:14:13.000
the diseases of silk worms for the silk industry,

00:14:13.000 --> 00:14:16.000
and his discoveries were really the beginning

00:14:16.000 --> 00:14:18.000
of the germ theory of disease.

00:14:18.000 --> 00:14:21.000
So very often, some kind of disaster --

00:14:21.000 --> 00:14:24.000
sometimes the consequence, for example,

00:14:24.000 --> 00:14:27.000
of over-cultivation of silk worms,

00:14:27.000 --> 00:14:29.000
which was a problem in Europe at the time --

00:14:29.000 --> 00:14:31.000
can be the key to something much bigger.

00:14:31.000 --> 00:14:33.000
So this means

00:14:33.000 --> 00:14:35.000
that we need to take a different view

00:14:35.000 --> 00:14:37.000
of unintended consequences.

00:14:37.000 --> 00:14:40.000
We need to take a really positive view.

00:14:40.000 --> 00:14:43.000
We need to see what they can do for us.

00:14:43.000 --> 00:14:45.000
We need to learn

00:14:45.000 --> 00:14:47.000
from those figures that I mentioned.

00:14:47.000 --> 00:14:50.000
We need to learn, for example, from Dr. Cushing,

00:14:50.000 --> 00:14:52.000
who killed patients

00:14:52.000 --> 00:14:54.000
in the course of his early operations.

00:14:54.000 --> 00:14:57.000
He had to have some errors. He had to have some mistakes.

00:14:57.000 --> 00:15:00.000
And he learned meticulously from his mistakes.

00:15:00.000 --> 00:15:02.000
And as a result,

00:15:02.000 --> 00:15:05.000
when we say, "This isn't brain surgery,"

00:15:05.000 --> 00:15:08.000
that pays tribute to how difficult it was

00:15:08.000 --> 00:15:10.000
for anyone to learn from their mistakes

00:15:10.000 --> 00:15:12.000
in a field of medicine

00:15:12.000 --> 00:15:15.000
that was considered so discouraging in its prospects.

00:15:15.000 --> 00:15:18.000
And we can also remember

00:15:18.000 --> 00:15:20.000
how the pharmaceutical companies

00:15:20.000 --> 00:15:22.000
were willing to pool their knowledge,

00:15:22.000 --> 00:15:24.000
to share their knowledge,

00:15:24.000 --> 00:15:26.000
in the face of an emergency,

00:15:26.000 --> 00:15:29.000
which they hadn't really been for years and years.

00:15:29.000 --> 00:15:32.000
They might have been able to do it earlier.

00:15:32.000 --> 00:15:35.000
The message, then, for me,

00:15:35.000 --> 00:15:37.000
about unintended consequences

00:15:37.000 --> 00:15:40.000
is chaos happens;

00:15:40.000 --> 00:15:42.000
let's make better use of it.

00:15:42.000 --> 00:15:44.000
Thank you very much.

00:15:44.000 --> 00:15:48.000
(Applause)

