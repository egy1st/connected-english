WEBVTT

00:00:00.476 --> 00:00:01.627
I do two things:

00:00:01.651 --> 00:00:03.769
I design mobile computers
and I study brains.

00:00:03.793 --> 00:00:06.723
Today's talk is about brains
and -- (Audience member cheers)

00:00:06.747 --> 00:00:08.564
Yay! I have a brain fan out there.

00:00:08.588 --> 00:00:11.735
(Laughter)

00:00:11.759 --> 00:00:13.314
If I could have my first slide,

00:00:13.338 --> 00:00:16.187
you'll see the title of my talk
and my two affiliations.

00:00:16.211 --> 00:00:19.679
So what I'm going to talk about is why
we don't have a good brain theory,

00:00:19.703 --> 00:00:21.980
why it is important
that we should develop one

00:00:22.004 --> 00:00:23.487
and what we can do about it.

00:00:23.511 --> 00:00:25.335
I'll try to do all that in 20 minutes.

00:00:25.359 --> 00:00:26.510
I have two affiliations.

00:00:26.534 --> 00:00:29.066
Most of you know me
from my Palm and Handspring days,

00:00:29.090 --> 00:00:31.773
but I also run a nonprofit
scientific research institute

00:00:31.797 --> 00:00:34.429
called the Redwood Neuroscience
Institute in Menlo Park.

00:00:34.453 --> 00:00:37.841
We study theoretical neuroscience
and how the neocortex works.

00:00:37.865 --> 00:00:39.463
I'm going to talk all about that.

00:00:39.487 --> 00:00:42.232
I have one slide on my other life,
the computer life,

00:00:42.256 --> 00:00:43.557
and that's this slide here.

00:00:43.581 --> 00:00:46.849
These are some of the products
I've worked on over the last 20 years,

00:00:46.873 --> 00:00:48.715
starting from the very original laptop

00:00:48.739 --> 00:00:50.526
to some of the first tablet computers

00:00:50.550 --> 00:00:52.848
and so on, ending up
most recently with the Treo,

00:00:52.872 --> 00:00:54.404
and we're continuing to do this.

00:00:54.428 --> 00:00:56.729
I've done this because
I believe mobile computing

00:00:56.753 --> 00:00:58.477
is the future of personal computing,

00:00:58.501 --> 00:01:00.955
and I'm trying to make
the world a little bit better

00:01:00.979 --> 00:01:02.275
by working on these things.

00:01:02.299 --> 00:01:04.173
But this was, I admit, all an accident.

00:01:04.197 --> 00:01:06.505
I really didn't want to do
any of these products.

00:01:06.529 --> 00:01:07.911
Very early in my career

00:01:07.935 --> 00:01:10.625
I decided I was not going to be
in the computer industry.

00:01:10.649 --> 00:01:12.370
Before that, I just have to tell you

00:01:12.394 --> 00:01:15.502
about this picture of Graffiti
I picked off the web the other day.

00:01:15.526 --> 00:01:18.779
I was looking for a picture for Graffiti
that'll text input language.

00:01:18.803 --> 00:01:22.492
I found a website dedicated to teachers
who want to make script-writing things

00:01:22.516 --> 00:01:24.190
across the top of their blackboard,

00:01:24.214 --> 00:01:27.047
and they had added Graffiti to it,
and I'm sorry about that.

00:01:27.071 --> 00:01:29.318
(Laughter)

00:01:29.342 --> 00:01:30.642
So what happened was,

00:01:30.666 --> 00:01:35.565
when I was young and got out
of engineering school at Cornell in '79,

00:01:35.589 --> 00:01:38.776
I went to work for Intel
and was in the computer industry,

00:01:38.800 --> 00:01:42.202
and three months into that,
I fell in love with something else.

00:01:42.226 --> 00:01:45.270
I said, "I made
the wrong career choice here,"

00:01:45.294 --> 00:01:47.533
and I fell in love with brains.

00:01:47.557 --> 00:01:49.090
This is not a real brain.

00:01:49.114 --> 00:01:51.833
This is a picture of one, a line drawing.

00:01:51.857 --> 00:01:53.976
And I don't remember
exactly how it happened,

00:01:54.000 --> 00:01:57.515
but I have one recollection,
which was pretty strong in my mind.

00:01:57.539 --> 00:01:59.149
In September of 1979,

00:01:59.173 --> 00:02:02.537
Scientific American came out
with a single-topic issue about the brain.

00:02:02.561 --> 00:02:04.499
It was one of their best issues ever.

00:02:04.523 --> 00:02:07.470
They talked about the neuron,
development, disease, vision

00:02:07.494 --> 00:02:10.090
and all the things you might want
to know about brains.

00:02:10.114 --> 00:02:11.616
It was really quite impressive.

00:02:11.640 --> 00:02:14.412
One might've had the impression
we knew a lot about brains.

00:02:14.436 --> 00:02:18.631
But the last article in that issue
was written by Francis Crick of DNA fame.

00:02:18.655 --> 00:02:21.679
Today is, I think, the 50th anniversary
of the discovery of DNA.

00:02:21.703 --> 00:02:24.778
And he wrote a story basically saying,
this is all well and good,

00:02:24.802 --> 00:02:27.545
but you know, we don't know
diddly squat about brains,

00:02:27.569 --> 00:02:29.308
and no one has a clue how they work,

00:02:29.332 --> 00:02:31.198
so don't believe what anyone tells you.

00:02:31.222 --> 00:02:33.387
This is a quote
from that article, he says:

00:02:33.411 --> 00:02:37.704
"What is conspicuously lacking" --
he's a very proper British gentleman --

00:02:37.728 --> 00:02:40.558
"What is conspicuously lacking
is a broad framework of ideas

00:02:40.582 --> 00:02:42.934
in which to interpret
these different approaches."

00:02:42.958 --> 00:02:44.926
I thought the word "framework" was great.

00:02:44.950 --> 00:02:46.767
He didn't say we didn't have a theory.

00:02:46.791 --> 00:02:49.516
He says we don't even know
how to begin to think about it.

00:02:49.540 --> 00:02:51.032
We don't even have a framework.

00:02:51.056 --> 00:02:54.106
We are in the pre-paradigm days,
if you want to use Thomas Kuhn.

00:02:54.130 --> 00:02:55.469
So I fell in love with this.

00:02:55.493 --> 00:02:59.068
I said, look: We have all this knowledge
about brains -- how hard can it be?

00:02:59.092 --> 00:03:02.530
It's something we can work on
in my lifetime; I could make a difference.

00:03:02.554 --> 00:03:06.173
So I tried to get out of the computer
business, into the brain business.

00:03:06.197 --> 00:03:08.201
First, I went to MIT,
the AI lab was there.

00:03:08.225 --> 00:03:10.620
I said, I want to build
intelligent machines too,

00:03:10.644 --> 00:03:13.161
but I want to study how brains work first.

00:03:13.185 --> 00:03:15.491
And they said, "Oh, you
don't need to do that.

00:03:15.515 --> 00:03:17.905
You're just going to program
computers, that's all.

00:03:17.929 --> 00:03:19.892
I said, you really ought to study brains.

00:03:19.916 --> 00:03:21.348
They said, "No, you're wrong."

00:03:21.372 --> 00:03:23.618
I said, "No, you're wrong,"
and I didn't get in.

00:03:23.642 --> 00:03:24.720
(Laughter)

00:03:24.744 --> 00:03:26.899
I was a little disappointed --
pretty young --

00:03:26.923 --> 00:03:28.859
but I went back again a few years later,

00:03:28.883 --> 00:03:31.242
this time in California,
and I went to Berkeley.

00:03:31.266 --> 00:03:33.696
And I said, I'll go
in from the biological side.

00:03:33.720 --> 00:03:36.809
So I got in the PhD program in biophysics.

00:03:36.833 --> 00:03:40.243
I was like, I'm studying brains now.
Well, I want to study theory.

00:03:40.267 --> 00:03:42.536
They said, "You can't
study theory about brains.

00:03:42.560 --> 00:03:44.555
You can't get funded for that.

00:03:44.579 --> 00:03:46.734
And as a graduate student,
you can't do that."

00:03:46.758 --> 00:03:47.976
So I said, oh my gosh.

00:03:48.000 --> 00:03:51.155
I was depressed; I said, but I can
make a difference in this field.

00:03:51.179 --> 00:03:53.187
I went back in the computer industry

00:03:53.211 --> 00:03:55.316
and said, I'll have to work
here for a while.

00:03:55.340 --> 00:03:57.733
That's when I designed
all those computer products.

00:03:57.757 --> 00:03:59.058
(Laughter)

00:03:59.082 --> 00:04:01.976
I said, I want to do this
for four years, make some money,

00:04:02.000 --> 00:04:05.976
I was having a family,
and I would mature a bit,

00:04:06.000 --> 00:04:08.816
and maybe the business
of neuroscience would mature a bit.

00:04:08.840 --> 00:04:11.841
Well, it took longer than four years.
It's been about 16 years.

00:04:11.865 --> 00:04:14.581
But I'm doing it now,
and I'm going to tell you about it.

00:04:14.605 --> 00:04:16.891
So why should we have a good brain theory?

00:04:16.915 --> 00:04:20.017
Well, there's lots of reasons
people do science.

00:04:20.041 --> 00:04:22.958
The most basic one is,
people like to know things.

00:04:22.982 --> 00:04:25.177
We're curious, and we go out
and get knowledge.

00:04:25.201 --> 00:04:27.067
Why do we study ants? It's interesting.

00:04:27.091 --> 00:04:30.557
Maybe we'll learn something useful,
but it's interesting and fascinating.

00:04:30.581 --> 00:04:32.638
But sometimes a science
has other attributes

00:04:32.662 --> 00:04:34.491
which makes it really interesting.

00:04:34.515 --> 00:04:37.142
Sometimes a science will tell
something about ourselves;

00:04:37.166 --> 00:04:38.390
it'll tell us who we are.

00:04:38.414 --> 00:04:41.166
Evolution did this
and Copernicus did this,

00:04:41.190 --> 00:04:43.524
where we have a new
understanding of who we are.

00:04:43.548 --> 00:04:46.976
And after all, we are our brains.
My brain is talking to your brain.

00:04:47.000 --> 00:04:49.030
Our bodies are hanging along for the ride,

00:04:49.054 --> 00:04:50.879
but my brain is talking to your brain.

00:04:50.903 --> 00:04:54.151
And if we want to understand
who we are and how we feel and perceive,

00:04:54.175 --> 00:04:55.566
we need to understand brains.

00:04:55.590 --> 00:04:59.374
Another thing is sometimes science leads
to big societal benefits, technologies,

00:04:59.398 --> 00:05:00.689
or businesses or whatever.

00:05:00.713 --> 00:05:03.591
This is one, too, because
when we understand how brains work,

00:05:03.615 --> 00:05:05.679
we'll be able to build
intelligent machines.

00:05:05.703 --> 00:05:07.401
That's a good thing on the whole,

00:05:07.425 --> 00:05:09.283
with tremendous benefits to society,

00:05:09.307 --> 00:05:10.976
just like a fundamental technology.

00:05:11.000 --> 00:05:13.850
So why don't we have
a good theory of brains?

00:05:13.874 --> 00:05:16.042
People have been working
on it for 100 years.

00:05:16.066 --> 00:05:18.785
Let's first take a look
at what normal science looks like.

00:05:18.809 --> 00:05:19.996
This is normal science.

00:05:20.020 --> 00:05:24.094
Normal science is a nice balance
between theory and experimentalists.

00:05:24.118 --> 00:05:26.809
The theorist guy says,
"I think this is what's going on,"

00:05:26.833 --> 00:05:28.794
the experimentalist says, "You're wrong."

00:05:28.818 --> 00:05:31.822
It goes back and forth,
this works in physics, this in geology.

00:05:31.846 --> 00:05:34.855
But if this is normal science,
what does neuroscience look like?

00:05:34.879 --> 00:05:36.674
This is what neuroscience looks like.

00:05:36.698 --> 00:05:38.140
We have this mountain of data,

00:05:38.164 --> 00:05:40.234
which is anatomy, physiology and behavior.

00:05:40.258 --> 00:05:43.452
You can't imagine how much detail
we know about brains.

00:05:43.476 --> 00:05:47.068
There were 28,000 people who went
to the neuroscience conference this year,

00:05:47.092 --> 00:05:49.455
and every one of them
is doing research in brains.

00:05:49.479 --> 00:05:51.173
A lot of data, but no theory.

00:05:51.197 --> 00:05:53.197
There's a little wimpy box on top there.

00:05:53.221 --> 00:05:56.603
And theory has not played a role
in any sort of grand way

00:05:56.627 --> 00:05:58.056
in the neurosciences.

00:05:58.080 --> 00:05:59.320
And it's a real shame.

00:05:59.344 --> 00:06:00.735
Now, why has this come about?

00:06:00.759 --> 00:06:03.747
If you ask neuroscientists
why is this the state of affairs,

00:06:03.771 --> 00:06:05.017
first, they'll admit it.

00:06:05.041 --> 00:06:06.526
But if you ask them, they say,

00:06:06.550 --> 00:06:09.282
there's various reasons
we don't have a good brain theory.

00:06:09.306 --> 00:06:11.275
Some say we still don't have enough data,

00:06:11.299 --> 00:06:14.358
we need more information,
there's all these things we don't know.

00:06:14.382 --> 00:06:17.223
Well, I just told you there's data
coming out of your ears.

00:06:17.247 --> 00:06:20.411
We have so much information,
we don't even know how to organize it.

00:06:20.435 --> 00:06:21.873
What good is more going to do?

00:06:21.897 --> 00:06:25.345
Maybe we'll be lucky and discover
some magic thing, but I don't think so.

00:06:25.369 --> 00:06:28.342
This is a symptom of the fact
that we just don't have a theory.

00:06:28.366 --> 00:06:30.976
We don't need more data,
we need a good theory.

00:06:31.000 --> 00:06:32.798
Another one is sometimes people say,

00:06:32.822 --> 00:06:35.976
"Brains are so complex,
it'll take another 50 years."

00:06:36.000 --> 00:06:39.354
I even think Chris said something
like this yesterday, something like,

00:06:39.378 --> 00:06:42.005
it's one of the most complicated
things in the universe.

00:06:42.029 --> 00:06:44.819
That's not true -- you're more
complicated than your brain.

00:06:44.843 --> 00:06:45.994
You've got a brain.

00:06:46.018 --> 00:06:48.168
And although the brain
looks very complicated,

00:06:48.192 --> 00:06:50.528
things look complicated
until you understand them.

00:06:50.552 --> 00:06:51.887
That's always been the case.

00:06:51.911 --> 00:06:55.154
So we can say, my neocortex,
the part of the brain I'm interested in,

00:06:55.178 --> 00:06:56.330
has 30 billion cells.

00:06:56.354 --> 00:06:58.786
But, you know what?
It's very, very regular.

00:06:58.810 --> 00:07:02.204
In fact, it looks like it's the same thing
repeated over and over again.

00:07:02.228 --> 00:07:04.764
It's not as complex as it looks.
That's not the issue.

00:07:04.788 --> 00:07:07.075
Some people say,
brains can't understand brains.

00:07:07.099 --> 00:07:09.087
Very Zen-like. Woo.

00:07:09.111 --> 00:07:11.299
(Laughter)

00:07:11.323 --> 00:07:14.182
You know, it sounds good, but why?
I mean, what's the point?

00:07:14.206 --> 00:07:16.775
It's just a bunch of cells.
You understand your liver.

00:07:16.799 --> 00:07:18.776
It's got a lot of cells in it too, right?

00:07:18.800 --> 00:07:21.294
So, you know, I don't think
there's anything to that.

00:07:21.318 --> 00:07:23.430
And finally, some people say,

00:07:23.454 --> 00:07:26.437
"I don't feel like a bunch
of cells -- I'm conscious.

00:07:26.461 --> 00:07:28.530
I've got this experience,
I'm in the world.

00:07:28.554 --> 00:07:30.464
I can't be just a bunch of cells."

00:07:30.488 --> 00:07:33.711
Well, people used to believe
there was a life force to be living,

00:07:33.735 --> 00:07:36.144
and we now know
that's really not true at all.

00:07:36.168 --> 00:07:38.066
And there's really no evidence,

00:07:38.090 --> 00:07:41.464
other than that people just disbelieve
that cells can do what they do.

00:07:41.488 --> 00:07:44.529
So some people have fallen
into the pit of metaphysical dualism,

00:07:44.553 --> 00:07:47.283
some really smart people, too,
but we can reject all that.

00:07:47.307 --> 00:07:50.202
(Laughter)

00:07:50.226 --> 00:07:51.967
No, there's something else,

00:07:51.991 --> 00:07:53.976
something really fundamental, and it is:

00:07:54.000 --> 00:07:56.451
another reason why we don't have
a good brain theory

00:07:56.475 --> 00:08:02.010
is because we have an intuitive,
strongly held but incorrect assumption

00:08:02.034 --> 00:08:04.146
that has prevented us
from seeing the answer.

00:08:04.170 --> 00:08:07.958
There's something we believe that just,
it's obvious, but it's wrong.

00:08:07.982 --> 00:08:11.548
Now, there's a history of this in science
and before I tell you what it is,

00:08:11.572 --> 00:08:13.871
I'll tell you about the history
of it in science.

00:08:13.895 --> 00:08:15.805
Look at other scientific revolutions --

00:08:15.829 --> 00:08:17.708
the solar system, that's Copernicus,

00:08:17.732 --> 00:08:20.551
Darwin's evolution,
and tectonic plates, that's Wegener.

00:08:21.059 --> 00:08:23.354
They all have a lot in common
with brain science.

00:08:23.378 --> 00:08:26.044
First, they had a lot
of unexplained data. A lot of it.

00:08:26.068 --> 00:08:28.862
But it got more manageable
once they had a theory.

00:08:28.886 --> 00:08:31.693
The best minds were stumped --
really smart people.

00:08:31.717 --> 00:08:33.721
We're not smarter now than they were then;

00:08:33.745 --> 00:08:36.272
it just turns out it's really
hard to think of things,

00:08:36.296 --> 00:08:38.972
but once you've thought of them,
it's easy to understand.

00:08:38.996 --> 00:08:41.102
My daughters understood
these three theories,

00:08:41.126 --> 00:08:43.644
in their basic framework, in kindergarten.

00:08:43.668 --> 00:08:46.934
It's not that hard --
here's the apple, here's the orange,

00:08:46.958 --> 00:08:48.976
the Earth goes around, that kind of stuff.

00:08:49.000 --> 00:08:51.586
Another thing is the answer
was there all along,

00:08:51.610 --> 00:08:54.389
but we kind of ignored it
because of this obvious thing.

00:08:54.413 --> 00:08:57.263
It was an intuitive,
strongly held belief that was wrong.

00:08:57.287 --> 00:08:58.977
In the case of the solar system,

00:08:59.001 --> 00:09:00.761
the idea that the Earth is spinning,

00:09:00.785 --> 00:09:02.976
the surface is going
a thousand miles an hour,

00:09:03.000 --> 00:09:06.249
and it's going through the solar system
at a million miles an hour --

00:09:06.273 --> 00:09:08.749
this is lunacy; we all know
the Earth isn't moving.

00:09:08.773 --> 00:09:11.650
Do you feel like you're moving
a thousand miles an hour?

00:09:11.674 --> 00:09:14.593
If you said Earth was spinning
around in space and was huge --

00:09:14.617 --> 00:09:17.208
they would lock you up,
that's what they did back then.

00:09:17.232 --> 00:09:20.507
So it was intuitive and obvious.
Now, what about evolution?

00:09:20.531 --> 00:09:21.685
Evolution, same thing.

00:09:21.709 --> 00:09:24.789
We taught our kids the Bible says
God created all these species,

00:09:24.813 --> 00:09:27.956
cats are cats; dogs are dogs;
people are people; plants are plants;

00:09:27.980 --> 00:09:29.221
they don't change.

00:09:29.245 --> 00:09:31.894
Noah put them on the ark
in that order, blah, blah.

00:09:31.918 --> 00:09:35.313
The fact is, if you believe in evolution,
we all have a common ancestor.

00:09:35.337 --> 00:09:38.619
We all have a common ancestor
with the plant in the lobby!

00:09:38.643 --> 00:09:42.329
This is what evolution tells us.
And it's true. It's kind of unbelievable.

00:09:42.353 --> 00:09:44.910
And the same thing about tectonic plates.

00:09:44.934 --> 00:09:46.656
All the mountains and the continents

00:09:46.680 --> 00:09:49.024
are kind of floating around
on top of the Earth.

00:09:49.048 --> 00:09:50.294
It doesn't make any sense.

00:09:50.318 --> 00:09:54.919
So what is the intuitive,
but incorrect assumption,

00:09:54.943 --> 00:09:56.910
that's kept us from understanding brains?

00:09:56.934 --> 00:10:00.227
I'll tell you. It'll seem obvious
that it's correct. That's the point.

00:10:00.251 --> 00:10:03.685
Then I'll make an argument why
you're incorrect on the other assumption.

00:10:03.709 --> 00:10:05.391
The intuitive but obvious thing is:

00:10:05.415 --> 00:10:07.729
somehow, intelligence
is defined by behavior;

00:10:07.753 --> 00:10:10.103
we're intelligent
because of how we do things

00:10:10.127 --> 00:10:11.699
and how we behave intelligently.

00:10:11.723 --> 00:10:13.602
And I'm going to tell you that's wrong.

00:10:13.626 --> 00:10:15.757
Intelligence is defined by prediction.

00:10:15.781 --> 00:10:18.196
I'm going to work you
through this in a few slides,

00:10:18.220 --> 00:10:20.314
and give you an example
of what this means.

00:10:20.338 --> 00:10:21.639
Here's a system.

00:10:21.663 --> 00:10:24.571
Engineers and scientists
like to look at systems like this.

00:10:24.595 --> 00:10:27.758
They say, we have a thing in a box.
We have its inputs and outputs.

00:10:27.782 --> 00:10:31.022
The AI people said, the thing in the box
is a programmable computer,

00:10:31.046 --> 00:10:32.725
because it's equivalent to a brain.

00:10:32.749 --> 00:10:36.255
We'll feed it some inputs and get it
to do something, have some behavior.

00:10:36.279 --> 00:10:39.101
Alan Turing defined the Turing test,
which essentially says,

00:10:39.125 --> 00:10:42.678
we'll know if something's intelligent
if it behaves identical to a human --

00:10:42.702 --> 00:10:44.808
a behavioral metric
of what intelligence is

00:10:44.832 --> 00:10:46.976
that has stuck in our minds
for a long time.

00:10:47.000 --> 00:10:49.392
Reality, though --
I call it real intelligence.

00:10:49.416 --> 00:10:51.591
Real intelligence
is built on something else.

00:10:51.615 --> 00:10:54.829
We experience the world
through a sequence of patterns,

00:10:54.853 --> 00:10:57.002
and we store them, and we recall them.

00:10:57.026 --> 00:10:59.571
When we recall them,
we match them up against reality,

00:10:59.595 --> 00:11:01.846
and we're making predictions all the time.

00:11:01.870 --> 00:11:04.828
It's an internal metric;
there's an internal metric about us,

00:11:04.852 --> 00:11:08.194
saying, do we understand the world,
am I making predictions, and so on.

00:11:08.218 --> 00:11:11.220
You're all being intelligent now,
but you're not doing anything.

00:11:11.244 --> 00:11:14.246
Maybe you're scratching yourself,
but you're not doing anything.

00:11:14.270 --> 00:11:17.426
But you're being intelligent;
you're understanding what I'm saying.

00:11:17.450 --> 00:11:19.745
Because you're intelligent
and you speak English,

00:11:19.769 --> 00:11:21.520
you know the word at the end of this

00:11:21.544 --> 00:11:22.703
sentence.

00:11:22.727 --> 00:11:25.879
The word came to you;
you make these predictions all the time.

00:11:25.903 --> 00:11:27.602
What I'm saying is,

00:11:27.626 --> 00:11:30.257
the internal prediction
is the output in the neocortex,

00:11:30.281 --> 00:11:32.822
and somehow, prediction
leads to intelligent behavior.

00:11:32.846 --> 00:11:33.997
Here's how that happens:

00:11:34.021 --> 00:11:35.976
Let's start with a non-intelligent brain.

00:11:36.000 --> 00:11:39.009
I'll argue a non-intelligent brain,
we'll call it an old brain.

00:11:39.033 --> 00:11:41.904
And we'll say it's
a non-mammal, like a reptile,

00:11:41.928 --> 00:11:43.913
say, an alligator; we have an alligator.

00:11:43.937 --> 00:11:47.308
And the alligator has
some very sophisticated senses.

00:11:47.332 --> 00:11:50.538
It's got good eyes and ears
and touch senses and so on,

00:11:50.562 --> 00:11:52.031
a mouth and a nose.

00:11:52.055 --> 00:11:54.046
It has very complex behavior.

00:11:54.070 --> 00:11:57.976
It can run and hide. It has fears
and emotions. It can eat you.

00:11:58.000 --> 00:12:01.590
It can attack.
It can do all kinds of stuff.

00:12:02.193 --> 00:12:05.049
But we don't consider
the alligator very intelligent,

00:12:05.073 --> 00:12:06.749
not in a human sort of way.

00:12:06.773 --> 00:12:09.129
But it has all this complex
behavior already.

00:12:09.510 --> 00:12:11.311
Now in evolution, what happened?

00:12:11.335 --> 00:12:13.720
First thing that happened
in evolution with mammals

00:12:13.744 --> 00:12:16.275
is we started to develop a thing
called the neocortex.

00:12:16.299 --> 00:12:20.092
I'm going to represent the neocortex
by this box on top of the old brain.

00:12:20.116 --> 00:12:23.469
Neocortex means "new layer."
It's a new layer on top of your brain.

00:12:23.493 --> 00:12:25.836
It's the wrinkly thing
on the top of your head

00:12:25.860 --> 00:12:28.944
that got wrinkly because it got shoved
in there and doesn't fit.

00:12:28.968 --> 00:12:29.976
(Laughter)

00:12:30.000 --> 00:12:32.242
Literally, it's about the size
of a table napkin

00:12:32.266 --> 00:12:33.840
and doesn't fit, so it's wrinkly.

00:12:33.864 --> 00:12:35.609
Now, look at how I've drawn this.

00:12:35.633 --> 00:12:37.019
The old brain is still there.

00:12:37.043 --> 00:12:40.698
You still have that alligator brain.
You do. It's your emotional brain.

00:12:40.722 --> 00:12:43.452
It's all those gut reactions you have.

00:12:43.476 --> 00:12:46.746
On top of it, we have this memory system
called the neocortex.

00:12:46.770 --> 00:12:51.064
And the memory system is sitting
over the sensory part of the brain.

00:12:51.088 --> 00:12:54.143
So as the sensory input
comes in and feeds from the old brain,

00:12:54.167 --> 00:12:56.321
it also goes up into the neocortex.

00:12:56.345 --> 00:12:58.258
And the neocortex is just memorizing.

00:12:58.282 --> 00:13:01.843
It's sitting there saying, I'm going
to memorize all the things going on:

00:13:01.867 --> 00:13:04.886
where I've been, people I've seen,
things I've heard, and so on.

00:13:04.910 --> 00:13:08.272
And in the future, when it sees
something similar to that again,

00:13:08.296 --> 00:13:10.931
in a similar environment,
or the exact same environment,

00:13:10.955 --> 00:13:14.510
it'll start playing it back:
"Oh, I've been here before,"

00:13:14.534 --> 00:13:16.898
and when you were here before,
this happened next.

00:13:16.922 --> 00:13:18.648
It allows you to predict the future.

00:13:18.672 --> 00:13:22.068
It literally feeds back
the signals into your brain;

00:13:22.092 --> 00:13:24.357
they'll let you see
what's going to happen next,

00:13:24.381 --> 00:13:26.976
will let you hear the word
"sentence" before I said it.

00:13:27.000 --> 00:13:30.185
And it's this feeding
back into the old brain

00:13:30.209 --> 00:13:32.786
that will allow you to make
more intelligent decisions.

00:13:32.810 --> 00:13:36.299
This is the most important slide
of my talk, so I'll dwell on it a little.

00:13:36.323 --> 00:13:39.898
And all the time you say,
"Oh, I can predict things,"

00:13:39.922 --> 00:13:43.282
so if you're a rat and you go
through a maze, and you learn the maze,

00:13:43.306 --> 00:13:45.745
next time you're in one,
you have the same behavior.

00:13:45.769 --> 00:13:48.760
But suddenly, you're smarter;
you say, "I recognize this maze,

00:13:48.784 --> 00:13:52.326
I know which way to go; I've been here
before; I can envision the future."

00:13:52.350 --> 00:13:53.518
That's what it's doing.

00:13:53.542 --> 00:13:56.382
This is true for all mammals --

00:13:56.406 --> 00:13:58.437
in humans, it got a lot worse.

00:13:58.461 --> 00:14:01.048
Humans actually developed
the front of the neocortex,

00:14:01.072 --> 00:14:03.293
called the anterior part of the neocortex.

00:14:03.317 --> 00:14:04.755
And nature did a little trick.

00:14:04.779 --> 00:14:07.466
It copied the posterior,
the back part, which is sensory,

00:14:07.490 --> 00:14:08.641
and put it in the front.

00:14:08.665 --> 00:14:11.145
Humans uniquely have
the same mechanism on the front,

00:14:11.169 --> 00:14:12.723
but we use it for motor control.

00:14:12.747 --> 00:14:16.328
So we're now able to do very sophisticated
motor planning, things like that.

00:14:16.352 --> 00:14:19.478
I don't have time to explain,
but to understand how a brain works,

00:14:19.502 --> 00:14:23.039
you have to understand how the first part
of the mammalian neocortex works,

00:14:23.063 --> 00:14:25.356
how it is we store patterns
and make predictions.

00:14:25.380 --> 00:14:27.568
Let me give you
a few examples of predictions.

00:14:27.592 --> 00:14:29.268
I already said the word "sentence."

00:14:29.292 --> 00:14:32.498
In music, if you've heard a song before,

00:14:32.522 --> 00:14:35.431
when you hear it, the next note
pops into your head already --

00:14:35.455 --> 00:14:36.606
you anticipate it.

00:14:36.630 --> 00:14:39.984
With an album, at the end of a song,
the next song pops into your head.

00:14:40.008 --> 00:14:42.313
It happens all the time,
you make predictions.

00:14:42.337 --> 00:14:45.376
I have this thing called
the "altered door" thought experiment.

00:14:45.400 --> 00:14:48.229
It says, you have a door at home;

00:14:48.253 --> 00:14:50.008
when you're here, I'm changing it --

00:14:50.032 --> 00:14:53.228
I've got a guy back at your house
right now, moving the door around,

00:14:53.252 --> 00:14:55.021
moving your doorknob over two inches.

00:14:55.045 --> 00:14:58.629
When you go home tonight, you'll put
your hand out, reach for the doorknob,

00:14:58.653 --> 00:15:00.167
notice it's in the wrong spot

00:15:00.191 --> 00:15:01.878
and go, "Whoa, something happened."

00:15:01.902 --> 00:15:04.003
It may take a second,
but something happened.

00:15:04.027 --> 00:15:06.030
I can change your doorknob
in other ways --

00:15:06.054 --> 00:15:09.295
make it larger, smaller, change
its brass to silver, make it a lever,

00:15:09.319 --> 00:15:11.895
I can change the door;
put colors on, put windows in.

00:15:11.919 --> 00:15:14.070
I can change a thousand things
about your door

00:15:14.094 --> 00:15:16.102
and in the two seconds
you take to open it,

00:15:16.126 --> 00:15:17.848
you'll notice something has changed.

00:15:17.872 --> 00:15:20.456
Now, the engineering approach,
the AI approach to this,

00:15:20.480 --> 00:15:23.155
is to build a door database
with all the door attributes.

00:15:23.179 --> 00:15:25.998
And as you go up to the door,
we check them off one at time:

00:15:26.022 --> 00:15:27.368
door, door, color ...

00:15:27.392 --> 00:15:29.492
We don't do that.
Your brain doesn't do that.

00:15:29.516 --> 00:15:32.056
Your brain is making
constant predictions all the time

00:15:32.080 --> 00:15:34.114
about what will happen
in your environment.

00:15:34.138 --> 00:15:36.884
As I put my hand on this table,
I expect to feel it stop.

00:15:36.908 --> 00:15:39.927
When I walk, every step,
if I missed it by an eighth of an inch,

00:15:39.951 --> 00:15:41.484
I'll know something has changed.

00:15:41.508 --> 00:15:44.328
You're constantly making predictions
about your environment.

00:15:44.352 --> 00:15:45.945
I'll talk about vision, briefly.

00:15:45.969 --> 00:15:47.352
This is a picture of a woman.

00:15:47.376 --> 00:15:50.866
When we look at people, our eyes saccade
over two to three times a second.

00:15:50.890 --> 00:15:53.419
We're not aware of it,
but our eyes are always moving.

00:15:53.443 --> 00:15:56.878
When we look at a face, we typically
go from eye to eye to nose to mouth.

00:15:56.902 --> 00:15:58.771
When your eye moves from eye to eye,

00:15:58.795 --> 00:16:00.953
if there was something
else there like a nose,

00:16:00.977 --> 00:16:04.523
you'd see a nose where an eye
is supposed to be and go, "Oh, shit!"

00:16:04.547 --> 00:16:05.943
(Laughter)

00:16:05.967 --> 00:16:08.076
"There's something wrong
about this person."

00:16:08.100 --> 00:16:10.105
That's because you're making a prediction.

00:16:10.129 --> 00:16:13.568
It's not like you just look over and say,
"What am I seeing? A nose? OK."

00:16:13.592 --> 00:16:16.226
No, you have an expectation
of what you're going to see.

00:16:16.250 --> 00:16:17.401
Every single moment.

00:16:17.425 --> 00:16:20.054
And finally, let's think
about how we test intelligence.

00:16:20.078 --> 00:16:23.159
We test it by prediction:
What is the next word in this ...?

00:16:23.183 --> 00:16:26.810
This is to this as this is to this.
What is the next number in this sentence?

00:16:26.834 --> 00:16:29.524
Here's three visions of an object.
What's the fourth one?

00:16:29.548 --> 00:16:32.052
That's how we test it.
It's all about prediction.

00:16:32.573 --> 00:16:34.767
So what is the recipe for brain theory?

00:16:35.219 --> 00:16:37.585
First of all, we have to have
the right framework.

00:16:37.609 --> 00:16:39.522
And the framework is a memory framework,

00:16:39.546 --> 00:16:41.570
not a computational or behavior framework,

00:16:41.594 --> 00:16:42.757
it's a memory framework.

00:16:42.781 --> 00:16:45.404
How do you store and recall
these sequences of patterns?

00:16:45.428 --> 00:16:46.870
It's spatiotemporal patterns.

00:16:46.894 --> 00:16:49.903
Then, if in that framework,
you take a bunch of theoreticians --

00:16:49.927 --> 00:16:52.173
biologists generally
are not good theoreticians.

00:16:52.197 --> 00:16:55.726
Not always, but generally, there's not
a good history of theory in biology.

00:16:55.750 --> 00:16:58.324
I've found the best people
to work with are physicists,

00:16:58.348 --> 00:16:59.731
engineers and mathematicians,

00:16:59.755 --> 00:17:01.451
who tend to think algorithmically.

00:17:01.475 --> 00:17:04.739
Then they have to learn
the anatomy and the physiology.

00:17:04.763 --> 00:17:09.259
You have to make these theories
very realistic in anatomical terms.

00:17:09.283 --> 00:17:12.048
Anyone who tells you their theory
about how the brain works

00:17:12.072 --> 00:17:14.169
and doesn't tell you exactly
how it's working

00:17:14.193 --> 00:17:15.496
and how the wiring works --

00:17:15.520 --> 00:17:16.787
it's not a theory.

00:17:16.811 --> 00:17:19.644
And that's what we do
at the Redwood Neuroscience Institute.

00:17:19.668 --> 00:17:22.976
I'd love to tell you we're making
fantastic progress in this thing,

00:17:23.000 --> 00:17:26.662
and I expect to be back on this stage
sometime in the not too distant future,

00:17:26.686 --> 00:17:27.850
to tell you about it.

00:17:27.874 --> 00:17:30.468
I'm really excited;
this is not going to take 50 years.

00:17:30.492 --> 00:17:32.070
What will brain theory look like?

00:17:32.094 --> 00:17:34.149
First of all, it's going
to be about memory.

00:17:34.173 --> 00:17:36.995
Not like computer memory --
not at all like computer memory.

00:17:37.019 --> 00:17:38.170
It's very different.

00:17:38.194 --> 00:17:40.451
It's a memory of very
high-dimensional patterns,

00:17:40.475 --> 00:17:42.437
like the things that come from your eyes.

00:17:42.461 --> 00:17:43.898
It's also memory of sequences:

00:17:43.922 --> 00:17:46.652
you cannot learn or recall anything
outside of a sequence.

00:17:46.676 --> 00:17:49.513
A song must be heard
in sequence over time,

00:17:49.537 --> 00:17:51.888
and you must play it back
in sequence over time.

00:17:51.912 --> 00:17:54.361
And these sequences
are auto-associatively recalled,

00:17:54.385 --> 00:17:57.258
so if I see something, I hear something,
it reminds me of it,

00:17:57.282 --> 00:17:58.815
and it plays back automatically.

00:17:58.839 --> 00:18:00.133
It's an automatic playback.

00:18:00.157 --> 00:18:02.705
And prediction of future inputs
is the desired output.

00:18:02.729 --> 00:18:05.349
And as I said, the theory
must be biologically accurate,

00:18:05.373 --> 00:18:07.857
it must be testable
and you must be able to build it.

00:18:07.881 --> 00:18:10.092
If you don't build it,
you don't understand it.

00:18:10.116 --> 00:18:11.648
One more slide.

00:18:11.672 --> 00:18:13.981
What is this going to result in?

00:18:14.005 --> 00:18:16.353
Are we going to really build
intelligent machines?

00:18:16.377 --> 00:18:20.175
Absolutely. And it's going to be
different than people think.

00:18:20.508 --> 00:18:22.900
No doubt that it's going
to happen, in my mind.

00:18:22.924 --> 00:18:26.040
First of all, we're going to build
this stuff out of silicon.

00:18:26.064 --> 00:18:28.976
The same techniques we use to build
silicon computer memories,

00:18:29.000 --> 00:18:30.151
we can use here.

00:18:30.175 --> 00:18:32.284
But they're very different
types of memories.

00:18:32.308 --> 00:18:34.331
And we'll attach
these memories to sensors,

00:18:34.355 --> 00:18:37.132
and the sensors will experience
real-live, real-world data,

00:18:37.156 --> 00:18:38.908
and learn about their environment.

00:18:38.932 --> 00:18:42.377
Now, it's very unlikely the first things
you'll see are like robots.

00:18:42.401 --> 00:18:44.976
Not that robots aren't useful;
people can build robots.

00:18:45.000 --> 00:18:48.767
But the robotics part is the hardest part.
That's old brain. That's really hard.

00:18:48.791 --> 00:18:50.798
The new brain is easier
than the old brain.

00:18:50.822 --> 00:18:53.904
So first we'll do things
that don't require a lot of robotics.

00:18:53.928 --> 00:18:56.107
So you're not going to see C-3PO.

00:18:56.131 --> 00:18:58.616
You're going to see things
more like intelligent cars

00:18:58.640 --> 00:19:01.448
that really understand
what traffic is, what driving is

00:19:01.472 --> 00:19:04.750
and have learned that cars
with the blinkers on for half a minute

00:19:04.774 --> 00:19:06.348
probably aren't going to turn.

00:19:06.372 --> 00:19:07.663
(Laughter)

00:19:07.687 --> 00:19:09.751
We can also do intelligent
security systems.

00:19:09.775 --> 00:19:13.348
Anytime we're basically using our brain
but not doing a lot of mechanics --

00:19:13.372 --> 00:19:15.431
those are the things
that will happen first.

00:19:15.455 --> 00:19:17.275
But ultimately, the world's the limit.

00:19:17.299 --> 00:19:19.031
I don't know how this will turn out.

00:19:19.055 --> 00:19:21.646
I know a lot of people who invented
the microprocessor.

00:19:21.670 --> 00:19:23.834
And if you talk to them,

00:19:23.858 --> 00:19:26.433
they knew what they were doing
was really significant,

00:19:26.457 --> 00:19:28.957
but they didn't really know
what was going to happen.

00:19:28.981 --> 00:19:31.749
They couldn't anticipate
cell phones and the Internet

00:19:31.773 --> 00:19:33.508
and all this kind of stuff.

00:19:33.532 --> 00:19:36.153
They just knew like,
"We're going to build calculators

00:19:36.177 --> 00:19:37.617
and traffic-light controllers.

00:19:37.641 --> 00:19:38.940
But it's going to be big!"

00:19:38.964 --> 00:19:41.305
In the same way, brain science
and these memories

00:19:41.329 --> 00:19:43.554
are going to be a very
fundamental technology,

00:19:43.578 --> 00:19:47.020
and it will lead to unbelievable changes
in the next 100 years.

00:19:47.044 --> 00:19:50.449
And I'm most excited about
how we're going to use them in science.

00:19:50.473 --> 00:19:53.310
So I think that's all my time -- I'm over,

00:19:53.334 --> 00:19:55.611
and I'm going to end my talk right there.

