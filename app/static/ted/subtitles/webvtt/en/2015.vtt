WEBVTT

00:00:00.988 --> 00:00:03.304
Let me tell you a story.

00:00:03.304 --> 00:00:05.103
It goes back 200 million years.

00:00:05.103 --> 00:00:07.087
It's a story of the neocortex,

00:00:07.087 --> 00:00:09.061
which means "new rind."

00:00:09.061 --> 00:00:11.492
So in these early mammals,

00:00:11.492 --> 00:00:13.547
because only mammals have a neocortex,

00:00:13.547 --> 00:00:15.211
rodent-like creatures.

00:00:15.211 --> 00:00:18.790
It was the size of a postage stamp and just as thin,

00:00:18.790 --> 00:00:20.229
and was a thin covering around

00:00:20.229 --> 00:00:22.493
their walnut-sized brain,

00:00:22.493 --> 00:00:26.194
but it was capable of a new type of thinking.

00:00:26.194 --> 00:00:27.761
Rather than the fixed behaviors

00:00:27.761 --> 00:00:29.753
that non-mammalian animals have,

00:00:29.753 --> 00:00:32.445
it could invent new behaviors.

00:00:32.445 --> 00:00:34.998
So a mouse is escaping a predator,

00:00:34.998 --> 00:00:36.538
its path is blocked,

00:00:36.538 --> 00:00:38.667
it'll try to invent a new solution.

00:00:38.667 --> 00:00:39.933
That may work, it may not,

00:00:39.933 --> 00:00:41.843
but if it does, it will remember that

00:00:41.843 --> 00:00:43.135
and have a new behavior,

00:00:43.135 --> 00:00:44.592
and that can actually spread virally

00:00:44.592 --> 00:00:46.787
through the rest of the community.

00:00:46.787 --> 00:00:48.396
Another mouse watching this could say,

00:00:48.396 --> 00:00:51.100
"Hey, that was pretty clever, going around that rock,"

00:00:51.100 --> 00:00:54.825
and it could adopt a new behavior as well.

00:00:54.825 --> 00:00:56.542
Non-mammalian animals

00:00:56.542 --> 00:00:58.255
couldn't do any of those things.

00:00:58.255 --> 00:00:59.470
They had fixed behaviors.

00:00:59.470 --> 00:01:00.801
Now they could learn a new behavior

00:01:00.801 --> 00:01:03.377
but not in the course of one lifetime.

00:01:03.377 --> 00:01:05.144
In the course of maybe a thousand lifetimes,

00:01:05.144 --> 00:01:08.474
it could evolve a new fixed behavior.

00:01:08.474 --> 00:01:11.851
That was perfectly okay 200 million years ago.

00:01:11.851 --> 00:01:13.832
The environment changed very slowly.

00:01:13.832 --> 00:01:15.386
It could take 10,000 years for there to be

00:01:15.386 --> 00:01:17.478
a significant environmental change,

00:01:17.478 --> 00:01:18.860
and during that period of time

00:01:18.860 --> 00:01:21.789
it would evolve a new behavior.

00:01:21.789 --> 00:01:23.310
Now that went along fine,

00:01:23.310 --> 00:01:25.014
but then something happened.

00:01:25.014 --> 00:01:27.260
Sixty-five million years ago,

00:01:27.260 --> 00:01:29.875
there was a sudden, violent
change to the environment.

00:01:29.875 --> 00:01:33.380
We call it the Cretaceous extinction event.

00:01:33.380 --> 00:01:35.673
That's when the dinosaurs went extinct,

00:01:35.673 --> 00:01:39.122
that's when 75 percent of the

00:01:39.122 --> 00:01:41.868
animal and plant species went extinct,

00:01:41.868 --> 00:01:43.613
and that's when mammals

00:01:43.613 --> 00:01:45.765
overtook their ecological niche,

00:01:45.765 --> 00:01:49.419
and to anthropomorphize, biological evolution said,

00:01:49.419 --> 00:01:51.444
"Hmm, this neocortex is pretty good stuff,"

00:01:51.444 --> 00:01:53.237
and it began to grow it.

00:01:53.237 --> 00:01:54.579
And mammals got bigger,

00:01:54.579 --> 00:01:57.494
their brains got bigger at an even faster pace,

00:01:57.494 --> 00:02:01.301
and the neocortex got bigger even faster than that

00:02:01.301 --> 00:02:04.230
and developed these distinctive ridges and folds

00:02:04.230 --> 00:02:07.111
basically to increase its surface area.

00:02:07.111 --> 00:02:08.930
If you took the human neocortex

00:02:08.930 --> 00:02:10.231
and stretched it out,

00:02:10.231 --> 00:02:11.944
it's about the size of a table napkin,

00:02:11.944 --> 00:02:13.250
and it's still a thin structure.

00:02:13.250 --> 00:02:15.230
It's about the thickness of a table napkin.

00:02:15.230 --> 00:02:17.727
But it has so many convolutions and ridges

00:02:17.727 --> 00:02:20.802
it's now 80 percent of our brain,

00:02:20.802 --> 00:02:23.263
and that's where we do our thinking,

00:02:23.263 --> 00:02:25.024
and it's the great sublimator.

00:02:25.024 --> 00:02:26.138
We still have that old brain

00:02:26.138 --> 00:02:28.902
that provides our basic drives and motivations,

00:02:28.902 --> 00:02:31.618
but I may have a drive for conquest,

00:02:31.618 --> 00:02:34.333
and that'll be sublimated by the neocortex

00:02:34.333 --> 00:02:37.242
into writing a poem or inventing an app

00:02:37.242 --> 00:02:38.751
or giving a TED Talk,

00:02:38.751 --> 00:02:42.373
and it's really the neocortex that's where

00:02:42.373 --> 00:02:44.341
the action is.

00:02:44.341 --> 00:02:46.058
Fifty years ago, I wrote a paper

00:02:46.058 --> 00:02:47.976
describing how I thought the brain worked,

00:02:47.976 --> 00:02:51.175
and I described it as a series of modules.

00:02:51.175 --> 00:02:53.303
Each module could do things with a pattern.

00:02:53.303 --> 00:02:56.049
It could learn a pattern. It could remember a pattern.

00:02:56.049 --> 00:02:57.456
It could implement a pattern.

00:02:57.456 --> 00:03:00.135
And these modules were organized in hierarchies,

00:03:00.135 --> 00:03:03.089
and we created that hierarchy with our own thinking.

00:03:03.089 --> 00:03:06.422
And there was actually very little to go on

00:03:06.422 --> 00:03:07.984
50 years ago.

00:03:07.984 --> 00:03:10.099
It led me to meet President Johnson.

00:03:10.099 --> 00:03:12.272
I've been thinking about this for 50 years,

00:03:12.272 --> 00:03:15.100
and a year and a half ago I came out with the book

00:03:15.100 --> 00:03:16.365
"How To Create A Mind,"

00:03:16.365 --> 00:03:17.978
which has the same thesis,

00:03:17.978 --> 00:03:20.790
but now there's a plethora of evidence.

00:03:20.790 --> 00:03:22.604
The amount of data we're getting about the brain

00:03:22.604 --> 00:03:24.807
from neuroscience is doubling every year.

00:03:24.807 --> 00:03:27.461
Spatial resolution of brainscanning of all types

00:03:27.461 --> 00:03:29.746
is doubling every year.

00:03:29.746 --> 00:03:31.463
We can now see inside a living brain

00:03:31.463 --> 00:03:34.333
and see individual interneural connections

00:03:34.333 --> 00:03:37.036
connecting in real time, firing in real time.

00:03:37.036 --> 00:03:39.455
We can see your brain create your thoughts.

00:03:39.455 --> 00:03:41.030
We can see your thoughts create your brain,

00:03:41.030 --> 00:03:43.029
which is really key to how it works.

00:03:43.029 --> 00:03:45.248
So let me describe briefly how it works.

00:03:45.248 --> 00:03:47.523
I've actually counted these modules.

00:03:47.523 --> 00:03:49.569
We have about 300 million of them,

00:03:49.569 --> 00:03:51.798
and we create them in these hierarchies.

00:03:51.798 --> 00:03:53.880
I'll give you a simple example.

00:03:53.880 --> 00:03:56.685
I've got a bunch of modules

00:03:56.685 --> 00:04:00.088
that can recognize the crossbar to a capital A,

00:04:00.088 --> 00:04:02.002
and that's all they care about.

00:04:02.002 --> 00:04:03.580
A beautiful song can play,

00:04:03.580 --> 00:04:05.014
a pretty girl could walk by,

00:04:05.014 --> 00:04:07.860
they don't care, but they see
a crossbar to a capital A,

00:04:07.860 --> 00:04:10.881
they get very excited and they say "crossbar,"

00:04:10.881 --> 00:04:12.993
and they put out a high probability

00:04:12.993 --> 00:04:14.627
on their output axon.

00:04:14.627 --> 00:04:15.960
That goes to the next level,

00:04:15.960 --> 00:04:18.710
and these layers are organized in conceptual levels.

00:04:18.710 --> 00:04:20.566
Each is more abstract than the next one,

00:04:20.566 --> 00:04:22.984
so the next one might say "capital A."

00:04:22.984 --> 00:04:25.875
That goes up to a higher
level that might say "Apple."

00:04:25.875 --> 00:04:28.042
Information flows down also.

00:04:28.042 --> 00:04:30.978
If the apple recognizer has seen A-P-P-L,

00:04:30.978 --> 00:04:34.197
it'll think to itself, "Hmm, I
think an E is probably likely,"

00:04:34.197 --> 00:04:36.761
and it'll send a signal down to all the E recognizers

00:04:36.761 --> 00:04:38.380
saying, "Be on the lookout for an E,

00:04:38.380 --> 00:04:39.936
I think one might be coming."

00:04:39.936 --> 00:04:42.779
The E recognizers will lower their threshold

00:04:42.779 --> 00:04:44.724
and they see some sloppy
thing, could be an E.

00:04:44.724 --> 00:04:46.214
Ordinarily you wouldn't think so,

00:04:46.214 --> 00:04:48.223
but we're expecting an E, it's good enough,

00:04:48.223 --> 00:04:50.040
and yeah, I've seen an E, and then apple says,

00:04:50.040 --> 00:04:51.768
"Yeah, I've seen an Apple."

00:04:51.768 --> 00:04:53.514
Go up another five levels,

00:04:53.514 --> 00:04:54.867
and you're now at a pretty high level

00:04:54.867 --> 00:04:56.436
of this hierarchy,

00:04:56.436 --> 00:04:58.789
and stretch down into the different senses,

00:04:58.789 --> 00:05:01.444
and you may have a module
that sees a certain fabric,

00:05:01.444 --> 00:05:04.288
hears a certain voice quality,
smells a certain perfume,

00:05:04.288 --> 00:05:06.801
and will say, "My wife has entered the room."

00:05:06.801 --> 00:05:08.696
Go up another 10 levels, and now you're at

00:05:08.696 --> 00:05:09.856
a very high level.

00:05:09.856 --> 00:05:11.793
You're probably in the frontal cortex,

00:05:11.793 --> 00:05:15.560
and you'll have modules that say, "That was ironic.

00:05:15.560 --> 00:05:17.930
That's funny. She's pretty."

00:05:17.930 --> 00:05:20.035
You might think that those are more sophisticated,

00:05:20.035 --> 00:05:21.541
but actually what's more complicated

00:05:21.541 --> 00:05:24.210
is the hierarchy beneath them.

00:05:24.210 --> 00:05:26.830
There was a 16-year-old girl, she had brain surgery,

00:05:26.830 --> 00:05:28.881
and she was conscious because the surgeons

00:05:28.881 --> 00:05:30.418
wanted to talk to her.

00:05:30.418 --> 00:05:32.240
You can do that because there's no pain receptors

00:05:32.240 --> 00:05:33.278
in the brain.

00:05:33.278 --> 00:05:35.078
And whenever they stimulated particular,

00:05:35.078 --> 00:05:37.541
very small points on her neocortex,

00:05:37.541 --> 00:05:40.206
shown here in red, she would laugh.

00:05:40.206 --> 00:05:41.646
So at first they thought they were triggering

00:05:41.646 --> 00:05:43.366
some kind of laugh reflex,

00:05:43.366 --> 00:05:45.885
but no, they quickly realized they had found

00:05:45.885 --> 00:05:48.929
the points in her neocortex that detect humor,

00:05:48.929 --> 00:05:50.898
and she just found everything hilarious

00:05:50.898 --> 00:05:53.335
whenever they stimulated these points.

00:05:53.335 --> 00:05:55.260
"You guys are so funny just standing around,"

00:05:55.260 --> 00:05:56.998
was the typical comment,

00:05:56.998 --> 00:05:59.300
and they weren't funny,

00:05:59.300 --> 00:06:02.547
not while doing surgery.

00:06:02.547 --> 00:06:07.377
So how are we doing today?

00:06:07.377 --> 00:06:10.431
Well, computers are actually beginning to master

00:06:10.431 --> 00:06:12.432
human language with techniques

00:06:12.432 --> 00:06:15.299
that are similar to the neocortex.

00:06:15.299 --> 00:06:16.813
I actually described the algorithm,

00:06:16.813 --> 00:06:18.867
which is similar to something called

00:06:18.867 --> 00:06:21.100
a hierarchical hidden Markov model,

00:06:21.100 --> 00:06:24.341
something I've worked on since the '90s.

00:06:24.341 --> 00:06:27.579
"Jeopardy" is a very broad natural language game,

00:06:27.579 --> 00:06:29.471
and Watson got a higher score

00:06:29.471 --> 00:06:31.471
than the best two players combined.

00:06:31.471 --> 00:06:33.970
It got this query correct:

00:06:33.970 --> 00:06:36.055
"A long, tiresome speech

00:06:36.055 --> 00:06:38.207
delivered by a frothy pie topping,"

00:06:38.207 --> 00:06:41.003
and it quickly responded,
"What is a meringue harangue?"

00:06:41.003 --> 00:06:43.638
And Jennings and the other guy didn't get that.

00:06:43.638 --> 00:06:45.564
It's a pretty sophisticated example of

00:06:45.564 --> 00:06:47.478
computers actually understanding human language,

00:06:47.478 --> 00:06:49.130
and it actually got its knowledge by reading

00:06:49.130 --> 00:06:52.915
Wikipedia and several other encyclopedias.

00:06:52.915 --> 00:06:55.048
Five to 10 years from now,

00:06:55.048 --> 00:06:57.232
search engines will actually be based on

00:06:57.232 --> 00:07:00.026
not just looking for combinations of words and links

00:07:00.026 --> 00:07:01.940
but actually understanding,

00:07:01.940 --> 00:07:04.351
reading for understanding the billions of pages

00:07:04.351 --> 00:07:07.084
on the web and in books.

00:07:07.084 --> 00:07:09.700
So you'll be walking along, and Google will pop up

00:07:09.700 --> 00:07:12.781
and say, "You know, Mary, you expressed concern

00:07:12.781 --> 00:07:15.800
to me a month ago that your glutathione supplement

00:07:15.800 --> 00:07:18.031
wasn't getting past the blood-brain barrier.

00:07:18.031 --> 00:07:20.624
Well, new research just came out 13 seconds ago

00:07:20.624 --> 00:07:22.335
that shows a whole new approach to that

00:07:22.335 --> 00:07:24.328
and a new way to take glutathione.

00:07:24.328 --> 00:07:26.890
Let me summarize it for you."

00:07:26.890 --> 00:07:30.574
Twenty years from now, we'll have nanobots,

00:07:30.574 --> 00:07:32.201
because another exponential trend

00:07:32.201 --> 00:07:33.816
is the shrinking of technology.

00:07:33.816 --> 00:07:36.186
They'll go into our brain

00:07:36.186 --> 00:07:37.889
through the capillaries

00:07:37.889 --> 00:07:40.366
and basically connect our neocortex

00:07:40.366 --> 00:07:43.551
to a synthetic neocortex in the cloud

00:07:43.551 --> 00:07:47.142
providing an extension of our neocortex.

00:07:47.142 --> 00:07:48.720
Now today, I mean,

00:07:48.720 --> 00:07:50.250
you have a computer in your phone,

00:07:50.250 --> 00:07:53.004
but if you need 10,000 computers for a few seconds

00:07:53.004 --> 00:07:54.499
to do a complex search,

00:07:54.499 --> 00:07:57.895
you can access that for a second or two in the cloud.

00:07:57.895 --> 00:08:00.990
In the 2030s, if you need some extra neocortex,

00:08:00.990 --> 00:08:03.263
you'll be able to connect to that in the cloud

00:08:03.263 --> 00:08:04.911
directly from your brain.

00:08:04.911 --> 00:08:06.454
So I'm walking along and I say,

00:08:06.454 --> 00:08:07.817
"Oh, there's Chris Anderson.

00:08:07.817 --> 00:08:09.342
He's coming my way.

00:08:09.342 --> 00:08:11.677
I'd better think of something clever to say.

00:08:11.677 --> 00:08:13.201
I've got three seconds.

00:08:13.201 --> 00:08:16.298
My 300 million modules in my neocortex

00:08:16.298 --> 00:08:17.538
isn't going to cut it.

00:08:17.538 --> 00:08:18.784
I need a billion more."

00:08:18.784 --> 00:08:22.107
I'll be able to access that in the cloud.

00:08:22.107 --> 00:08:24.919
And our thinking, then, will be a hybrid

00:08:24.919 --> 00:08:28.441
of biological and non-biological thinking,

00:08:28.441 --> 00:08:30.339
but the non-biological portion

00:08:30.339 --> 00:08:33.021
is subject to my law of accelerating returns.

00:08:33.021 --> 00:08:35.260
It will grow exponentially.

00:08:35.260 --> 00:08:37.276
And remember what happens

00:08:37.276 --> 00:08:39.921
the last time we expanded our neocortex?

00:08:39.921 --> 00:08:41.347
That was two million years ago

00:08:41.347 --> 00:08:42.583
when we became humanoids

00:08:42.583 --> 00:08:44.177
and developed these large foreheads.

00:08:44.177 --> 00:08:46.760
Other primates have a slanted brow.

00:08:46.760 --> 00:08:48.505
They don't have the frontal cortex.

00:08:48.505 --> 00:08:52.190
But the frontal cortex is not
really qualitatively different.

00:08:52.190 --> 00:08:54.933
It's a quantitative expansion of neocortex,

00:08:54.933 --> 00:08:57.636
but that additional quantity of thinking

00:08:57.636 --> 00:08:59.415
was the enabling factor for us to take

00:08:59.415 --> 00:09:02.761
a qualitative leap and invent language

00:09:02.761 --> 00:09:04.728
and art and science and technology

00:09:04.728 --> 00:09:06.182
and TED conferences.

00:09:06.182 --> 00:09:08.313
No other species has done that.

00:09:08.313 --> 00:09:10.388
And so, over the next few decades,

00:09:10.388 --> 00:09:12.148
we're going to do it again.

00:09:12.148 --> 00:09:14.422
We're going to again expand our neocortex,

00:09:14.422 --> 00:09:16.178
only this time we won't be limited

00:09:16.178 --> 00:09:20.458
by a fixed architecture of enclosure.

00:09:20.458 --> 00:09:23.762
It'll be expanded without limit.

00:09:23.762 --> 00:09:26.005
That additional quantity will again

00:09:26.005 --> 00:09:29.010
be the enabling factor for another qualitative leap

00:09:29.010 --> 00:09:30.645
in culture and technology.

00:09:30.645 --> 00:09:32.699
Thank you very much.

00:09:32.699 --> 00:09:35.785
(Applause)

