WEBVTT

00:00:01.373 --> 00:00:04.722
Most of us think of motion
as a very visual thing.

00:00:05.889 --> 00:00:10.977
If I walk across this stage
or gesture with my hands while I speak,

00:00:10.977 --> 00:00:13.238
that motion is something that you can see.

00:00:14.255 --> 00:00:19.737
But there's a world of important motion
that's too subtle for the human eye,

00:00:19.737 --> 00:00:21.778
and over the past few years,

00:00:21.778 --> 00:00:23.775
we've started to find that cameras

00:00:23.775 --> 00:00:27.185
can often see this motion
even when humans can't.

00:00:28.305 --> 00:00:29.856
So let me show you what I mean.

00:00:30.717 --> 00:00:34.339
On the left here, you see video
of a person's wrist,

00:00:34.339 --> 00:00:37.486
and on the right, you see video
of a sleeping infant,

00:00:37.486 --> 00:00:40.632
but if I didn't tell you
that these were videos,

00:00:40.632 --> 00:00:44.393
you might assume that you were looking
at two regular images,

00:00:44.393 --> 00:00:46.065
because in both cases,

00:00:46.065 --> 00:00:49.112
these videos appear to be
almost completely still.

00:00:50.175 --> 00:00:54.060
But there's actually a lot
of subtle motion going on here,

00:00:54.060 --> 00:00:56.452
and if you were to touch
the wrist on the left,

00:00:56.452 --> 00:00:58.448
you would feel a pulse,

00:00:58.448 --> 00:01:00.933
and if you were to hold
the infant on the right,

00:01:00.933 --> 00:01:03.324
you would feel the rise
and fall of her chest

00:01:03.324 --> 00:01:04.714
as she took each breath.

00:01:05.762 --> 00:01:09.338
And these motions carry
a lot of significance,

00:01:09.338 --> 00:01:12.681
but they're usually
too subtle for us to see,

00:01:12.681 --> 00:01:14.957
so instead, we have to observe them

00:01:14.957 --> 00:01:17.857
through direct contact, through touch.

00:01:18.997 --> 00:01:20.262
But a few years ago,

00:01:20.262 --> 00:01:24.667
my colleagues at MIT developed
what they call a motion microscope,

00:01:24.667 --> 00:01:29.051
which is software that finds
these subtle motions in video

00:01:29.051 --> 00:01:32.613
and amplifies them so that they
become large enough for us to see.

00:01:33.416 --> 00:01:36.899
And so, if we use their software
on the left video,

00:01:36.899 --> 00:01:40.149
it lets us see the pulse in this wrist,

00:01:40.149 --> 00:01:41.844
and if we were to count that pulse,

00:01:41.844 --> 00:01:44.199
we could even figure out
this person's heart rate.

00:01:45.095 --> 00:01:48.160
And if we used the same software
on the right video,

00:01:48.160 --> 00:01:51.387
it lets us see each breath
that this infant takes,

00:01:51.387 --> 00:01:55.524
and we can use this as a contact-free way
to monitor her breathing.

00:01:56.884 --> 00:02:02.232
And so this technology is really powerful
because it takes these phenomena

00:02:02.232 --> 00:02:04.599
that we normally have
to experience through touch

00:02:04.599 --> 00:02:07.556
and it lets us capture them visually
and non-invasively.

00:02:09.104 --> 00:02:13.515
So a couple years ago, I started working
with the folks that created that software,

00:02:13.515 --> 00:02:16.882
and we decided to pursue a crazy idea.

00:02:16.882 --> 00:02:19.575
We thought, it's cool
that we can use software

00:02:19.575 --> 00:02:22.710
to visualize tiny motions like this,

00:02:22.710 --> 00:02:27.168
and you can almost think of it
as a way to extend our sense of touch.

00:02:27.168 --> 00:02:31.227
But what if we could do the same thing
with our ability to hear?

00:02:32.508 --> 00:02:37.173
What if we could use video
to capture the vibrations of sound,

00:02:37.173 --> 00:02:40.000
which are just another kind of motion,

00:02:40.000 --> 00:02:43.346
and turn everything that we see
into a microphone?

00:02:44.236 --> 00:02:46.207
Now, this is a bit of a strange idea,

00:02:46.207 --> 00:02:48.793
so let me try to put it
in perspective for you.

00:02:49.523 --> 00:02:53.011
Traditional microphones
work by converting the motion

00:02:53.011 --> 00:02:56.610
of an internal diaphragm
into an electrical signal,

00:02:56.610 --> 00:03:00.928
and that diaphragm is designed
to move readily with sound

00:03:00.928 --> 00:03:05.735
so that its motion can be recorded
and interpreted as audio.

00:03:05.735 --> 00:03:09.403
But sound causes all objects to vibrate.

00:03:09.403 --> 00:03:14.883
Those vibrations are just usually
too subtle and too fast for us to see.

00:03:14.883 --> 00:03:18.621
So what if we record them
with a high-speed camera

00:03:18.621 --> 00:03:22.197
and then use software
to extract tiny motions

00:03:22.197 --> 00:03:24.287
from our high-speed video,

00:03:24.287 --> 00:03:28.561
and analyze those motions to figure out
what sounds created them?

00:03:29.859 --> 00:03:35.308
This would let us turn visible objects
into visual microphones from a distance.

00:03:37.080 --> 00:03:39.263
And so we tried this out,

00:03:39.263 --> 00:03:41.190
and here's one of our experiments,

00:03:41.190 --> 00:03:44.139
where we took this potted plant
that you see on the right

00:03:44.139 --> 00:03:46.577
and we filmed it with a high-speed camera

00:03:46.577 --> 00:03:50.106
while a nearby loudspeaker
played this sound.

00:03:50.275 --> 00:03:58.465
(Music: "Mary Had a Little Lamb")

00:03:59.820 --> 00:04:02.644
And so here's the video that we recorded,

00:04:02.644 --> 00:04:06.568
and we recorded it at thousands
of frames per second,

00:04:06.568 --> 00:04:08.890
but even if you look very closely,

00:04:08.890 --> 00:04:10.841
all you'll see are some leaves

00:04:10.841 --> 00:04:13.906
that are pretty much
just sitting there doing nothing,

00:04:13.906 --> 00:04:18.712
because our sound only moved those leaves
by about a micrometer.

00:04:19.103 --> 00:04:23.379
That's one ten-thousandth of a centimeter,

00:04:23.379 --> 00:04:27.535
which spans somewhere between
a hundredth and a thousandth

00:04:27.535 --> 00:04:29.834
of a pixel in this image.

00:04:29.881 --> 00:04:32.768
So you can squint all you want,

00:04:32.768 --> 00:04:36.103
but motion that small is pretty much
perceptually invisible.

00:04:37.667 --> 00:04:41.824
But it turns out that something
can be perceptually invisible

00:04:41.824 --> 00:04:44.633
and still be numerically significant,

00:04:44.633 --> 00:04:46.635
because with the right algorithms,

00:04:46.635 --> 00:04:50.322
we can take this silent,
seemingly still video

00:04:50.322 --> 00:04:51.849
and we can recover this sound.

00:04:52.690 --> 00:05:00.074
(Music: "Mary Had a Little Lamb")

00:05:00.074 --> 00:05:05.902
(Applause)

00:05:10.058 --> 00:05:11.997
So how is this possible?

00:05:11.997 --> 00:05:16.341
How can we get so much information
out of so little motion?

00:05:16.341 --> 00:05:21.702
Well, let's say that those leaves
move by just a single micrometer,

00:05:21.702 --> 00:05:26.010
and let's say that that shifts our image
by just a thousandth of a pixel.

00:05:27.269 --> 00:05:29.841
That may not seem like much,

00:05:29.841 --> 00:05:31.837
but a single frame of video

00:05:31.837 --> 00:05:35.094
may have hundreds of thousands
of pixels in it,

00:05:35.094 --> 00:05:38.548
and so if we combine all
of the tiny motions that we see

00:05:38.548 --> 00:05:40.846
from across that entire image,

00:05:40.846 --> 00:05:43.469
then suddenly a thousandth of a pixel

00:05:43.469 --> 00:05:46.244
can start to add up
to something pretty significant.

00:05:46.870 --> 00:05:50.505
On a personal note, we were pretty psyched
when we figured this out.

00:05:50.505 --> 00:05:52.825
(Laughter)

00:05:52.825 --> 00:05:56.078
But even with the right algorithm,

00:05:56.078 --> 00:05:59.695
we were still missing
a pretty important piece of the puzzle.

00:05:59.695 --> 00:06:03.299
You see, there are a lot of factors
that affect when and how well

00:06:03.299 --> 00:06:05.296
this technique will work.

00:06:05.296 --> 00:06:08.500
There's the object and how far away it is;

00:06:08.500 --> 00:06:10.894
there's the camera
and the lens that you use;

00:06:10.894 --> 00:06:14.985
how much light is shining on the object
and how loud your sound is.

00:06:15.945 --> 00:06:19.320
And even with the right algorithm,

00:06:19.320 --> 00:06:22.710
we had to be very careful
with our early experiments,

00:06:22.710 --> 00:06:25.102
because if we got
any of these factors wrong,

00:06:25.102 --> 00:06:27.470
there was no way to tell
what the problem was.

00:06:27.470 --> 00:06:30.117
We would just get noise back.

00:06:30.117 --> 00:06:33.437
And so a lot of our early
experiments looked like this.

00:06:33.437 --> 00:06:35.643
And so here I am,

00:06:35.643 --> 00:06:39.683
and on the bottom left, you can kind of
see our high-speed camera,

00:06:39.683 --> 00:06:41.866
which is pointed at a bag of chips,

00:06:41.866 --> 00:06:44.815
and the whole thing is lit
by these bright lamps.

00:06:44.815 --> 00:06:49.180
And like I said, we had to be
very careful in these early experiments,

00:06:49.180 --> 00:06:51.688
so this is how it went down.

00:06:51.688 --> 00:06:55.449
(Video) Abe Davis: Three, two, one, go.

00:06:55.449 --> 00:07:00.836
Mary had a little lamb!
Little lamb! Little lamb!

00:07:00.836 --> 00:07:05.336
(Laughter)

00:07:05.336 --> 00:07:08.150
AD: So this experiment
looks completely ridiculous.

00:07:08.150 --> 00:07:09.938
(Laughter)

00:07:09.938 --> 00:07:12.283
I mean, I'm screaming at a bag of chips --

00:07:12.283 --> 00:07:13.834
(Laughter) --

00:07:13.834 --> 00:07:15.951
and we're blasting it with so much light,

00:07:15.951 --> 00:07:20.430
we literally melted the first bag
we tried this on. (Laughter)

00:07:20.525 --> 00:07:23.799
But ridiculous as this experiment looks,

00:07:23.799 --> 00:07:25.587
it was actually really important,

00:07:25.587 --> 00:07:28.513
because we were able
to recover this sound.

00:07:28.513 --> 00:07:33.225
(Audio) Mary had a little lamb!
Little lamb! Little lamb!

00:07:33.225 --> 00:07:37.313
(Applause)

00:07:37.313 --> 00:07:39.194
AD: And this was really significant,

00:07:39.194 --> 00:07:43.313
because it was the first time
we recovered intelligible human speech

00:07:43.424 --> 00:07:45.765
from silent video of an object.

00:07:45.765 --> 00:07:48.156
And so it gave us this point of reference,

00:07:48.156 --> 00:07:52.027
and gradually we could start
to modify the experiment,

00:07:52.106 --> 00:07:55.911
using different objects
or moving the object further away,

00:07:55.911 --> 00:07:58.681
using less light or quieter sounds.

00:07:59.887 --> 00:08:02.761
And we analyzed all of these experiments

00:08:02.761 --> 00:08:06.383
until we really understood
the limits of our technique,

00:08:06.383 --> 00:08:08.333
because once we understood those limits,

00:08:08.333 --> 00:08:10.679
we could figure out how to push them.

00:08:10.679 --> 00:08:13.860
And that led to experiments like this one,

00:08:13.860 --> 00:08:16.599
where again, I'm going to speak
to a bag of chips,

00:08:16.599 --> 00:08:21.429
but this time we've moved our camera
about 15 feet away,

00:08:21.429 --> 00:08:24.262
outside, behind a soundproof window,

00:08:24.262 --> 00:08:27.065
and the whole thing is lit
by only natural sunlight.

00:08:28.529 --> 00:08:30.684
And so here's the video that we captured.

00:08:32.450 --> 00:08:37.009
And this is what things sounded like
from inside, next to the bag of chips.

00:08:37.009 --> 00:08:42.047
(Audio) Mary had a little lamb
whose fleece was white as snow,

00:08:42.047 --> 00:08:47.666
and everywhere that Mary went,
that lamb was sure to go.

00:08:47.666 --> 00:08:51.683
AD: And here's what we were able
to recover from our silent video

00:08:51.683 --> 00:08:54.028
captured outside behind that window.

00:08:54.028 --> 00:08:58.463
(Audio) Mary had a little lamb
whose fleece was white as snow,

00:08:58.463 --> 00:09:03.920
and everywhere that Mary went,
that lamb was sure to go.

00:09:03.920 --> 00:09:10.421
(Applause)

00:09:10.421 --> 00:09:13.963
AD: And there are other ways
that we can push these limits as well.

00:09:13.963 --> 00:09:15.761
So here's a quieter experiment

00:09:15.761 --> 00:09:19.871
where we filmed some earphones
plugged into a laptop computer,

00:09:19.871 --> 00:09:23.981
and in this case, our goal was to recover
the music that was playing on that laptop

00:09:23.981 --> 00:09:26.280
from just silent video

00:09:26.280 --> 00:09:28.787
of these two little plastic earphones,

00:09:28.787 --> 00:09:30.970
and we were able to do this so well

00:09:30.970 --> 00:09:33.431
that I could even Shazam our results.

00:09:33.431 --> 00:09:35.842
(Laughter)

00:09:37.191 --> 00:09:47.225
(Music: "Under Pressure" by Queen)

00:09:49.615 --> 00:09:54.584
(Applause)

00:09:54.584 --> 00:09:59.135
And we can also push things
by changing the hardware that we use.

00:09:59.135 --> 00:10:01.596
Because the experiments
I've shown you so far

00:10:01.596 --> 00:10:03.918
were done with a camera,
a high-speed camera,

00:10:03.918 --> 00:10:06.797
that can record video
about a 100 times faster

00:10:06.797 --> 00:10:08.724
than most cell phones,

00:10:08.724 --> 00:10:11.533
but we've also found a way
to use this technique

00:10:11.533 --> 00:10:13.763
with more regular cameras,

00:10:13.763 --> 00:10:17.832
and we do that by taking advantage
of what's called a rolling shutter.

00:10:17.832 --> 00:10:22.630
You see, most cameras
record images one row at a time,

00:10:22.630 --> 00:10:28.332
and so if an object moves
during the recording of a single image,

00:10:28.344 --> 00:10:31.061
there's a slight time delay
between each row,

00:10:31.061 --> 00:10:34.218
and this causes slight artifacts

00:10:34.218 --> 00:10:37.701
that get coded into each frame of a video.

00:10:37.701 --> 00:10:41.507
And so what we found
is that by analyzing these artifacts,

00:10:41.507 --> 00:10:46.122
we can actually recover sound
using a modified version of our algorithm.

00:10:46.122 --> 00:10:48.034
So here's an experiment we did

00:10:48.034 --> 00:10:49.729
where we filmed a bag of candy

00:10:49.729 --> 00:10:51.470
while a nearby loudspeaker played

00:10:51.470 --> 00:10:54.442
the same "Mary Had a Little Lamb"
music from before,

00:10:54.442 --> 00:10:58.645
but this time, we used just a regular
store-bought camera,

00:10:58.645 --> 00:11:01.819
and so in a second, I'll play for you
the sound that we recovered,

00:11:01.819 --> 00:11:03.869
and it's going to sound
distorted this time,

00:11:03.869 --> 00:11:06.705
but listen and see if you can still
recognize the music.

00:11:07.723 --> 00:11:13.946
(Audio: "Mary Had a Little Lamb")

00:11:25.527 --> 00:11:28.992
And so, again, that sounds distorted,

00:11:28.992 --> 00:11:33.378
but what's really amazing here
is that we were able to do this

00:11:33.378 --> 00:11:36.004
with something
that you could literally run out

00:11:36.004 --> 00:11:37.448
and pick up at a Best Buy.

00:11:39.122 --> 00:11:40.485
So at this point,

00:11:40.485 --> 00:11:42.459
a lot of people see this work,

00:11:42.459 --> 00:11:45.872
and they immediately think
about surveillance.

00:11:45.872 --> 00:11:48.287
And to be fair,

00:11:48.287 --> 00:11:52.420
it's not hard to imagine how you might use
this technology to spy on someone.

00:11:52.420 --> 00:11:56.367
But keep in mind that there's already
a lot of very mature technology

00:11:56.367 --> 00:11:57.946
out there for surveillance.

00:11:57.946 --> 00:12:00.036
In fact, people have been using lasers

00:12:00.036 --> 00:12:02.835
to eavesdrop on objects
from a distance for decades.

00:12:03.978 --> 00:12:06.003
But what's really new here,

00:12:06.003 --> 00:12:07.443
what's really different,

00:12:07.443 --> 00:12:11.738
is that now we have a way
to picture the vibrations of an object,

00:12:11.738 --> 00:12:15.151
which gives us a new lens
through which to look at the world,

00:12:15.151 --> 00:12:16.661
and we can use that lens

00:12:16.661 --> 00:12:21.560
to learn not just about forces like sound
that cause an object to vibrate,

00:12:21.560 --> 00:12:23.848
but also about the object itself.

00:12:24.975 --> 00:12:26.668
And so I want to take a step back

00:12:26.668 --> 00:12:30.917
and think about how that might change
the ways that we use video,

00:12:30.917 --> 00:12:34.470
because we usually use video
to look at things,

00:12:34.470 --> 00:12:36.792
and I've just shown you how we can use it

00:12:36.792 --> 00:12:38.649
to listen to things.

00:12:38.649 --> 00:12:42.620
But there's another important way
that we learn about the world:

00:12:42.620 --> 00:12:44.895
that's by interacting with it.

00:12:44.895 --> 00:12:48.006
We push and pull and poke and prod things.

00:12:48.006 --> 00:12:51.187
We shake things and see what happens.

00:12:51.187 --> 00:12:55.460
And that's something that video
still won't let us do,

00:12:55.460 --> 00:12:57.596
at least not traditionally.

00:12:57.596 --> 00:12:59.546
So I want to show you some new work,

00:12:59.546 --> 00:13:02.213
and this is based on an idea I had
just a few months ago,

00:13:02.213 --> 00:13:05.514
so this is actually the first time
I've shown it to a public audience.

00:13:05.514 --> 00:13:10.877
And the basic idea is that we're going
to use the vibrations in a video

00:13:10.877 --> 00:13:15.358
to capture objects in a way
that will let us interact with them

00:13:15.358 --> 00:13:17.332
and see how they react to us.

00:13:19.120 --> 00:13:20.884
So here's an object,

00:13:20.884 --> 00:13:24.716
and in this case, it's a wire figure
in the shape of a human,

00:13:24.716 --> 00:13:27.804
and we're going to film that object
with just a regular camera.

00:13:27.804 --> 00:13:29.928
So there's nothing special
about this camera.

00:13:29.928 --> 00:13:32.889
In fact, I've actually done this
with my cell phone before.

00:13:32.889 --> 00:13:35.141
But we do want to see the object vibrate,

00:13:35.141 --> 00:13:36.274
so to make that happen,

00:13:36.274 --> 00:13:39.620
we're just going to bang a little bit
on the surface where it's resting

00:13:39.620 --> 00:13:41.758
while we record this video.

00:13:47.398 --> 00:13:51.069
So that's it: just five seconds
of regular video,

00:13:51.069 --> 00:13:53.205
while we bang on this surface,

00:13:53.205 --> 00:13:56.718
and we're going to use
the vibrations in that video

00:13:56.718 --> 00:14:01.262
to learn about the structural
and material properties of our object,

00:14:01.262 --> 00:14:06.096
and we're going to use that information
to create something new and interactive.

00:14:12.866 --> 00:14:15.519
And so here's what we've created.

00:14:15.519 --> 00:14:17.748
And it looks like a regular image,

00:14:17.748 --> 00:14:20.859
but this isn't an image,
and it's not a video,

00:14:20.859 --> 00:14:23.227
because now I can take my mouse

00:14:23.227 --> 00:14:26.086
and I can start interacting
with the object.

00:14:32.936 --> 00:14:35.293
And so what you see here

00:14:35.389 --> 00:14:37.615
is a simulation of how this object

00:14:37.615 --> 00:14:42.073
would respond to new forces
that we've never seen before,

00:14:42.073 --> 00:14:45.706
and we created it from just
five seconds of regular video.

00:14:47.249 --> 00:14:51.964
(Applause)

00:14:57.421 --> 00:15:00.648
And so this is a really powerful
way to look at the world,

00:15:00.648 --> 00:15:03.620
because it lets us predict
how objects will respond

00:15:03.620 --> 00:15:05.443
to new situations,

00:15:05.443 --> 00:15:08.916
and you could imagine, for instance,
looking at an old bridge

00:15:08.916 --> 00:15:12.443
and wondering what would happen,
how would that bridge hold up

00:15:12.443 --> 00:15:15.276
if I were to drive my car across it.

00:15:15.276 --> 00:15:18.050
And that's a question
that you probably want to answer

00:15:18.050 --> 00:15:20.610
before you start driving
across that bridge.

00:15:21.988 --> 00:15:25.260
And of course, there are going to be
limitations to this technique,

00:15:25.260 --> 00:15:27.722
just like there were
with the visual microphone,

00:15:27.722 --> 00:15:30.903
but we found that it works
in a lot of situations

00:15:30.903 --> 00:15:32.778
that you might not expect,

00:15:32.778 --> 00:15:35.546
especially if you give it longer videos.

00:15:35.546 --> 00:15:38.054
So for example,
here's a video that I captured

00:15:38.054 --> 00:15:40.353
of a bush outside of my apartment,

00:15:40.353 --> 00:15:43.441
and I didn't do anything to this bush,

00:15:43.441 --> 00:15:46.146
but by capturing a minute-long video,

00:15:46.146 --> 00:15:49.524
a gentle breeze caused enough vibrations

00:15:49.524 --> 00:15:53.111
that we could learn enough about this bush
to create this simulation.

00:15:55.270 --> 00:16:01.412
(Applause)

00:16:01.412 --> 00:16:04.384
And so you could imagine giving this
to a film director,

00:16:04.384 --> 00:16:06.103
and letting him control, say,

00:16:06.103 --> 00:16:11.025
the strength and direction of wind
in a shot after it's been recorded.

00:16:12.810 --> 00:16:17.345
Or, in this case, we pointed our camera
at a hanging curtain,

00:16:17.345 --> 00:16:21.474
and you can't even see
any motion in this video,

00:16:21.474 --> 00:16:24.399
but by recording a two-minute-long video,

00:16:24.399 --> 00:16:26.837
natural air currents in this room

00:16:26.837 --> 00:16:31.249
created enough subtle,
imperceptible motions and vibrations

00:16:31.249 --> 00:16:33.814
that we could learn enough
to create this simulation.

00:16:36.243 --> 00:16:38.609
And ironically,

00:16:38.609 --> 00:16:41.697
we're kind of used to having
this kind of interactivity

00:16:41.697 --> 00:16:44.344
when it comes to virtual objects,

00:16:44.344 --> 00:16:47.641
when it comes to video games
and 3D models,

00:16:47.641 --> 00:16:52.045
but to be able to capture this information
from real objects in the real world

00:16:52.045 --> 00:16:54.862
using just simple, regular video,

00:16:54.862 --> 00:16:57.045
is something new that has
a lot of potential.

00:16:58.410 --> 00:17:03.314
So here are the amazing people
who worked with me on these projects.

00:17:04.057 --> 00:17:09.653
(Applause)

00:17:12.819 --> 00:17:15.876
And what I've shown you today
is only the beginning.

00:17:15.876 --> 00:17:17.989
We've just started to scratch the surface

00:17:17.989 --> 00:17:20.961
of what you can do
with this kind of imaging,

00:17:20.961 --> 00:17:23.247
because it gives us a new way

00:17:23.342 --> 00:17:28.066
to capture our surroundings
with common, accessible technology.

00:17:28.066 --> 00:17:29.995
And so looking to the future,

00:17:29.995 --> 00:17:32.032
it's going to be
really exciting to explore

00:17:32.032 --> 00:17:33.888
what this can tell us about the world.

00:17:34.381 --> 00:17:35.585
Thank you.

00:17:35.610 --> 00:17:41.717
(Applause)

