WEBVTT

00:00:00.960 --> 00:00:02.160
I want you to imagine

00:00:03.000 --> 00:00:04.200
walking into a room,

00:00:05.480 --> 00:00:07.616
a control room with a bunch of people,

00:00:07.640 --> 00:00:10.480
a hundred people, hunched
over a desk with little dials,

00:00:11.280 --> 00:00:12.800
and that that control room

00:00:13.680 --> 00:00:17.376
will shape the thoughts and feelings

00:00:17.400 --> 00:00:18.640
of a billion people.

00:00:20.560 --> 00:00:22.440
This might sound like science fiction,

00:00:23.320 --> 00:00:25.536
but this actually exists

00:00:25.560 --> 00:00:26.760
right now, today.

00:00:28.040 --> 00:00:31.400
I know because I used to be
in one of those control rooms.

00:00:32.159 --> 00:00:34.456
I was a design ethicist at Google,

00:00:34.480 --> 00:00:37.840
where I studied how do you ethically
steer people's thoughts?

00:00:38.560 --> 00:00:41.456
Because what we don't talk about
is how the handful of people

00:00:41.480 --> 00:00:44.016
working at a handful
of technology companies

00:00:44.040 --> 00:00:49.080
through their choices will steer
what a billion people are thinking today.

00:00:50.400 --> 00:00:52.136
Because when you pull out your phone

00:00:52.160 --> 00:00:55.256
and they design how this works
or what's on the feed,

00:00:55.280 --> 00:00:58.496
it's scheduling little blocks
of time in our minds.

00:00:58.520 --> 00:01:01.656
If you see a notification,
it schedules you to have thoughts

00:01:01.680 --> 00:01:03.720
that maybe you didn't intend to have.

00:01:04.400 --> 00:01:07.016
If you swipe over that notification,

00:01:07.040 --> 00:01:09.421
it schedules you into spending
a little bit of time

00:01:09.445 --> 00:01:10.826
getting sucked into something

00:01:10.850 --> 00:01:13.805
that maybe you didn't intend
to get sucked into.

00:01:15.320 --> 00:01:16.840
When we talk about technology,

00:01:18.040 --> 00:01:20.736
we tend to talk about it
as this blue sky opportunity.

00:01:20.760 --> 00:01:22.240
It could go any direction.

00:01:23.400 --> 00:01:25.256
And I want to get serious for a moment

00:01:25.280 --> 00:01:27.960
and tell you why it's going
in a very specific direction.

00:01:28.840 --> 00:01:31.040
Because it's not evolving randomly.

00:01:32.000 --> 00:01:34.016
There's a hidden goal
driving the direction

00:01:34.040 --> 00:01:36.176
of all of the technology we make,

00:01:36.200 --> 00:01:39.120
and that goal is the race
for our attention.

00:01:40.840 --> 00:01:43.576
Because every news site,

00:01:43.600 --> 00:01:46.336
TED, elections, politicians,

00:01:46.360 --> 00:01:48.336
games, even meditation apps

00:01:48.360 --> 00:01:50.320
have to compete for one thing,

00:01:51.160 --> 00:01:52.896
which is our attention,

00:01:52.920 --> 00:01:54.520
and there's only so much of it.

00:01:56.440 --> 00:01:58.856
And the best way to get people's attention

00:01:58.880 --> 00:02:01.320
is to know how someone's mind works.

00:02:01.800 --> 00:02:04.136
And there's a whole bunch
of persuasive techniques

00:02:04.160 --> 00:02:07.656
that I learned in college at a lab
called the Persuasive Technology Lab

00:02:07.680 --> 00:02:09.280
to get people's attention.

00:02:09.880 --> 00:02:11.360
A simple example is YouTube.

00:02:12.000 --> 00:02:14.936
YouTube wants to maximize
how much time you spend.

00:02:14.960 --> 00:02:16.160
And so what do they do?

00:02:16.840 --> 00:02:19.120
They autoplay the next video.

00:02:19.760 --> 00:02:21.576
And let's say that works really well.

00:02:21.600 --> 00:02:24.016
They're getting a little bit
more of people's time.

00:02:24.040 --> 00:02:26.416
Well, if you're Netflix,
you look at that and say,

00:02:26.440 --> 00:02:28.298
well, that's shrinking my market share,

00:02:28.322 --> 00:02:30.322
so I'm going to autoplay the next episode.

00:02:31.320 --> 00:02:32.696
But then if you're Facebook,

00:02:32.720 --> 00:02:35.056
you say, that's shrinking
all of my market share,

00:02:35.080 --> 00:02:37.736
so now I have to autoplay
all the videos in the newsfeed

00:02:37.760 --> 00:02:39.522
before waiting for you to click play.

00:02:40.320 --> 00:02:43.480
So the internet is not evolving at random.

00:02:44.320 --> 00:02:48.736
The reason it feels
like it's sucking us in the way it is

00:02:48.760 --> 00:02:51.136
is because of this race for attention.

00:02:51.160 --> 00:02:52.576
We know where this is going.

00:02:52.600 --> 00:02:54.120
Technology is not neutral,

00:02:55.320 --> 00:02:58.736
and it becomes this race
to the bottom of the brain stem

00:02:58.760 --> 00:03:00.960
of who can go lower to get it.

00:03:01.920 --> 00:03:04.256
Let me give you an example of Snapchat.

00:03:04.280 --> 00:03:07.976
If you didn't know,
Snapchat is the number one way

00:03:08.000 --> 00:03:10.256
that teenagers in
the United States communicate.

00:03:10.280 --> 00:03:14.456
So if you're like me, and you use
text messages to communicate,

00:03:14.480 --> 00:03:16.256
Snapchat is that for teenagers,

00:03:16.280 --> 00:03:18.976
and there's, like,
a hundred million of them that use it.

00:03:19.000 --> 00:03:21.216
And they invented
a feature called Snapstreaks,

00:03:21.240 --> 00:03:23.136
which shows the number of days in a row

00:03:23.160 --> 00:03:25.776
that two people have
communicated with each other.

00:03:25.800 --> 00:03:27.656
In other words, what they just did

00:03:27.680 --> 00:03:30.640
is they gave two people
something they don't want to lose.

00:03:32.000 --> 00:03:35.456
Because if you're a teenager,
and you have 150 days in a row,

00:03:35.480 --> 00:03:37.456
you don't want that to go away.

00:03:37.480 --> 00:03:41.640
And so think of the little blocks of time
that that schedules in kids' minds.

00:03:42.160 --> 00:03:44.496
This isn't theoretical:
when kids go on vacation,

00:03:44.520 --> 00:03:47.776
it's been shown they give their passwords
to up to five other friends

00:03:47.800 --> 00:03:50.016
to keep their Snapstreaks going,

00:03:50.040 --> 00:03:52.056
even when they can't do it.

00:03:52.080 --> 00:03:54.016
And they have, like, 30 of these things,

00:03:54.040 --> 00:03:57.416
and so they have to get through
taking photos of just pictures or walls

00:03:57.440 --> 00:03:59.920
or ceilings just to get through their day.

00:04:01.200 --> 00:04:03.896
So it's not even like
they're having real conversations.

00:04:03.920 --> 00:04:05.856
We have a temptation to think about this

00:04:05.880 --> 00:04:08.576
as, oh, they're just using Snapchat

00:04:08.600 --> 00:04:10.616
the way we used to
gossip on the telephone.

00:04:10.640 --> 00:04:11.840
It's probably OK.

00:04:12.480 --> 00:04:14.736
Well, what this misses
is that in the 1970s,

00:04:14.760 --> 00:04:17.375
when you were just
gossiping on the telephone,

00:04:17.399 --> 00:04:20.416
there wasn't a hundred engineers
on the other side of the screen

00:04:20.440 --> 00:04:22.496
who knew exactly
how your psychology worked

00:04:22.520 --> 00:04:25.160
and orchestrated you
into a double bind with each other.

00:04:26.440 --> 00:04:29.840
Now, if this is making you
feel a little bit of outrage,

00:04:30.680 --> 00:04:33.256
notice that that thought
just comes over you.

00:04:33.280 --> 00:04:36.600
Outrage is a really good way also
of getting your attention,

00:04:37.880 --> 00:04:39.456
because we don't choose outrage.

00:04:39.480 --> 00:04:40.896
It happens to us.

00:04:40.920 --> 00:04:42.776
And if you're the Facebook newsfeed,

00:04:42.800 --> 00:04:44.216
whether you'd want to or not,

00:04:44.240 --> 00:04:46.976
you actually benefit when there's outrage.

00:04:47.000 --> 00:04:49.936
Because outrage
doesn't just schedule a reaction

00:04:49.960 --> 00:04:52.840
in emotional time, space, for you.

00:04:53.440 --> 00:04:55.856
We want to share that outrage
with other people.

00:04:55.880 --> 00:04:57.456
So we want to hit share and say,

00:04:57.480 --> 00:04:59.520
"Can you believe the thing
that they said?"

00:05:00.520 --> 00:05:03.896
And so outrage works really well
at getting attention,

00:05:03.920 --> 00:05:07.816
such that if Facebook had a choice
between showing you the outrage feed

00:05:07.840 --> 00:05:09.160
and a calm newsfeed,

00:05:10.120 --> 00:05:12.256
they would want
to show you the outrage feed,

00:05:12.280 --> 00:05:14.336
not because someone
consciously chose that,

00:05:14.360 --> 00:05:17.040
but because that worked better
at getting your attention.

00:05:19.120 --> 00:05:24.600
And the newsfeed control room
is not accountable to us.

00:05:25.040 --> 00:05:27.336
It's only accountable
to maximizing attention.

00:05:27.360 --> 00:05:28.576
It's also accountable,

00:05:28.600 --> 00:05:30.976
because of the business model
of advertising,

00:05:31.000 --> 00:05:34.336
for anybody who can pay the most
to actually walk into the control room

00:05:34.360 --> 00:05:35.936
and say, "That group over there,

00:05:35.960 --> 00:05:38.600
I want to schedule these thoughts
into their minds."

00:05:39.760 --> 00:05:40.960
So you can target,

00:05:42.040 --> 00:05:43.976
you can precisely target a lie

00:05:44.000 --> 00:05:46.920
directly to the people
who are most susceptible.

00:05:48.080 --> 00:05:50.960
And because this is profitable,
it's only going to get worse.

00:05:53.040 --> 00:05:54.840
So I'm here today

00:05:56.160 --> 00:05:58.160
because the costs are so obvious.

00:06:00.280 --> 00:06:02.416
I don't know a more urgent
problem than this,

00:06:02.440 --> 00:06:05.560
because this problem
is underneath all other problems.

00:06:06.720 --> 00:06:09.896
It's not just taking away our agency

00:06:09.920 --> 00:06:12.520
to spend our attention
and live the lives that we want,

00:06:13.720 --> 00:06:17.256
it's changing the way
that we have our conversations,

00:06:17.280 --> 00:06:19.016
it's changing our democracy,

00:06:19.040 --> 00:06:21.656
and it's changing our ability
to have the conversations

00:06:21.680 --> 00:06:23.680
and relationships we want with each other.

00:06:25.160 --> 00:06:26.936
And it affects everyone,

00:06:26.960 --> 00:06:30.320
because a billion people
have one of these in their pocket.

00:06:33.360 --> 00:06:34.760
So how do we fix this?

00:06:37.080 --> 00:06:40.016
We need to make three radical changes

00:06:40.040 --> 00:06:41.840
to technology and to our society.

00:06:43.720 --> 00:06:47.520
The first is we need to acknowledge
that we are persuadable.

00:06:48.840 --> 00:06:50.216
Once you start understanding

00:06:50.240 --> 00:06:53.016
that your mind can be scheduled
into having little thoughts

00:06:53.040 --> 00:06:55.616
or little blocks of time
that you didn't choose,

00:06:55.640 --> 00:06:57.696
wouldn't we want to use that understanding

00:06:57.720 --> 00:06:59.880
and protect against the way
that that happens?

00:07:00.600 --> 00:07:03.896
I think we need to see ourselves
fundamentally in a new way.

00:07:03.920 --> 00:07:06.136
It's almost like a new period
of human history,

00:07:06.160 --> 00:07:07.376
like the Enlightenment,

00:07:07.400 --> 00:07:09.616
but almost a kind of
self-aware Enlightenment,

00:07:09.640 --> 00:07:11.720
that we can be persuaded,

00:07:12.320 --> 00:07:14.560
and there might be something
we want to protect.

00:07:15.400 --> 00:07:19.976
The second is we need new models
and accountability systems

00:07:20.000 --> 00:07:23.496
so that as the world gets better
and more and more persuasive over time --

00:07:23.520 --> 00:07:25.856
because it's only going
to get more persuasive --

00:07:25.880 --> 00:07:27.736
that the people in those control rooms

00:07:27.760 --> 00:07:30.216
are accountable and transparent
to what we want.

00:07:30.240 --> 00:07:32.936
The only form of ethical
persuasion that exists

00:07:32.960 --> 00:07:34.896
is when the goals of the persuader

00:07:34.920 --> 00:07:37.120
are aligned with the goals
of the persuadee.

00:07:37.640 --> 00:07:41.480
And that involves questioning big things,
like the business model of advertising.

00:07:42.720 --> 00:07:44.296
Lastly,

00:07:44.320 --> 00:07:46.000
we need a design renaissance,

00:07:47.080 --> 00:07:50.136
because once you have
this view of human nature,

00:07:50.160 --> 00:07:53.136
that you can steer the timelines
of a billion people --

00:07:53.160 --> 00:07:55.896
just imagine, there's people
who have some desire

00:07:55.920 --> 00:07:58.776
about what they want to do
and what they want to be thinking

00:07:58.800 --> 00:08:01.936
and what they want to be feeling
and how they want to be informed,

00:08:01.960 --> 00:08:04.496
and we're all just tugged
into these other directions.

00:08:04.520 --> 00:08:08.216
And you have a billion people just tugged
into all these different directions.

00:08:08.240 --> 00:08:10.296
Well, imagine an entire design renaissance

00:08:10.320 --> 00:08:13.416
that tried to orchestrate
the exact and most empowering

00:08:13.440 --> 00:08:16.576
time-well-spent way
for those timelines to happen.

00:08:16.600 --> 00:08:18.256
And that would involve two things:

00:08:18.280 --> 00:08:20.416
one would be protecting
against the timelines

00:08:20.440 --> 00:08:22.296
that we don't want to be experiencing,

00:08:22.320 --> 00:08:24.736
the thoughts that we
wouldn't want to be happening,

00:08:24.760 --> 00:08:28.096
so that when that ding happens,
not having the ding that sends us away;

00:08:28.120 --> 00:08:31.736
and the second would be empowering us
to live out the timeline that we want.

00:08:31.760 --> 00:08:33.640
So let me give you a concrete example.

00:08:34.280 --> 00:08:36.736
Today, let's say your friend
cancels dinner on you,

00:08:36.760 --> 00:08:40.535
and you are feeling a little bit lonely.

00:08:40.559 --> 00:08:42.376
And so what do you do in that moment?

00:08:42.400 --> 00:08:43.679
You open up Facebook.

00:08:44.960 --> 00:08:46.656
And in that moment,

00:08:46.680 --> 00:08:50.056
the designers in the control room
want to schedule exactly one thing,

00:08:50.080 --> 00:08:53.120
which is to maximize how much time
you spend on the screen.

00:08:54.640 --> 00:08:58.536
Now, instead, imagine if those designers
created a different timeline

00:08:58.560 --> 00:09:02.056
that was the easiest way,
using all of their data,

00:09:02.080 --> 00:09:05.176
to actually help you get out
with the people that you care about?

00:09:05.200 --> 00:09:10.616
Just think, alleviating
all loneliness in society,

00:09:10.640 --> 00:09:14.136
if that was the timeline that Facebook
wanted to make possible for people.

00:09:14.160 --> 00:09:15.875
Or imagine a different conversation.

00:09:15.899 --> 00:09:19.216
Let's say you wanted to post
something supercontroversial on Facebook,

00:09:19.240 --> 00:09:21.656
which is a really important
thing to be able to do,

00:09:21.680 --> 00:09:23.376
to talk about controversial topics.

00:09:23.400 --> 00:09:25.736
And right now, when there's
that big comment box,

00:09:25.760 --> 00:09:29.136
it's almost asking you,
what key do you want to type?

00:09:29.160 --> 00:09:31.976
In other words, it's scheduling
a little timeline of things

00:09:32.000 --> 00:09:34.136
you're going to continue
to do on the screen.

00:09:34.160 --> 00:09:37.136
And imagine instead that there was
another button there saying,

00:09:37.160 --> 00:09:39.216
what would be most
time well spent for you?

00:09:39.240 --> 00:09:40.816
And you click "host a dinner."

00:09:40.840 --> 00:09:42.936
And right there
underneath the item it said,

00:09:42.960 --> 00:09:44.656
"Who wants to RSVP for the dinner?"

00:09:44.680 --> 00:09:47.936
And so you'd still have a conversation
about something controversial,

00:09:47.960 --> 00:09:51.696
but you'd be having it in the most
empowering place on your timeline,

00:09:51.720 --> 00:09:54.736
which would be at home that night
with a bunch of a friends over

00:09:54.760 --> 00:09:55.960
to talk about it.

00:09:57.000 --> 00:10:00.160
So imagine we're running, like,
a find and replace

00:10:01.000 --> 00:10:03.576
on all of the timelines
that are currently steering us

00:10:03.600 --> 00:10:06.160
towards more and more
screen time persuasively

00:10:07.080 --> 00:10:09.616
and replacing all of those timelines

00:10:09.640 --> 00:10:11.280
with what do we want in our lives.

00:10:14.960 --> 00:10:16.440
It doesn't have to be this way.

00:10:18.360 --> 00:10:20.616
Instead of handicapping our attention,

00:10:20.640 --> 00:10:23.456
imagine if we used all of this data
and all of this power

00:10:23.480 --> 00:10:25.096
and this new view of human nature

00:10:25.120 --> 00:10:27.976
to give us a superhuman ability to focus

00:10:28.000 --> 00:10:32.136
and a superhuman ability to put
our attention to what we cared about

00:10:32.160 --> 00:10:34.776
and a superhuman ability
to have the conversations

00:10:34.800 --> 00:10:36.800
that we need to have for democracy.

00:10:39.600 --> 00:10:42.280
The most complex challenges in the world

00:10:44.280 --> 00:10:47.400
require not just us
to use our attention individually.

00:10:48.440 --> 00:10:51.760
They require us to use our attention
and coordinate it together.

00:10:52.440 --> 00:10:55.256
Climate change is going to require
that a lot of people

00:10:55.280 --> 00:10:57.376
are being able
to coordinate their attention

00:10:57.400 --> 00:10:59.296
in the most empowering way together.

00:10:59.320 --> 00:11:02.400
And imagine creating
a superhuman ability to do that.

00:11:07.000 --> 00:11:11.160
Sometimes the world's
most pressing and important problems

00:11:12.040 --> 00:11:15.880
are not these hypothetical future things
that we could create in the future.

00:11:16.560 --> 00:11:18.296
Sometimes the most pressing problems

00:11:18.320 --> 00:11:20.656
are the ones that are
right underneath our noses,

00:11:20.680 --> 00:11:23.800
the things that are already directing
a billion people's thoughts.

00:11:24.600 --> 00:11:27.976
And maybe instead of getting excited
about the new augmented reality

00:11:28.000 --> 00:11:31.296
and virtual reality
and these cool things that could happen,

00:11:31.320 --> 00:11:34.616
which are going to be susceptible
to the same race for attention,

00:11:34.640 --> 00:11:36.816
if we could fix the race for attention

00:11:36.840 --> 00:11:39.560
on the thing that's already
in a billion people's pockets.

00:11:40.040 --> 00:11:41.616
Maybe instead of getting excited

00:11:41.640 --> 00:11:45.816
about the most exciting
new cool fancy education apps,

00:11:45.840 --> 00:11:48.736
we could fix the way
kids' minds are getting manipulated

00:11:48.760 --> 00:11:51.240
into sending empty messages
back and forth.

00:11:52.040 --> 00:11:56.336
(Applause)

00:11:56.360 --> 00:11:57.616
Maybe instead of worrying

00:11:57.640 --> 00:12:01.416
about hypothetical future
runaway artificial intelligences

00:12:01.440 --> 00:12:03.320
that are maximizing for one goal,

00:12:04.680 --> 00:12:07.336
we could solve the runaway
artificial intelligence

00:12:07.360 --> 00:12:09.416
that already exists right now,

00:12:09.440 --> 00:12:12.360
which are these newsfeeds
maximizing for one thing.

00:12:14.080 --> 00:12:17.896
It's almost like instead of running away
to colonize new planets,

00:12:17.920 --> 00:12:19.976
we could fix the one
that we're already on.

00:12:20.000 --> 00:12:24.120
(Applause)

00:12:28.040 --> 00:12:29.816
Solving this problem

00:12:29.840 --> 00:12:33.640
is critical infrastructure
for solving every other problem.

00:12:34.600 --> 00:12:38.616
There's nothing in your life
or in our collective problems

00:12:38.640 --> 00:12:42.200
that does not require our ability
to put our attention where we care about.

00:12:43.800 --> 00:12:45.040
At the end of our lives,

00:12:46.240 --> 00:12:48.880
all we have is our attention and our time.

00:12:49.800 --> 00:12:51.696
What will be time well spent for ours?

00:12:51.720 --> 00:12:52.936
Thank you.

00:12:52.960 --> 00:12:56.080
(Applause)

00:13:05.760 --> 00:13:08.696
Chris Anderson: Tristan, thank you.
Hey, stay up here a sec.

00:13:08.720 --> 00:13:10.056
First of all, thank you.

00:13:10.080 --> 00:13:12.856
I know we asked you to do this talk
on pretty short notice,

00:13:12.880 --> 00:13:15.096
and you've had quite a stressful week

00:13:15.120 --> 00:13:17.560
getting this thing together, so thank you.

00:13:18.680 --> 00:13:22.656
Some people listening might say,
what you complain about is addiction,

00:13:22.680 --> 00:13:26.176
and all these people doing this stuff,
for them it's actually interesting.

00:13:26.200 --> 00:13:27.456
All these design decisions

00:13:27.480 --> 00:13:30.576
have built user content
that is fantastically interesting.

00:13:30.600 --> 00:13:33.016
The world's more interesting
than it ever has been.

00:13:33.040 --> 00:13:34.296
What's wrong with that?

00:13:34.320 --> 00:13:36.576
Tristan Harris:
I think it's really interesting.

00:13:36.600 --> 00:13:40.616
One way to see this
is if you're just YouTube, for example,

00:13:40.640 --> 00:13:43.296
you want to always show
the more interesting next video.

00:13:43.320 --> 00:13:46.336
You want to get better and better
at suggesting that next video,

00:13:46.360 --> 00:13:48.816
but even if you could propose
the perfect next video

00:13:48.840 --> 00:13:50.496
that everyone would want to watch,

00:13:50.520 --> 00:13:53.856
it would just be better and better
at keeping you hooked on the screen.

00:13:53.880 --> 00:13:55.536
So what's missing in that equation

00:13:55.560 --> 00:13:57.696
is figuring out what
our boundaries would be.

00:13:57.720 --> 00:14:00.936
You would want YouTube to know
something about, say, falling asleep.

00:14:00.960 --> 00:14:02.576
The CEO of Netflix recently said,

00:14:02.600 --> 00:14:05.336
"our biggest competitors
are Facebook, YouTube and sleep."

00:14:05.360 --> 00:14:09.816
And so what we need to recognize
is that the human architecture is limited

00:14:09.840 --> 00:14:12.816
and that we have certain boundaries
or dimensions of our lives

00:14:12.840 --> 00:14:14.816
that we want to be honored and respected,

00:14:14.840 --> 00:14:16.656
and technology could help do that.

00:14:16.680 --> 00:14:19.296
(Applause)

00:14:19.320 --> 00:14:21.016
CA: I mean, could you make the case

00:14:21.040 --> 00:14:27.096
that part of the problem here is that
we've got a naÃ¯ve model of human nature?

00:14:27.120 --> 00:14:29.856
So much of this is justified
in terms of human preference,

00:14:29.880 --> 00:14:32.496
where we've got these algorithms
that do an amazing job

00:14:32.520 --> 00:14:34.216
of optimizing for human preference,

00:14:34.240 --> 00:14:35.576
but which preference?

00:14:35.600 --> 00:14:39.096
There's the preferences
of things that we really care about

00:14:39.120 --> 00:14:40.496
when we think about them

00:14:40.520 --> 00:14:43.576
versus the preferences
of what we just instinctively click on.

00:14:43.600 --> 00:14:48.256
If we could implant that more nuanced
view of human nature in every design,

00:14:48.280 --> 00:14:49.736
would that be a step forward?

00:14:49.760 --> 00:14:51.736
TH: Absolutely. I mean, I think right now

00:14:51.760 --> 00:14:55.256
it's as if all of our technology
is basically only asking our lizard brain

00:14:55.280 --> 00:14:57.776
what's the best way
to just impulsively get you to do

00:14:57.800 --> 00:14:59.936
the next tiniest thing with your time,

00:14:59.960 --> 00:15:01.616
instead of asking you in your life

00:15:01.640 --> 00:15:03.816
what we would be most
time well spent for you?

00:15:03.840 --> 00:15:07.136
What would be the perfect timeline
that might include something later,

00:15:07.160 --> 00:15:10.336
would be time well spent for you
here at TED in your last day here?

00:15:10.360 --> 00:15:13.336
CA: So if Facebook and Google
and everyone said to us first up,

00:15:13.360 --> 00:15:16.256
"Hey, would you like us
to optimize for your reflective brain

00:15:16.280 --> 00:15:17.936
or your lizard brain? You choose."

00:15:17.960 --> 00:15:20.040
TH: Right. That would be one way. Yes.

00:15:22.358 --> 00:15:25.216
CA: You said persuadability,
that's an interesting word to me

00:15:25.240 --> 00:15:28.096
because to me there's
two different types of persuadability.

00:15:28.120 --> 00:15:30.656
There's the persuadability
that we're trying right now

00:15:30.680 --> 00:15:32.856
of reason and thinking
and making an argument,

00:15:32.880 --> 00:15:35.576
but I think you're almost
talking about a different kind,

00:15:35.590 --> 00:15:37.496
a more visceral type of persuadability,

00:15:37.520 --> 00:15:40.416
of being persuaded without
even knowing that you're thinking.

00:15:40.440 --> 00:15:43.296
TH: Exactly. The reason
I care about this problem so much is

00:15:43.320 --> 00:15:46.496
I studied at a lab called
the Persuasive Technology Lab at Stanford

00:15:46.520 --> 00:15:49.066
that taught [students how to recognize]
exactly these techniques.

00:15:49.106 --> 00:15:52.096
There's conferences and workshops
that teach people all these covert ways

00:15:52.120 --> 00:15:55.096
of getting people's attention
and orchestrating people's lives.

00:15:55.120 --> 00:15:57.776
And it's because most people
don't know that that exists

00:15:57.800 --> 00:15:59.696
that this conversation is so important.

00:15:59.720 --> 00:16:03.496
CA: Tristan, you and I, we both know
so many people from all these companies.

00:16:03.520 --> 00:16:05.496
There are actually many here in the room,

00:16:05.520 --> 00:16:07.997
and I don't know about you,
but my experience of them

00:16:08.021 --> 00:16:10.096
is that there is
no shortage of good intent.

00:16:10.120 --> 00:16:12.296
People want a better world.

00:16:12.320 --> 00:16:15.840
They are actually -- they really want it.

00:16:16.320 --> 00:16:20.496
And I don't think anything you're saying
is that these are evil people.

00:16:20.520 --> 00:16:24.216
It's a system where there's
these unintended consequences

00:16:24.240 --> 00:16:26.096
that have really got out of control --

00:16:26.120 --> 00:16:27.616
TH: Of this race for attention.

00:16:27.640 --> 00:16:30.816
It's the classic race to the bottom
when you have to get attention,

00:16:30.840 --> 00:16:32.056
and it's so tense.

00:16:32.080 --> 00:16:34.816
The only way to get more
is to go lower on the brain stem,

00:16:34.840 --> 00:16:37.256
to go lower into outrage,
to go lower into emotion,

00:16:37.280 --> 00:16:38.976
to go lower into the lizard brain.

00:16:39.000 --> 00:16:42.816
CA: Well, thank you so much for helping us
all get a little bit wiser about this.

00:16:42.840 --> 00:16:45.256
Tristan Harris, thank you.
TH: Thank you very much.

00:16:45.280 --> 00:16:47.520
(Applause)

