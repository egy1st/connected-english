WEBVTT

00:00:00.885 --> 00:00:02.430
In ancient Greece,

00:00:03.436 --> 00:00:07.379
when anyone from slaves to soldiers,
poets and politicians,

00:00:07.403 --> 00:00:11.407
needed to make a big decision
on life's most important questions,

00:00:11.431 --> 00:00:12.822
like, "Should I get married?"

00:00:12.846 --> 00:00:14.703
or "Should we embark on this voyage?"

00:00:14.727 --> 00:00:17.655
or "Should our army
advance into this territory?"

00:00:17.679 --> 00:00:20.258
they all consulted the oracle.

00:00:21.020 --> 00:00:22.460
So this is how it worked:

00:00:22.484 --> 00:00:25.596
you would bring her a question
and you would get on your knees,

00:00:25.620 --> 00:00:27.491
and then she would go into this trance.

00:00:27.515 --> 00:00:29.064
It would take a couple of days,

00:00:29.088 --> 00:00:31.251
and then eventually
she would come out of it,

00:00:31.275 --> 00:00:33.811
giving you her predictions as your answer.

00:00:34.910 --> 00:00:37.476
From the oracle bones of ancient China

00:00:37.500 --> 00:00:39.845
to ancient Greece to Mayan calendars,

00:00:39.869 --> 00:00:42.165
people have craved for prophecy

00:00:42.189 --> 00:00:45.326
in order to find out
what's going to happen next.

00:00:46.516 --> 00:00:49.755
And that's because we all want
to make the right decision.

00:00:49.779 --> 00:00:51.324
We don't want to miss something.

00:00:51.892 --> 00:00:53.635
The future is scary,

00:00:53.659 --> 00:00:56.376
so it's much nicer
knowing that we can make a decision

00:00:56.400 --> 00:00:58.382
with some assurance of the outcome.

00:00:59.079 --> 00:01:00.690
Well, we have a new oracle,

00:01:00.714 --> 00:01:02.859
and it's name is big data,

00:01:02.883 --> 00:01:06.822
or we call it "Watson"
or "deep learning" or "neural net."

00:01:07.340 --> 00:01:11.352
And these are the kinds of questions
we ask of our oracle now,

00:01:11.376 --> 00:01:15.298
like, "What's the most efficient way
to ship these phones

00:01:15.322 --> 00:01:17.145
from China to Sweden?"

00:01:17.169 --> 00:01:18.969
Or, "What are the odds

00:01:18.993 --> 00:01:22.356
of my child being born
with a genetic disorder?"

00:01:22.952 --> 00:01:26.196
Or, "What are the sales volume
we can predict for this product?"

00:01:28.108 --> 00:01:32.155
I have a dog. Her name is Elle,
and she hates the rain.

00:01:32.179 --> 00:01:35.485
And I have tried everything
to untrain her.

00:01:35.509 --> 00:01:38.280
But because I have failed at this,

00:01:38.304 --> 00:01:41.590
I also have to consult
an oracle, called Dark Sky,

00:01:41.614 --> 00:01:43.249
every time before we go on a walk,

00:01:43.273 --> 00:01:46.850
for very accurate weather predictions
in the next 10 minutes.

00:01:49.535 --> 00:01:50.838
She's so sweet.

00:01:51.827 --> 00:01:57.534
So because of all of this,
our oracle is a $122 billion industry.

00:01:58.006 --> 00:02:01.382
Now, despite the size of this industry,

00:02:01.406 --> 00:02:03.862
the returns are surprisingly low.

00:02:04.342 --> 00:02:06.836
Investing in big data is easy,

00:02:06.860 --> 00:02:08.793
but using it is hard.

00:02:09.981 --> 00:02:14.021
Over 73 percent of big data projects
aren't even profitable,

00:02:14.045 --> 00:02:16.476
and I have executives
coming up to me saying,

00:02:16.500 --> 00:02:18.289
"We're experiencing the same thing.

00:02:18.313 --> 00:02:20.066
We invested in some big data system,

00:02:20.090 --> 00:02:23.058
and our employees aren't making
better decisions.

00:02:23.082 --> 00:02:26.244
And they're certainly not coming up
with more breakthrough ideas."

00:02:26.914 --> 00:02:30.098
So this is all really interesting to me,

00:02:30.122 --> 00:02:32.132
because I'm a technology ethnographer.

00:02:32.630 --> 00:02:35.194
I study and I advise companies

00:02:35.218 --> 00:02:37.701
on the patterns
of how people use technology,

00:02:37.725 --> 00:02:40.403
and one of my interest areas is data.

00:02:40.427 --> 00:02:45.620
So why is having more data
not helping us make better decisions,

00:02:45.644 --> 00:02:48.427
especially for companies
who have all these resources

00:02:48.451 --> 00:02:50.187
to invest in these big data systems?

00:02:50.211 --> 00:02:52.609
Why isn't it getting any easier for them?

00:02:53.990 --> 00:02:56.624
So, I've witnessed the struggle firsthand.

00:02:57.374 --> 00:03:00.858
In 2009, I started
a research position with Nokia.

00:03:01.232 --> 00:03:02.390
And at the time,

00:03:02.414 --> 00:03:05.572
Nokia was one of the largest
cell phone companies in the world,

00:03:05.596 --> 00:03:08.798
dominating emerging markets
like China, Mexico and India --

00:03:08.822 --> 00:03:11.324
all places where I had done
a lot of research

00:03:11.348 --> 00:03:14.024
on how low-income people use technology.

00:03:14.048 --> 00:03:16.378
And I spent a lot of extra time in China

00:03:16.402 --> 00:03:18.994
getting to know the informal economy.

00:03:19.018 --> 00:03:21.419
So I did things like working
as a street vendor

00:03:21.443 --> 00:03:24.017
selling dumplings to construction workers.

00:03:24.041 --> 00:03:25.399
Or I did fieldwork,

00:03:25.423 --> 00:03:28.381
spending nights and days
in internet cafÃ©s,

00:03:28.405 --> 00:03:30.951
hanging out with Chinese youth,
so I could understand

00:03:30.975 --> 00:03:33.259
how they were using
games and mobile phones

00:03:33.283 --> 00:03:36.653
and using it between moving
from the rural areas to the cities.

00:03:38.335 --> 00:03:42.262
Through all of this qualitative evidence
that I was gathering,

00:03:42.286 --> 00:03:45.110
I was starting to see so clearly

00:03:45.134 --> 00:03:49.606
that a big change was about to happen
among low-income Chinese people.

00:03:51.020 --> 00:03:55.387
Even though they were surrounded
by advertisements for luxury products

00:03:55.411 --> 00:03:58.906
like fancy toilets --
who wouldn't want one? --

00:03:58.930 --> 00:04:01.820
and apartments and cars,

00:04:01.844 --> 00:04:03.664
through my conversations with them,

00:04:03.688 --> 00:04:07.529
I found out that the ads
the actually enticed them the most

00:04:07.553 --> 00:04:09.549
were the ones for iPhones,

00:04:09.573 --> 00:04:12.625
promising them this entry
into this high-tech life.

00:04:13.469 --> 00:04:16.632
And even when I was living with them
in urban slums like this one,

00:04:16.656 --> 00:04:19.652
I saw people investing
over half of their monthly income

00:04:19.676 --> 00:04:21.299
into buying a phone,

00:04:21.323 --> 00:04:23.625
and increasingly, they were "shanzhai,"

00:04:23.649 --> 00:04:27.037
which are affordable knock-offs
of iPhones and other brands.

00:04:28.303 --> 00:04:29.928
They're very usable.

00:04:30.890 --> 00:04:32.212
Does the job.

00:04:32.750 --> 00:04:38.539
And after years of living
with migrants and working with them

00:04:38.563 --> 00:04:41.997
and just really doing everything
that they were doing,

00:04:42.021 --> 00:04:45.618
I started piecing
all these data points together --

00:04:45.642 --> 00:04:48.765
from the things that seem random,
like me selling dumplings,

00:04:48.789 --> 00:04:50.593
to the things that were more obvious,

00:04:50.617 --> 00:04:53.849
like tracking how much they were spending
on their cell phone bills.

00:04:53.873 --> 00:04:56.512
And I was able to create
this much more holistic picture

00:04:56.536 --> 00:04:57.692
of what was happening.

00:04:57.716 --> 00:04:59.438
And that's when I started to realize

00:04:59.462 --> 00:05:02.971
that even the poorest in China
would want a smartphone,

00:05:02.995 --> 00:05:07.980
and that they would do almost anything
to get their hands on one.

00:05:09.073 --> 00:05:11.477
You have to keep in mind,

00:05:11.501 --> 00:05:14.585
iPhones had just come out, it was 2009,

00:05:14.609 --> 00:05:16.494
so this was, like, eight years ago,

00:05:16.518 --> 00:05:18.955
and Androids had just started
looking like iPhones.

00:05:18.979 --> 00:05:21.486
And a lot of very smart
and realistic people said,

00:05:21.510 --> 00:05:23.717
"Those smartphones -- that's just a fad.

00:05:24.243 --> 00:05:27.239
Who wants to carry around
these heavy things

00:05:27.263 --> 00:05:30.750
where batteries drain quickly
and they break every time you drop them?"

00:05:32.793 --> 00:05:33.994
But I had a lot of data,

00:05:34.018 --> 00:05:36.278
and I was very confident
about my insights,

00:05:36.302 --> 00:05:39.131
so I was very excited
to share them with Nokia.

00:05:41.332 --> 00:05:43.849
But Nokia was not convinced,

00:05:43.873 --> 00:05:46.208
because it wasn't big data.

00:05:47.022 --> 00:05:49.426
They said, "We have
millions of data points,

00:05:49.450 --> 00:05:53.697
and we don't see any indicators
of anyone wanting to buy a smartphone,

00:05:53.721 --> 00:05:58.109
and your data set of 100,
as diverse as it is, is too weak

00:05:58.133 --> 00:05:59.847
for us to even take seriously."

00:06:00.908 --> 00:06:02.513
And I said, "Nokia, you're right.

00:06:02.537 --> 00:06:04.097
Of course you wouldn't see this,

00:06:04.121 --> 00:06:07.492
because you're sending out surveys
assuming that people don't know

00:06:07.516 --> 00:06:08.675
what a smartphone is,

00:06:08.699 --> 00:06:11.065
so of course you're not going
to get any data back

00:06:11.089 --> 00:06:13.661
about people wanting to buy
a smartphone in two years.

00:06:13.685 --> 00:06:15.803
Your surveys, your methods
have been designed

00:06:15.827 --> 00:06:17.849
to optimize an existing business model,

00:06:17.873 --> 00:06:20.481
and I'm looking
at these emergent human dynamics

00:06:20.505 --> 00:06:21.859
that haven't happened yet.

00:06:21.883 --> 00:06:24.321
We're looking outside of market dynamics

00:06:24.345 --> 00:06:25.976
so that we can get ahead of it."

00:06:27.373 --> 00:06:29.617
Well, you know what happened to Nokia?

00:06:29.641 --> 00:06:32.006
Their business fell off a cliff.

00:06:32.791 --> 00:06:36.518
This -- this is the cost
of missing something.

00:06:37.163 --> 00:06:39.162
It was unfathomable.

00:06:40.003 --> 00:06:41.654
But Nokia's not alone.

00:06:42.258 --> 00:06:44.839
I see organizations
throwing out data all the time

00:06:44.863 --> 00:06:47.424
because it didn't come from a quant model

00:06:47.448 --> 00:06:49.216
or it doesn't fit in one.

00:06:50.219 --> 00:06:52.267
But it's not big data's fault.

00:06:52.942 --> 00:06:56.849
It's the way we use big data;
it's our responsibility.

00:06:57.730 --> 00:06:59.641
Big data's reputation for success

00:06:59.665 --> 00:07:03.424
comes from quantifying
very specific environments,

00:07:03.448 --> 00:07:08.361
like electricity power grids
or delivery logistics or genetic code,

00:07:08.385 --> 00:07:12.703
when we're quantifying in systems
that are more or less contained.

00:07:12.727 --> 00:07:15.696
But not all systems
are as neatly contained.

00:07:15.720 --> 00:07:18.978
When you're quantifying
and systems are more dynamic,

00:07:19.002 --> 00:07:22.801
especially systems
that involve human beings,

00:07:22.825 --> 00:07:25.251
forces are complex and unpredictable,

00:07:25.275 --> 00:07:28.761
and these are things
that we don't know how to model so well.

00:07:29.204 --> 00:07:32.017
Once you predict something
about human behavior,

00:07:32.041 --> 00:07:33.896
new factors emerge,

00:07:33.920 --> 00:07:36.285
because conditions
are constantly changing.

00:07:36.309 --> 00:07:38.112
That's why it's a never-ending cycle.

00:07:38.136 --> 00:07:39.600
You think you know something,

00:07:39.624 --> 00:07:41.866
and then something unknown
enters the picture.

00:07:41.890 --> 00:07:45.212
And that's why just relying
on big data alone

00:07:45.236 --> 00:07:48.085
increases the chance
that we'll miss something,

00:07:48.109 --> 00:07:51.886
while giving us this illusion
that we already know everything.

00:07:52.406 --> 00:07:56.262
And what makes it really hard
to see this paradox

00:07:56.286 --> 00:07:58.945
and even wrap our brains around it

00:07:58.969 --> 00:08:02.660
is that we have this thing
that I call the quantification bias,

00:08:02.684 --> 00:08:06.606
which is the unconscious belief
of valuing the measurable

00:08:06.630 --> 00:08:08.224
over the immeasurable.

00:08:09.222 --> 00:08:12.506
And we often experience this at our work.

00:08:12.530 --> 00:08:15.180
Maybe we work alongside
colleagues who are like this,

00:08:15.204 --> 00:08:17.632
or even our whole entire
company may be like this,

00:08:17.656 --> 00:08:20.202
where people become
so fixated on that number,

00:08:20.226 --> 00:08:22.293
that they can't see anything
outside of it,

00:08:22.317 --> 00:08:26.265
even when you present them evidence
right in front of their face.

00:08:27.123 --> 00:08:30.494
And this is a very appealing message,

00:08:30.518 --> 00:08:32.861
because there's nothing
wrong with quantifying;

00:08:32.885 --> 00:08:34.315
it's actually very satisfying.

00:08:34.339 --> 00:08:38.701
I get a great sense of comfort
from looking at an Excel spreadsheet,

00:08:38.725 --> 00:08:40.126
even very simple ones.

00:08:40.150 --> 00:08:41.164
(Laughter)

00:08:41.188 --> 00:08:42.340
It's just kind of like,

00:08:42.364 --> 00:08:45.868
"Yes! The formula worked. It's all OK.
Everything is under control."

00:08:46.792 --> 00:08:49.182
But the problem is

00:08:49.206 --> 00:08:51.867
that quantifying is addictive.

00:08:51.891 --> 00:08:53.273
And when we forget that

00:08:53.297 --> 00:08:56.335
and when we don't have something
to kind of keep that in check,

00:08:56.359 --> 00:08:58.477
it's very easy to just throw out data

00:08:58.501 --> 00:09:01.219
because it can't be expressed
as a numerical value.

00:09:01.243 --> 00:09:04.164
It's very easy just to slip
into silver-bullet thinking,

00:09:04.188 --> 00:09:06.767
as if some simple solution existed.

00:09:07.600 --> 00:09:11.662
Because this is a great moment of danger
for any organization,

00:09:11.686 --> 00:09:14.320
because oftentimes,
the future we need to predict --

00:09:14.344 --> 00:09:16.510
it isn't in that haystack,

00:09:16.534 --> 00:09:19.072
but it's that tornado
that's bearing down on us

00:09:19.096 --> 00:09:20.584
outside of the barn.

00:09:22.960 --> 00:09:25.286
There is no greater risk

00:09:25.310 --> 00:09:26.976
than being blind to the unknown.

00:09:27.000 --> 00:09:29.149
It can cause you to make
the wrong decisions.

00:09:29.173 --> 00:09:31.147
It can cause you to miss something big.

00:09:31.734 --> 00:09:34.835
But we don't have to go down this path.

00:09:35.453 --> 00:09:38.648
It turns out that the oracle
of ancient Greece

00:09:38.672 --> 00:09:42.638
holds the secret key
that shows us the path forward.

00:09:43.654 --> 00:09:46.249
Now, recent geological research has shown

00:09:46.273 --> 00:09:49.837
that the Temple of Apollo,
where the most famous oracle sat,

00:09:49.861 --> 00:09:52.945
was actually built
over two earthquake faults.

00:09:52.969 --> 00:09:55.855
And these faults would release
these petrochemical fumes

00:09:55.879 --> 00:09:57.564
from underneath the Earth's crust,

00:09:57.588 --> 00:10:01.454
and the oracle literally sat
right above these faults,

00:10:01.478 --> 00:10:05.066
inhaling enormous amounts
of ethylene gas, these fissures.

00:10:05.090 --> 00:10:06.098
(Laughter)

00:10:06.122 --> 00:10:07.295
It's true.

00:10:07.319 --> 00:10:08.336
(Laughter)

00:10:08.360 --> 00:10:11.869
It's all true, and that's what made her
babble and hallucinate

00:10:11.893 --> 00:10:13.617
and go into this trance-like state.

00:10:13.641 --> 00:10:15.411
She was high as a kite!

00:10:15.435 --> 00:10:19.896
(Laughter)

00:10:19.920 --> 00:10:22.699
So how did anyone --

00:10:22.723 --> 00:10:25.753
How did anyone get
any useful advice out of her

00:10:25.777 --> 00:10:26.967
in this state?

00:10:27.497 --> 00:10:29.878
Well, you see those people
surrounding the oracle?

00:10:29.902 --> 00:10:31.781
You see those people holding her up,

00:10:31.805 --> 00:10:33.522
because she's, like, a little woozy?

00:10:33.546 --> 00:10:35.854
And you see that guy
on your left-hand side

00:10:35.878 --> 00:10:37.476
holding the orange notebook?

00:10:38.105 --> 00:10:39.835
Well, those were the temple guides,

00:10:39.859 --> 00:10:42.875
and they worked hand in hand
with the oracle.

00:10:44.084 --> 00:10:46.600
When inquisitors would come
and get on their knees,

00:10:46.624 --> 00:10:48.964
that's when the temple guides
would get to work,

00:10:48.988 --> 00:10:50.852
because after they asked her questions,

00:10:50.876 --> 00:10:52.877
they would observe their emotional state,

00:10:52.901 --> 00:10:55.225
and then they would ask them
follow-up questions,

00:10:55.249 --> 00:10:58.083
like, "Why do you want to know
this prophecy? Who are you?

00:10:58.107 --> 00:11:00.371
What are you going to do
with this information?"

00:11:00.395 --> 00:11:03.577
And then the temple guides would take
this more ethnographic,

00:11:03.601 --> 00:11:05.757
this more qualitative information,

00:11:05.781 --> 00:11:07.856
and interpret the oracle's babblings.

00:11:09.428 --> 00:11:11.720
So the oracle didn't stand alone,

00:11:11.744 --> 00:11:13.892
and neither should our big data systems.

00:11:14.630 --> 00:11:15.791
Now to be clear,

00:11:15.815 --> 00:11:19.274
I'm not saying that big data systems
are huffing ethylene gas,

00:11:19.298 --> 00:11:21.651
or that they're even giving
invalid predictions.

00:11:21.675 --> 00:11:22.836
The total opposite.

00:11:22.860 --> 00:11:24.928
But what I am saying

00:11:24.952 --> 00:11:28.784
is that in the same way
that the oracle needed her temple guides,

00:11:28.808 --> 00:11:31.096
our big data systems need them, too.

00:11:31.120 --> 00:11:35.229
They need people like ethnographers
and user researchers

00:11:35.253 --> 00:11:37.759
who can gather what I call thick data.

00:11:38.502 --> 00:11:41.493
This is precious data from humans,

00:11:41.517 --> 00:11:45.619
like stories, emotions and interactions
that cannot be quantified.

00:11:45.643 --> 00:11:47.965
It's the kind of data
that I collected for Nokia

00:11:47.989 --> 00:11:50.658
that comes in in the form
of a very small sample size,

00:11:50.682 --> 00:11:53.637
but delivers incredible depth of meaning.

00:11:53.661 --> 00:11:57.341
And what makes it so thick and meaty

00:11:58.445 --> 00:12:02.474
is the experience of understanding
the human narrative.

00:12:02.498 --> 00:12:06.137
And that's what helps to see
what's missing in our models.

00:12:06.851 --> 00:12:10.896
Thick data grounds our business questions
in human questions,

00:12:10.920 --> 00:12:14.482
and that's why integrating
big and thick data

00:12:14.506 --> 00:12:16.195
forms a more complete picture.

00:12:16.772 --> 00:12:19.653
Big data is able to offer
insights at scale

00:12:19.677 --> 00:12:22.324
and leverage the best
of machine intelligence,

00:12:22.348 --> 00:12:25.920
whereas thick data can help us
rescue the context loss

00:12:25.944 --> 00:12:28.042
that comes from making big data usable,

00:12:28.066 --> 00:12:30.247
and leverage the best
of human intelligence.

00:12:30.271 --> 00:12:33.823
And when you actually integrate the two,
that's when things get really fun,

00:12:33.847 --> 00:12:36.283
because then you're no longer
just working with data

00:12:36.307 --> 00:12:37.503
you've already collected.

00:12:37.527 --> 00:12:40.264
You get to also work with data
that hasn't been collected.

00:12:40.288 --> 00:12:42.007
You get to ask questions about why:

00:12:42.031 --> 00:12:43.348
Why is this happening?

00:12:43.778 --> 00:12:45.157
Now, when Netflix did this,

00:12:45.181 --> 00:12:48.216
they unlocked a whole new way
to transform their business.

00:12:49.406 --> 00:12:53.362
Netflix is known for their really great
recommendation algorithm,

00:12:53.386 --> 00:12:58.183
and they had this $1 million prize
for anyone who could improve it.

00:12:58.207 --> 00:12:59.521
And there were winners.

00:13:00.255 --> 00:13:04.578
But Netflix discovered
the improvements were only incremental.

00:13:05.404 --> 00:13:07.368
So to really find out what was going on,

00:13:07.392 --> 00:13:11.133
they hired an ethnographer,
Grant McCracken,

00:13:11.157 --> 00:13:12.703
to gather thick data insights.

00:13:12.727 --> 00:13:16.651
And what he discovered was something
that they hadn't seen initially

00:13:16.675 --> 00:13:18.030
in the quantitative data.

00:13:19.072 --> 00:13:21.800
He discovered that people loved
to binge-watch.

00:13:21.824 --> 00:13:24.177
In fact, people didn't even
feel guilty about it.

00:13:24.201 --> 00:13:25.456
They enjoyed it.

00:13:25.480 --> 00:13:26.506
(Laughter)

00:13:26.530 --> 00:13:28.886
So Netflix was like,
"Oh. This is a new insight."

00:13:28.910 --> 00:13:30.848
So they went to their data science team,

00:13:30.872 --> 00:13:33.190
and they were able to scale
this big data insight

00:13:33.214 --> 00:13:35.801
in with their quantitative data.

00:13:35.825 --> 00:13:38.995
And once they verified it
and validated it,

00:13:39.019 --> 00:13:43.780
Netflix decided to do something
very simple but impactful.

00:13:44.834 --> 00:13:51.326
They said, instead of offering
the same show from different genres

00:13:51.350 --> 00:13:55.238
or more of the different shows
from similar users,

00:13:55.262 --> 00:13:57.816
we'll just offer more of the same show.

00:13:57.840 --> 00:13:59.945
We'll make it easier
for you to binge-watch.

00:13:59.969 --> 00:14:01.455
And they didn't stop there.

00:14:01.479 --> 00:14:02.953
They did all these things

00:14:02.977 --> 00:14:05.936
to redesign their entire
viewer experience,

00:14:05.960 --> 00:14:07.718
to really encourage binge-watching.

00:14:08.230 --> 00:14:11.471
It's why people and friends disappear
for whole weekends at a time,

00:14:11.495 --> 00:14:13.838
catching up on shows
like "Master of None."

00:14:13.862 --> 00:14:18.035
By integrating big data and thick data,
they not only improved their business,

00:14:18.059 --> 00:14:20.871
but they transformed how we consume media.

00:14:20.895 --> 00:14:25.447
And now their stocks are projected
to double in the next few years.

00:14:26.280 --> 00:14:30.110
But this isn't just about
watching more videos

00:14:30.134 --> 00:14:31.754
or selling more smartphones.

00:14:32.143 --> 00:14:36.193
For some, integrating thick data
insights into the algorithm

00:14:36.217 --> 00:14:38.480
could mean life or death,

00:14:38.504 --> 00:14:40.650
especially for the marginalized.

00:14:41.738 --> 00:14:45.172
All around the country,
police departments are using big data

00:14:45.196 --> 00:14:47.159
for predictive policing,

00:14:47.183 --> 00:14:50.267
to set bond amounts
and sentencing recommendations

00:14:50.291 --> 00:14:53.438
in ways that reinforce existing biases.

00:14:54.296 --> 00:14:56.719
NSA's Skynet machine learning algorithm

00:14:56.743 --> 00:15:02.187
has possibly aided in the deaths
of thousands of civilians in Pakistan

00:15:02.211 --> 00:15:04.932
from misreading cellular device metadata.

00:15:07.131 --> 00:15:10.534
As all of our lives become more automated,

00:15:10.558 --> 00:15:13.638
from automobiles to health insurance
or to employment,

00:15:13.662 --> 00:15:16.012
it is likely that all of us

00:15:16.036 --> 00:15:19.025
will be impacted
by the quantification bias.

00:15:20.972 --> 00:15:23.593
Now, the good news
is that we've come a long way

00:15:23.617 --> 00:15:26.067
from huffing ethylene gas
to make predictions.

00:15:26.091 --> 00:15:29.161
We have better tools,
so let's just use them better.

00:15:29.185 --> 00:15:31.508
Let's integrate the big data
with the thick data.

00:15:31.532 --> 00:15:33.793
Let's bring our temple guides
with the oracles,

00:15:33.817 --> 00:15:37.193
and whether this work happens
in companies or nonprofits

00:15:37.217 --> 00:15:39.686
or government or even in the software,

00:15:39.710 --> 00:15:41.502
all of it matters,

00:15:41.526 --> 00:15:44.549
because that means
we're collectively committed

00:15:44.573 --> 00:15:46.764
to making better data,

00:15:46.788 --> 00:15:48.624
better algorithms, better outputs

00:15:48.648 --> 00:15:50.291
and better decisions.

00:15:50.315 --> 00:15:53.873
This is how we'll avoid
missing that something.

00:15:55.222 --> 00:15:59.170
(Applause)

