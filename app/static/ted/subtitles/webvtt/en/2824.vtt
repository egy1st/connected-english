WEBVTT

00:00:01.000 --> 00:00:05.080
Today I'm going to talk
about technology and society.

00:00:07.040 --> 00:00:10.736
The Department of Transport
estimated that last year

00:00:10.760 --> 00:00:14.840
35,000 people died
from traffic crashes in the US alone.

00:00:16.040 --> 00:00:20.840
Worldwide, 1.2 million people
die every year in traffic accidents.

00:00:21.760 --> 00:00:25.856
If there was a way we could eliminate
90 percent of those accidents,

00:00:25.880 --> 00:00:27.080
would you support it?

00:00:27.720 --> 00:00:29.016
Of course you would.

00:00:29.040 --> 00:00:32.695
This is what driverless car technology
promises to achieve

00:00:32.720 --> 00:00:35.536
by eliminating the main
source of accidents --

00:00:35.560 --> 00:00:36.760
human error.

00:00:37.920 --> 00:00:43.336
Now picture yourself
in a driverless car in the year 2030,

00:00:43.360 --> 00:00:46.816
sitting back and watching
this vintage TEDxCambridge video.

00:00:46.840 --> 00:00:48.840
(Laughter)

00:00:49.520 --> 00:00:50.736
All of a sudden,

00:00:50.760 --> 00:00:54.040
the car experiences mechanical failure
and is unable to stop.

00:00:55.360 --> 00:00:56.880
If the car continues,

00:00:57.720 --> 00:01:01.840
it will crash into a bunch
of pedestrians crossing the street,

00:01:03.080 --> 00:01:05.215
but the car may swerve,

00:01:05.239 --> 00:01:07.096
hitting one bystander,

00:01:07.120 --> 00:01:09.200
killing them to save the pedestrians.

00:01:10.040 --> 00:01:12.640
What should the car do,
and who should decide?

00:01:13.520 --> 00:01:17.056
What if instead the car
could swerve into a wall,

00:01:17.080 --> 00:01:20.376
crashing and killing you, the passenger,

00:01:20.400 --> 00:01:22.720
in order to save those pedestrians?

00:01:23.240 --> 00:01:26.320
This scenario is inspired
by the trolley problem,

00:01:26.960 --> 00:01:30.736
which was invented
by philosophers a few decades ago

00:01:30.760 --> 00:01:32.000
to think about ethics.

00:01:34.120 --> 00:01:36.616
Now, the way we think
about this problem matters.

00:01:36.640 --> 00:01:39.256
We may for example
not think about it at all.

00:01:39.280 --> 00:01:42.656
We may say this scenario is unrealistic,

00:01:42.680 --> 00:01:45.000
incredibly unlikely, or just silly.

00:01:45.760 --> 00:01:48.496
But I think this criticism
misses the point

00:01:48.520 --> 00:01:50.680
because it takes
the scenario too literally.

00:01:51.920 --> 00:01:54.656
Of course no accident
is going to look like this;

00:01:54.680 --> 00:01:58.016
no accident has two or three options

00:01:58.040 --> 00:02:00.040
where everybody dies somehow.

00:02:01.480 --> 00:02:04.056
Instead, the car is going
to calculate something

00:02:04.080 --> 00:02:08.976
like the probability of hitting
a certain group of people,

00:02:09.000 --> 00:02:12.336
if you swerve one direction
versus another direction,

00:02:12.360 --> 00:02:15.816
you might slightly increase the risk
to passengers or other drivers

00:02:15.840 --> 00:02:17.376
versus pedestrians.

00:02:17.400 --> 00:02:19.560
It's going to be
a more complex calculation,

00:02:20.480 --> 00:02:23.000
but it's still going
to involve trade-offs,

00:02:23.840 --> 00:02:26.720
and trade-offs often require ethics.

00:02:27.840 --> 00:02:30.576
We might say then,
"Well, let's not worry about this.

00:02:30.600 --> 00:02:35.240
Let's wait until technology
is fully ready and 100 percent safe."

00:02:36.520 --> 00:02:40.200
Suppose that we can indeed
eliminate 90 percent of those accidents,

00:02:41.080 --> 00:02:43.920
or even 99 percent in the next 10 years.

00:02:44.920 --> 00:02:48.096
What if eliminating
the last one percent of accidents

00:02:48.120 --> 00:02:51.240
requires 50 more years of research?

00:02:52.400 --> 00:02:54.200
Should we not adopt the technology?

00:02:54.720 --> 00:02:59.496
That's 60 million people
dead in car accidents

00:02:59.520 --> 00:03:01.280
if we maintain the current rate.

00:03:02.760 --> 00:03:03.976
So the point is,

00:03:04.000 --> 00:03:07.616
waiting for full safety is also a choice,

00:03:07.640 --> 00:03:09.800
and it also involves trade-offs.

00:03:11.560 --> 00:03:15.896
People online on social media
have been coming up with all sorts of ways

00:03:15.920 --> 00:03:17.936
to not think about this problem.

00:03:17.960 --> 00:03:21.176
One person suggested
the car should just swerve somehow

00:03:21.200 --> 00:03:23.336
in between the passengers --

00:03:23.360 --> 00:03:24.376
(Laughter)

00:03:24.400 --> 00:03:25.656
and the bystander.

00:03:25.680 --> 00:03:29.040
Of course if that's what the car can do,
that's what the car should do.

00:03:29.920 --> 00:03:32.760
We're interested in scenarios
in which this is not possible.

00:03:33.280 --> 00:03:38.696
And my personal favorite
was a suggestion by a blogger

00:03:38.720 --> 00:03:41.736
to have an eject button in the car
that you press --

00:03:41.760 --> 00:03:42.976
(Laughter)

00:03:43.000 --> 00:03:44.667
just before the car self-destructs.

00:03:44.691 --> 00:03:46.371
(Laughter)

00:03:47.840 --> 00:03:53.040
So if we acknowledge that cars
will have to make trade-offs on the road,

00:03:54.200 --> 00:03:56.080
how do we think about those trade-offs,

00:03:57.320 --> 00:03:58.896
and how do we decide?

00:03:58.920 --> 00:04:02.056
Well, maybe we should run a survey
to find out what society wants,

00:04:02.080 --> 00:04:03.536
because ultimately,

00:04:03.560 --> 00:04:07.520
regulations and the law
are a reflection of societal values.

00:04:08.040 --> 00:04:09.280
So this is what we did.

00:04:09.880 --> 00:04:11.496
With my collaborators,

00:04:11.520 --> 00:04:13.856
Jean-FranÃ§ois Bonnefon and Azim Shariff,

00:04:13.880 --> 00:04:15.496
we ran a survey

00:04:15.520 --> 00:04:18.375
in which we presented people
with these types of scenarios.

00:04:18.399 --> 00:04:22.176
We gave them two options
inspired by two philosophers:

00:04:22.200 --> 00:04:24.840
Jeremy Bentham and Immanuel Kant.

00:04:25.600 --> 00:04:28.696
Bentham says the car
should follow utilitarian ethics:

00:04:28.720 --> 00:04:32.136
it should take the action
that will minimize total harm --

00:04:32.160 --> 00:04:34.976
even if that action will kill a bystander

00:04:35.000 --> 00:04:37.440
and even if that action
will kill the passenger.

00:04:38.120 --> 00:04:43.096
Immanuel Kant says the car
should follow duty-bound principles,

00:04:43.120 --> 00:04:44.680
like "Thou shalt not kill."

00:04:45.480 --> 00:04:49.936
So you should not take an action
that explicitly harms a human being,

00:04:49.960 --> 00:04:52.416
and you should let the car take its course

00:04:52.440 --> 00:04:54.400
even if that's going to harm more people.

00:04:55.640 --> 00:04:56.840
What do you think?

00:04:57.360 --> 00:04:58.880
Bentham or Kant?

00:04:59.760 --> 00:05:01.016
Here's what we found.

00:05:01.040 --> 00:05:02.840
Most people sided with Bentham.

00:05:04.160 --> 00:05:07.936
So it seems that people
want cars to be utilitarian,

00:05:07.960 --> 00:05:09.376
minimize total harm,

00:05:09.400 --> 00:05:10.976
and that's what we should all do.

00:05:11.000 --> 00:05:12.200
Problem solved.

00:05:13.240 --> 00:05:14.720
But there is a little catch.

00:05:15.920 --> 00:05:19.656
When we asked people
whether they would purchase such cars,

00:05:19.680 --> 00:05:21.296
they said, "Absolutely not."

00:05:21.320 --> 00:05:23.616
(Laughter)

00:05:23.640 --> 00:05:27.536
They would like to buy cars
that protect them at all costs,

00:05:27.560 --> 00:05:31.176
but they want everybody else
to buy cars that minimize harm.

00:05:31.200 --> 00:05:33.720
(Laughter)

00:05:34.720 --> 00:05:36.576
We've seen this problem before.

00:05:36.600 --> 00:05:38.160
It's called a social dilemma.

00:05:39.160 --> 00:05:40.976
And to understand the social dilemma,

00:05:41.000 --> 00:05:43.040
we have to go a little bit
back in history.

00:05:44.000 --> 00:05:46.576
In the 1800s,

00:05:46.600 --> 00:05:50.336
English economist William Forster Lloyd
published a pamphlet

00:05:50.360 --> 00:05:52.576
which describes the following scenario.

00:05:52.600 --> 00:05:54.256
You have a group of farmers --

00:05:54.280 --> 00:05:55.616
English farmers --

00:05:55.640 --> 00:05:58.320
who are sharing a common land
for their sheep to graze.

00:05:59.520 --> 00:06:02.096
Now, if each farmer
brings a certain number of sheep --

00:06:02.120 --> 00:06:03.616
let's say three sheep --

00:06:03.640 --> 00:06:05.736
the land will be rejuvenated,

00:06:05.760 --> 00:06:06.976
the farmers are happy,

00:06:07.000 --> 00:06:08.616
the sheep are happy,

00:06:08.640 --> 00:06:09.840
everything is good.

00:06:10.440 --> 00:06:12.960
Now, if one farmer brings one extra sheep,

00:06:13.800 --> 00:06:18.520
that farmer will do slightly better,
and no one else will be harmed.

00:06:19.160 --> 00:06:22.800
But if every farmer made
that individually rational decision,

00:06:23.840 --> 00:06:26.560
the land will be overrun,
and it will be depleted

00:06:27.360 --> 00:06:29.536
to the detriment of all the farmers,

00:06:29.560 --> 00:06:31.680
and of course,
to the detriment of the sheep.

00:06:32.720 --> 00:06:36.400
We see this problem in many places:

00:06:37.080 --> 00:06:40.256
in the difficulty of managing overfishing,

00:06:40.280 --> 00:06:44.840
or in reducing carbon emissions
to mitigate climate change.

00:06:47.160 --> 00:06:50.080
When it comes to the regulation
of driverless cars,

00:06:51.080 --> 00:06:55.416
the common land now
is basically public safety --

00:06:55.440 --> 00:06:56.680
that's the common good --

00:06:57.400 --> 00:06:59.376
and the farmers are the passengers

00:06:59.400 --> 00:07:03.000
or the car owners who are choosing
to ride in those cars.

00:07:04.960 --> 00:07:07.576
And by making the individually
rational choice

00:07:07.600 --> 00:07:10.416
of prioritizing their own safety,

00:07:10.440 --> 00:07:13.576
they may collectively be
diminishing the common good,

00:07:13.600 --> 00:07:15.800
which is minimizing total harm.

00:07:18.320 --> 00:07:20.456
It's called the tragedy of the commons,

00:07:20.480 --> 00:07:21.776
traditionally,

00:07:21.800 --> 00:07:24.896
but I think in the case
of driverless cars,

00:07:24.920 --> 00:07:27.776
the problem may be
a little bit more insidious

00:07:27.800 --> 00:07:31.296
because there is not necessarily
an individual human being

00:07:31.320 --> 00:07:33.016
making those decisions.

00:07:33.040 --> 00:07:36.336
So car manufacturers
may simply program cars

00:07:36.360 --> 00:07:38.880
that will maximize safety
for their clients,

00:07:40.080 --> 00:07:43.056
and those cars may learn
automatically on their own

00:07:43.080 --> 00:07:46.600
that doing so requires slightly
increasing risk for pedestrians.

00:07:47.520 --> 00:07:48.936
So to use the sheep metaphor,

00:07:48.960 --> 00:07:52.576
it's like we now have electric sheep
that have a mind of their own.

00:07:52.600 --> 00:07:54.056
(Laughter)

00:07:54.080 --> 00:07:57.160
And they may go and graze
even if the farmer doesn't know it.

00:07:58.640 --> 00:08:02.616
So this is what we may call
the tragedy of the algorithmic commons,

00:08:02.640 --> 00:08:05.000
and if offers new types of challenges.

00:08:10.520 --> 00:08:12.416
Typically, traditionally,

00:08:12.440 --> 00:08:15.776
we solve these types
of social dilemmas using regulation,

00:08:15.800 --> 00:08:18.536
so either governments
or communities get together,

00:08:18.560 --> 00:08:22.296
and they decide collectively
what kind of outcome they want

00:08:22.320 --> 00:08:24.976
and what sort of constraints
on individual behavior

00:08:25.000 --> 00:08:26.200
they need to implement.

00:08:27.600 --> 00:08:30.216
And then using monitoring and enforcement,

00:08:30.240 --> 00:08:32.799
they can make sure
that the public good is preserved.

00:08:33.440 --> 00:08:35.015
So why don't we just,

00:08:35.039 --> 00:08:36.535
as regulators,

00:08:36.559 --> 00:08:39.456
require that all cars minimize harm?

00:08:39.480 --> 00:08:41.720
After all, this is
what people say they want.

00:08:43.200 --> 00:08:44.616
And more importantly,

00:08:44.640 --> 00:08:47.736
I can be sure that as an individual,

00:08:47.760 --> 00:08:51.616
if I buy a car that may
sacrifice me in a very rare case,

00:08:51.640 --> 00:08:53.296
I'm not the only sucker doing that

00:08:53.320 --> 00:08:56.000
while everybody else
enjoys unconditional protection.

00:08:57.120 --> 00:09:00.456
In our survey, we did ask people
whether they would support regulation

00:09:00.480 --> 00:09:01.680
and here's what we found.

00:09:02.360 --> 00:09:06.120
First of all, people
said no to regulation;

00:09:07.280 --> 00:09:08.536
and second, they said,

00:09:08.560 --> 00:09:12.496
"Well if you regulate cars to do this
and to minimize total harm,

00:09:12.520 --> 00:09:14.000
I will not buy those cars."

00:09:15.400 --> 00:09:16.776
So ironically,

00:09:16.800 --> 00:09:20.296
by regulating cars to minimize harm,

00:09:20.320 --> 00:09:22.160
we may actually end up with more harm

00:09:23.040 --> 00:09:26.696
because people may not
opt into the safer technology

00:09:26.720 --> 00:09:28.800
even if it's much safer
than human drivers.

00:09:30.360 --> 00:09:33.776
I don't have the final
answer to this riddle,

00:09:33.800 --> 00:09:35.376
but I think as a starting point,

00:09:35.400 --> 00:09:38.696
we need society to come together

00:09:38.720 --> 00:09:41.480
to decide what trade-offs
we are comfortable with

00:09:42.360 --> 00:09:45.840
and to come up with ways
in which we can enforce those trade-offs.

00:09:46.520 --> 00:09:49.056
As a starting point,
my brilliant students,

00:09:49.080 --> 00:09:51.536
Edmond Awad and Sohan Dsouza,

00:09:51.560 --> 00:09:53.360
built the Moral Machine website,

00:09:54.200 --> 00:09:56.880
which generates random scenarios at you --

00:09:58.080 --> 00:10:00.536
basically a bunch
of random dilemmas in a sequence

00:10:00.560 --> 00:10:04.480
where you have to choose what
the car should do in a given scenario.

00:10:05.040 --> 00:10:09.640
And we vary the ages and even
the species of the different victims.

00:10:11.040 --> 00:10:14.736
So far we've collected
over five million decisions

00:10:14.760 --> 00:10:16.960
by over one million people worldwide

00:10:18.400 --> 00:10:19.600
from the website.

00:10:20.360 --> 00:10:22.776
And this is helping us
form an early picture

00:10:22.800 --> 00:10:25.416
of what trade-offs
people are comfortable with

00:10:25.440 --> 00:10:27.336
and what matters to them --

00:10:27.360 --> 00:10:28.800
even across cultures.

00:10:30.240 --> 00:10:31.736
But more importantly,

00:10:31.760 --> 00:10:35.136
doing this exercise
is helping people recognize

00:10:35.160 --> 00:10:37.976
the difficulty of making those choices

00:10:38.000 --> 00:10:41.800
and that the regulators
are tasked with impossible choices.

00:10:43.360 --> 00:10:46.936
And maybe this will help us as a society
understand the kinds of trade-offs

00:10:46.960 --> 00:10:50.016
that will be implemented
ultimately in regulation.

00:10:50.040 --> 00:10:51.776
And indeed, I was very happy to hear

00:10:51.800 --> 00:10:53.816
that the first set of regulations

00:10:53.840 --> 00:10:55.976
that came from
the Department of Transport --

00:10:56.000 --> 00:10:57.376
announced last week --

00:10:57.400 --> 00:11:03.976
included a 15-point checklist
for all carmakers to provide,

00:11:04.000 --> 00:11:07.256
and number 14 was ethical consideration --

00:11:07.280 --> 00:11:09.000
how are you going to deal with that.

00:11:11.800 --> 00:11:14.456
We also have people
reflect on their own decisions

00:11:14.480 --> 00:11:17.480
by giving them summaries
of what they chose.

00:11:18.440 --> 00:11:20.096
I'll give you one example --

00:11:20.120 --> 00:11:23.656
I'm just going to warn you
that this is not your typical example,

00:11:23.680 --> 00:11:25.056
your typical user.

00:11:25.080 --> 00:11:28.696
This is the most sacrificed and the most
saved character for this person.

00:11:28.720 --> 00:11:33.920
(Laughter)

00:11:34.680 --> 00:11:36.576
Some of you may agree with him,

00:11:36.600 --> 00:11:38.240
or her, we don't know.

00:11:40.480 --> 00:11:46.616
But this person also seems to slightly
prefer passengers over pedestrians

00:11:46.640 --> 00:11:48.736
in their choices

00:11:48.760 --> 00:11:51.576
and is very happy to punish jaywalking.

00:11:51.600 --> 00:11:54.640
(Laughter)

00:11:57.320 --> 00:11:58.536
So let's wrap up.

00:11:58.559 --> 00:12:01.975
We started with the question --
let's call it the ethical dilemma --

00:12:02.000 --> 00:12:05.056
of what the car should do
in a specific scenario:

00:12:05.080 --> 00:12:06.280
swerve or stay?

00:12:07.240 --> 00:12:09.976
But then we realized
that the problem was a different one.

00:12:10.000 --> 00:12:14.536
It was the problem of how to get
society to agree on and enforce

00:12:14.560 --> 00:12:16.496
the trade-offs they're comfortable with.

00:12:16.520 --> 00:12:17.776
It's a social dilemma.

00:12:17.800 --> 00:12:22.816
In the 1940s, Isaac Asimov
wrote his famous laws of robotics --

00:12:22.840 --> 00:12:24.160
the three laws of robotics.

00:12:25.240 --> 00:12:27.696
A robot may not harm a human being,

00:12:27.720 --> 00:12:30.256
a robot may not disobey a human being,

00:12:30.280 --> 00:12:33.536
and a robot may not allow
itself to come to harm --

00:12:33.560 --> 00:12:35.520
in this order of importance.

00:12:36.360 --> 00:12:38.496
But after 40 years or so

00:12:38.520 --> 00:12:42.256
and after so many stories
pushing these laws to the limit,

00:12:42.280 --> 00:12:45.976
Asimov introduced the zeroth law

00:12:46.000 --> 00:12:48.256
which takes precedence above all,

00:12:48.280 --> 00:12:51.560
and it's that a robot
may not harm humanity as a whole.

00:12:52.480 --> 00:12:56.856
I don't know what this means
in the context of driverless cars

00:12:56.880 --> 00:12:59.616
or any specific situation,

00:12:59.640 --> 00:13:01.856
and I don't know how we can implement it,

00:13:01.880 --> 00:13:03.416
but I think that by recognizing

00:13:03.440 --> 00:13:09.576
that the regulation of driverless cars
is not only a technological problem

00:13:09.600 --> 00:13:12.880
but also a societal cooperation problem,

00:13:13.800 --> 00:13:16.680
I hope that we can at least begin
to ask the right questions.

00:13:17.200 --> 00:13:18.416
Thank you.

00:13:18.440 --> 00:13:21.360
(Applause)

