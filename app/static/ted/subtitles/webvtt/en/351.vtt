WEBVTT

00:00:00.000 --> 00:00:06.000
If you ask people about what part of psychology do they think is hard,

00:00:06.000 --> 00:00:09.000
and you say, "Well, what about thinking and emotions?"

00:00:09.000 --> 00:00:12.000
Most people will say, "Emotions are terribly hard.

00:00:12.000 --> 00:00:18.000
They're incredibly complex. They can't -- I have no idea of how they work.

00:00:18.000 --> 00:00:20.000
But thinking is really very straightforward:

00:00:20.000 --> 00:00:24.000
it's just sort of some kind of logical reasoning, or something.

00:00:24.000 --> 00:00:27.000
But that's not the hard part."

00:00:27.000 --> 00:00:29.000
So here's a list of problems that come up.

00:00:29.000 --> 00:00:32.000
One nice problem is, what do we do about health?

00:00:32.000 --> 00:00:36.000
The other day, I was reading something, and the person said

00:00:36.000 --> 00:00:42.000
probably the largest single cause of disease is handshaking in the West.

00:00:42.000 --> 00:00:46.000
And there was a little study about people who don't handshake,

00:00:46.000 --> 00:00:49.000
and comparing them with ones who do handshake.

00:00:49.000 --> 00:00:54.000
And I haven't the foggiest idea of where you find the ones that don't handshake,

00:00:54.000 --> 00:00:57.000
because they must be hiding.

00:00:57.000 --> 00:01:01.000
And the people who avoid that

00:01:01.000 --> 00:01:05.000
have 30 percent less infectious disease or something.

00:01:05.000 --> 00:01:08.000
Or maybe it was 31 and a quarter percent.

00:01:08.000 --> 00:01:12.000
So if you really want to solve the problem of epidemics and so forth,

00:01:12.000 --> 00:01:16.000
let's start with that. And since I got that idea,

00:01:16.000 --> 00:01:20.000
I've had to shake hundreds of hands.

00:01:20.000 --> 00:01:25.000
And I think the only way to avoid it

00:01:25.000 --> 00:01:27.000
is to have some horrible visible disease,

00:01:27.000 --> 00:01:30.000
and then you don't have to explain.

00:01:30.000 --> 00:01:34.000
Education: how do we improve education?

00:01:34.000 --> 00:01:38.000
Well, the single best way is to get them to understand

00:01:38.000 --> 00:01:41.000
that what they're being told is a whole lot of nonsense.

00:01:41.000 --> 00:01:43.000
And then, of course, you have to do something

00:01:43.000 --> 00:01:48.000
about how to moderate that, so that anybody can -- so they'll listen to you.

00:01:48.000 --> 00:01:52.000
Pollution, energy shortage, environmental diversity, poverty.

00:01:52.000 --> 00:01:56.000
How do we make stable societies? Longevity.

00:01:56.000 --> 00:01:59.000
Okay, there're lots of problems to worry about.

00:01:59.000 --> 00:02:01.000
Anyway, the question I think people should talk about --

00:02:01.000 --> 00:02:06.000
and it's absolutely taboo -- is, how many people should there be?

00:02:06.000 --> 00:02:13.000
And I think it should be about 100 million or maybe 500 million.

00:02:13.000 --> 00:02:18.000
And then notice that a great many of these problems disappear.

00:02:18.000 --> 00:02:20.000
If you had 100 million people

00:02:20.000 --> 00:02:26.000
properly spread out, then if there's some garbage,

00:02:26.000 --> 00:02:33.000
you throw it away, preferably where you can't see it, and it will rot.

00:02:33.000 --> 00:02:38.000
Or you throw it into the ocean and some fish will benefit from it.

00:02:38.000 --> 00:02:40.000
The problem is, how many people should there be?

00:02:40.000 --> 00:02:43.000
And it's a sort of choice we have to make.

00:02:43.000 --> 00:02:46.000
Most people are about 60 inches high or more,

00:02:46.000 --> 00:02:50.000
and there's these cube laws. So if you make them this big,

00:02:50.000 --> 00:02:53.000
by using nanotechnology, I suppose --

00:02:53.000 --> 00:02:54.000
(Laughter)

00:02:54.000 --> 00:02:56.000
-- then you could have a thousand times as many.

00:02:56.000 --> 00:02:58.000
That would solve the problem, but I don't see anybody

00:02:58.000 --> 00:03:01.000
doing any research on making people smaller.

00:03:01.000 --> 00:03:06.000
Now, it's nice to reduce the population, but a lot of people want to have children.

00:03:06.000 --> 00:03:09.000
And there's one solution that's probably only a few years off.

00:03:09.000 --> 00:03:14.000
You know you have 46 chromosomes. If you're lucky, you've got 23

00:03:14.000 --> 00:03:20.000
from each parent. Sometimes you get an extra one or drop one out,

00:03:20.000 --> 00:03:24.000
but -- so you can skip the grandparent and great-grandparent stage

00:03:24.000 --> 00:03:29.000
and go right to the great-great-grandparent. And you have 46 people

00:03:29.000 --> 00:03:32.000
and you give them a scanner, or whatever you need,

00:03:32.000 --> 00:03:36.000
and they look at their chromosomes and each of them says

00:03:36.000 --> 00:03:41.000
which one he likes best, or she -- no reason to have just two sexes

00:03:41.000 --> 00:03:46.000
any more, even. So each child has 46 parents,

00:03:46.000 --> 00:03:52.000
and I suppose you could let each group of 46 parents have 15 children.

00:03:52.000 --> 00:03:54.000
Wouldn't that be enough? And then the children

00:03:54.000 --> 00:03:58.000
would get plenty of support, and nurturing, and mentoring,

00:03:58.000 --> 00:04:00.000
and the world population would decline very rapidly

00:04:00.000 --> 00:04:03.000
and everybody would be totally happy.

00:04:03.000 --> 00:04:06.000
Timesharing is a little further off in the future.

00:04:06.000 --> 00:04:09.000
And there's this great novel that Arthur Clarke wrote twice,

00:04:09.000 --> 00:04:13.000
called "Against the Fall of Night" and "The City and the Stars."

00:04:13.000 --> 00:04:16.000
They're both wonderful and largely the same,

00:04:16.000 --> 00:04:18.000
except that computers happened in between.

00:04:18.000 --> 00:04:23.000
And Arthur was looking at this old book, and he said, "Well, that was wrong.

00:04:23.000 --> 00:04:25.000
The future must have some computers."

00:04:25.000 --> 00:04:30.000
So in the second version of it, there are 100 billion

00:04:30.000 --> 00:04:38.000
or 1,000 billion people on Earth, but they're all stored on hard disks or floppies,

00:04:38.000 --> 00:04:40.000
or whatever they have in the future.

00:04:40.000 --> 00:04:44.000
And you let a few million of them out at a time.

00:04:44.000 --> 00:04:48.000
A person comes out, they live for a thousand years

00:04:48.000 --> 00:04:54.000
doing whatever they do, and then, when it's time to go back

00:04:54.000 --> 00:04:58.000
for a billion years -- or a million, I forget, the numbers don't matter --

00:04:58.000 --> 00:05:02.000
but there really aren't very many people on Earth at a time.

00:05:02.000 --> 00:05:04.000
And you get to think about yourself and your memories,

00:05:04.000 --> 00:05:09.000
and before you go back into suspension, you edit your memories

00:05:09.000 --> 00:05:12.000
and you change your personality and so forth.

00:05:12.000 --> 00:05:18.000
The plot of the book is that there's not enough diversity,

00:05:18.000 --> 00:05:21.000
so that the people who designed the city

00:05:21.000 --> 00:05:25.000
make sure that every now and then an entirely new person is created.

00:05:25.000 --> 00:05:31.000
And in the novel, a particular one named Alvin is created. And he says,

00:05:31.000 --> 00:05:35.000
maybe this isn't the best way, and wrecks the whole system.

00:05:35.000 --> 00:05:37.000
I don't think the solutions that I proposed

00:05:37.000 --> 00:05:40.000
are good enough or smart enough.

00:05:40.000 --> 00:05:44.000
I think the big problem is that we're not smart enough

00:05:44.000 --> 00:05:48.000
to understand which of the problems we're facing are good enough.

00:05:48.000 --> 00:05:52.000
Therefore, we have to build super intelligent machines like HAL.

00:05:52.000 --> 00:05:57.000
As you remember, at some point in the book for "2001,"

00:05:57.000 --> 00:06:02.000
HAL realizes that the universe is too big, and grand, and profound

00:06:02.000 --> 00:06:06.000
for those really stupid astronauts. If you contrast HAL's behavior

00:06:06.000 --> 00:06:10.000
with the triviality of the people on the spaceship,

00:06:10.000 --> 00:06:13.000
you can see what's written between the lines.

00:06:13.000 --> 00:06:16.000
Well, what are we going to do about that? We could get smarter.

00:06:16.000 --> 00:06:21.000
I think that we're pretty smart, as compared to chimpanzees,

00:06:21.000 --> 00:06:27.000
but we're not smart enough to deal with the colossal problems that we face,

00:06:27.000 --> 00:06:29.000
either in abstract mathematics

00:06:29.000 --> 00:06:34.000
or in figuring out economies, or balancing the world around.

00:06:34.000 --> 00:06:37.000
So one thing we can do is live longer.

00:06:37.000 --> 00:06:39.000
And nobody knows how hard that is,

00:06:39.000 --> 00:06:42.000
but we'll probably find out in a few years.

00:06:42.000 --> 00:06:45.000
You see, there's two forks in the road. We know that people live

00:06:45.000 --> 00:06:49.000
twice as long as chimpanzees almost,

00:06:49.000 --> 00:06:53.000
and nobody lives more than 120 years,

00:06:53.000 --> 00:06:56.000
for reasons that aren't very well understood.

00:06:56.000 --> 00:06:59.000
But lots of people now live to 90 or 100,

00:06:59.000 --> 00:07:03.000
unless they shake hands too much or something like that.

00:07:03.000 --> 00:07:08.000
And so maybe if we lived 200 years, we could accumulate enough skills

00:07:08.000 --> 00:07:13.000
and knowledge to solve some problems.

00:07:13.000 --> 00:07:15.000
So that's one way of going about it.

00:07:15.000 --> 00:07:18.000
And as I said, we don't know how hard that is. It might be --

00:07:18.000 --> 00:07:24.000
after all, most other mammals live half as long as the chimpanzee,

00:07:24.000 --> 00:07:27.000
so we're sort of three and a half or four times, have four times

00:07:27.000 --> 00:07:33.000
the longevity of most mammals. And in the case of the primates,

00:07:33.000 --> 00:07:37.000
we have almost the same genes. We only differ from chimpanzees,

00:07:37.000 --> 00:07:43.000
in the present state of knowledge, which is absolute hogwash,

00:07:43.000 --> 00:07:45.000
maybe by just a few hundred genes.

00:07:45.000 --> 00:07:48.000
What I think is that the gene counters don't know what they're doing yet.

00:07:48.000 --> 00:07:51.000
And whatever you do, don't read anything about genetics

00:07:51.000 --> 00:07:54.000
that's published within your lifetime, or something.

00:07:54.000 --> 00:07:57.000
(Laughter)

00:07:57.000 --> 00:08:01.000
The stuff has a very short half-life, same with brain science.

00:08:01.000 --> 00:08:07.000
And so it might be that if we just fix four or five genes,

00:08:07.000 --> 00:08:09.000
we can live 200 years.

00:08:09.000 --> 00:08:12.000
Or it might be that it's just 30 or 40,

00:08:12.000 --> 00:08:14.000
and I doubt that it's several hundred.

00:08:14.000 --> 00:08:18.000
So this is something that people will be discussing

00:08:18.000 --> 00:08:21.000
and lots of ethicists -- you know, an ethicist is somebody

00:08:21.000 --> 00:08:24.000
who sees something wrong with whatever you have in mind.

00:08:24.000 --> 00:08:27.000
(Laughter)

00:08:27.000 --> 00:08:31.000
And it's very hard to find an ethicist who considers any change

00:08:31.000 --> 00:08:35.000
worth making, because he says, what about the consequences?

00:08:35.000 --> 00:08:38.000
And, of course, we're not responsible for the consequences

00:08:38.000 --> 00:08:44.000
of what we're doing now, are we? Like all this complaint about clones.

00:08:44.000 --> 00:08:47.000
And yet two random people will mate and have this child,

00:08:47.000 --> 00:08:51.000
and both of them have some pretty rotten genes,

00:08:51.000 --> 00:08:55.000
and the child is likely to come out to be average.

00:08:55.000 --> 00:09:01.000
Which, by chimpanzee standards, is very good indeed.

00:09:01.000 --> 00:09:04.000
If we do have longevity, then we'll have to face the population growth

00:09:04.000 --> 00:09:08.000
problem anyway. Because if people live 200 or 1,000 years,

00:09:08.000 --> 00:09:14.000
then we can't let them have a child more than about once every 200 or 1,000 years.

00:09:14.000 --> 00:09:17.000
And so there won't be any workforce.

00:09:17.000 --> 00:09:21.000
And one of the things Laurie Garrett pointed out, and others have,

00:09:21.000 --> 00:09:26.000
is that a society that doesn't have people

00:09:26.000 --> 00:09:29.000
of working age is in real trouble. And things are going to get worse,

00:09:29.000 --> 00:09:35.000
because there's nobody to educate the children or to feed the old.

00:09:35.000 --> 00:09:37.000
And when I'm talking about a long lifetime, of course,

00:09:37.000 --> 00:09:43.000
I don't want somebody who's 200 years old to be like our image

00:09:43.000 --> 00:09:47.000
of what a 200-year-old is -- which is dead, actually.

00:09:47.000 --> 00:09:49.000
You know, there's about 400 different parts of the brain

00:09:49.000 --> 00:09:51.000
which seem to have different functions.

00:09:51.000 --> 00:09:54.000
Nobody knows how most of them work in detail,

00:09:54.000 --> 00:09:58.000
but we do know that there're lots of different things in there.

00:09:58.000 --> 00:10:00.000
And they don't always work together. I like Freud's theory

00:10:00.000 --> 00:10:04.000
that most of them are cancelling each other out.

00:10:04.000 --> 00:10:08.000
And so if you think of yourself as a sort of city

00:10:08.000 --> 00:10:14.000
with a hundred resources, then, when you're afraid, for example,

00:10:14.000 --> 00:10:18.000
you may discard your long-range goals, but you may think deeply

00:10:18.000 --> 00:10:22.000
and focus on exactly how to achieve that particular goal.

00:10:22.000 --> 00:10:25.000
You throw everything else away. You become a monomaniac --

00:10:25.000 --> 00:10:29.000
all you care about is not stepping out on that platform.

00:10:29.000 --> 00:10:33.000
And when you're hungry, food becomes more attractive, and so forth.

00:10:33.000 --> 00:10:39.000
So I see emotions as highly evolved subsets of your capability.

00:10:39.000 --> 00:10:43.000
Emotion is not something added to thought. An emotional state

00:10:43.000 --> 00:10:47.000
is what you get when you remove 100 or 200

00:10:47.000 --> 00:10:50.000
of your normally available resources.

00:10:50.000 --> 00:10:53.000
So thinking of emotions as the opposite of -- as something

00:10:53.000 --> 00:10:57.000
less than thinking is immensely productive. And I hope,

00:10:57.000 --> 00:11:01.000
in the next few years, to show that this will lead to smart machines.

00:11:01.000 --> 00:11:04.000
And I guess I better skip all the rest of this, which are some details

00:11:04.000 --> 00:11:09.000
on how we might make those smart machines and --

00:11:09.000 --> 00:11:14.000
(Laughter)

00:11:14.000 --> 00:11:19.000
-- and the main idea is in fact that the core of a really smart machine

00:11:19.000 --> 00:11:24.000
is one that recognizes that a certain kind of problem is facing you.

00:11:24.000 --> 00:11:27.000
This is a problem of such and such a type,

00:11:27.000 --> 00:11:32.000
and therefore there's a certain way or ways of thinking

00:11:32.000 --> 00:11:34.000
that are good for that problem.

00:11:34.000 --> 00:11:38.000
So I think the future, main problem of psychology is to classify

00:11:38.000 --> 00:11:42.000
types of predicaments, types of situations, types of obstacles

00:11:42.000 --> 00:11:48.000
and also to classify available and possible ways to think and pair them up.

00:11:48.000 --> 00:11:51.000
So you see, it's almost like a Pavlovian --

00:11:51.000 --> 00:11:53.000
we lost the first hundred years of psychology

00:11:53.000 --> 00:11:56.000
by really trivial theories, where you say,

00:11:56.000 --> 00:12:02.000
how do people learn how to react to a situation? What I'm saying is,

00:12:02.000 --> 00:12:07.000
after we go through a lot of levels, including designing

00:12:07.000 --> 00:12:10.000
a huge, messy system with thousands of ports,

00:12:10.000 --> 00:12:14.000
we'll end up again with the central problem of psychology.

00:12:14.000 --> 00:12:17.000
Saying, not what are the situations,

00:12:17.000 --> 00:12:19.000
but what are the kinds of problems

00:12:19.000 --> 00:12:22.000
and what are the kinds of strategies, how do you learn them,

00:12:22.000 --> 00:12:25.000
how do you connect them up, how does a really creative person

00:12:25.000 --> 00:12:30.000
invent a new way of thinking out of the available resources and so forth.

00:12:30.000 --> 00:12:32.000
So, I think in the next 20 years,

00:12:32.000 --> 00:12:37.000
if we can get rid of all of the traditional approaches to artificial intelligence,

00:12:37.000 --> 00:12:39.000
like neural nets and genetic algorithms

00:12:39.000 --> 00:12:45.000
and rule-based systems, and just turn our sights a little bit higher to say,

00:12:45.000 --> 00:12:47.000
can we make a system that can use all those things

00:12:47.000 --> 00:12:51.000
for the right kind of problem? Some problems are good for neural nets;

00:12:51.000 --> 00:12:54.000
we know that others, neural nets are hopeless on them.

00:12:54.000 --> 00:12:57.000
Genetic algorithms are great for certain things;

00:12:57.000 --> 00:13:01.000
I suspect I know what they're bad at, and I won't tell you.

00:13:01.000 --> 00:13:02.000
(Laughter)

00:13:02.000 --> 00:13:04.000
Thank you.

00:13:04.000 --> 00:13:10.000
(Applause)

