WEBVTT

00:00:00.000 --> 00:00:05.000
What I want to tell you about today is how I see robots invading our lives

00:00:05.000 --> 00:00:08.000
at multiple levels, over multiple timescales.

00:00:08.000 --> 00:00:12.000
And when I look out in the future, I can't imagine a world, 500 years from now,

00:00:12.000 --> 00:00:14.000
where we don't have robots everywhere.

00:00:14.000 --> 00:00:19.000
Assuming -- despite all the dire predictions from many people about our future --

00:00:19.000 --> 00:00:23.000
assuming we're still around, I can't imagine the world not being populated with robots.

00:00:23.000 --> 00:00:26.000
And then the question is, well, if they're going to be here in 500 years,

00:00:26.000 --> 00:00:28.000
are they going to be everywhere sooner than that?

00:00:28.000 --> 00:00:30.000
Are they going to be around in 50 years?

00:00:30.000 --> 00:00:33.000
Yeah, I think that's pretty likely -- there's going to be lots of robots everywhere.

00:00:33.000 --> 00:00:36.000
And in fact I think that's going to be a lot sooner than that.

00:00:36.000 --> 00:00:40.000
I think we're sort of on the cusp of robots becoming common,

00:00:40.000 --> 00:00:46.000
and I think we're sort of around 1978 or 1980 in personal computer years,

00:00:46.000 --> 00:00:49.000
where the first few robots are starting to appear.

00:00:49.000 --> 00:00:53.000
Computers sort of came around through games and toys.

00:00:53.000 --> 00:00:56.000
And you know, the first computer most people had in the house

00:00:56.000 --> 00:00:58.000
may have been a computer to play Pong,

00:00:58.000 --> 00:01:00.000
a little microprocessor embedded,

00:01:00.000 --> 00:01:03.000
and then other games that came after that.

00:01:03.000 --> 00:01:06.000
And we're starting to see that same sort of thing with robots:

00:01:06.000 --> 00:01:10.000
LEGO Mindstorms, Furbies -- who here -- did anyone here have a Furby?

00:01:10.000 --> 00:01:13.000
Yeah, there's 38 million of them sold worldwide.

00:01:13.000 --> 00:01:15.000
They are pretty common. And they're a little tiny robot,

00:01:15.000 --> 00:01:17.000
a simple robot with some sensors,

00:01:17.000 --> 00:01:19.000
a little bit of processing actuation.

00:01:19.000 --> 00:01:22.000
On the right there is another robot doll, who you could get a couple of years ago.

00:01:22.000 --> 00:01:24.000
And just as in the early days,

00:01:24.000 --> 00:01:29.000
when there was a lot of sort of amateur interaction over computers,

00:01:29.000 --> 00:01:33.000
you can now get various hacking kits, how-to-hack books.

00:01:33.000 --> 00:01:37.000
And on the left there is a platform from Evolution Robotics,

00:01:37.000 --> 00:01:40.000
where you put a PC on, and you program this thing with a GUI

00:01:40.000 --> 00:01:43.000
to wander around your house and do various stuff.

00:01:43.000 --> 00:01:46.000
And then there's a higher price point sort of robot toys --

00:01:46.000 --> 00:01:50.000
the Sony Aibo. And on the right there, is one that the NEC developed,

00:01:50.000 --> 00:01:53.000
the PaPeRo, which I don't think they're going to release.

00:01:53.000 --> 00:01:56.000
But nevertheless, those sorts of things are out there.

00:01:56.000 --> 00:02:00.000
And we've seen, over the last two or three years, lawn-mowing robots,

00:02:00.000 --> 00:02:06.000
Husqvarna on the bottom, Friendly Robotics on top there, an Israeli company.

00:02:06.000 --> 00:02:08.000
And then in the last 12 months or so

00:02:08.000 --> 00:02:12.000
we've started to see a bunch of home-cleaning robots appear.

00:02:12.000 --> 00:02:15.000
The top left one is a very nice home-cleaning robot

00:02:15.000 --> 00:02:19.000
from a company called Dyson, in the U.K. Except it was so expensive --

00:02:19.000 --> 00:02:21.000
3,500 dollars -- they didn't release it.

00:02:21.000 --> 00:02:24.000
But at the bottom left, you see Electrolux, which is on sale.

00:02:24.000 --> 00:02:26.000
Another one from Karcher.

00:02:26.000 --> 00:02:28.000
At the bottom right is one that I built in my lab

00:02:28.000 --> 00:02:31.000
about 10 years ago, and we finally turned that into a product.

00:02:31.000 --> 00:02:33.000
And let me just show you that.

00:02:33.000 --> 00:02:37.000
We're going to give this away I think, Chris said, after the talk.

00:02:37.000 --> 00:02:43.000
This is a robot that you can go out and buy, and that will clean up your floor.

00:02:47.000 --> 00:02:52.000
And it starts off sort of just going around in ever-increasing circles.

00:02:52.000 --> 00:02:56.000
If it hits something -- you people see that?

00:02:56.000 --> 00:02:59.000
Now it's doing wall-following, it's following around my feet

00:02:59.000 --> 00:03:03.000
to clean up around me. Let's see, let's --

00:03:03.000 --> 00:03:08.000
oh, who stole my Rice Krispies? They stole my Rice Krispies!

00:03:08.000 --> 00:03:14.000
(Laughter)

00:03:14.000 --> 00:03:17.000
Don't worry, relax, no, relax, it's a robot, it's smart!

00:03:17.000 --> 00:03:20.000
(Laughter)

00:03:20.000 --> 00:03:24.000
See, the three-year-old kids, they don't worry about it.

00:03:24.000 --> 00:03:26.000
It's grown-ups that get really upset.

00:03:26.000 --> 00:03:27.000
(Laughter)

00:03:27.000 --> 00:03:29.000
We'll just put some crap here.

00:03:29.000 --> 00:03:33.000
(Laughter)

00:03:33.000 --> 00:03:35.000
Okay.

00:03:35.000 --> 00:03:39.000
(Laughter)

00:03:39.000 --> 00:03:42.000
I don't know if you see -- so, I put a bunch of Rice Krispies there,

00:03:42.000 --> 00:03:49.000
I put some pennies, let's just shoot it at that, see if it cleans up.

00:03:52.000 --> 00:03:54.000
Yeah, OK. So --

00:03:54.000 --> 00:03:58.000
we'll leave that for later.

00:03:58.000 --> 00:04:03.000
(Applause)

00:04:04.000 --> 00:04:08.000
Part of the trick was building a better cleaning mechanism, actually;

00:04:08.000 --> 00:04:12.000
the intelligence on board was fairly simple.

00:04:12.000 --> 00:04:14.000
And that's true with a lot of robots.

00:04:14.000 --> 00:04:18.000
We've all, I think, become, sort of computational chauvinists,

00:04:18.000 --> 00:04:20.000
and think that computation is everything,

00:04:20.000 --> 00:04:22.000
but the mechanics still matter.

00:04:22.000 --> 00:04:25.000
Here's another robot, the PackBot,

00:04:25.000 --> 00:04:27.000
that we've been building for a bunch of years.

00:04:27.000 --> 00:04:33.000
It's a military surveillance robot, to go in ahead of troops --

00:04:33.000 --> 00:04:36.000
looking at caves, for instance.

00:04:36.000 --> 00:04:38.000
But we had to make it fairly robust,

00:04:38.000 --> 00:04:45.000
much more robust than the robots we build in our labs.

00:04:45.000 --> 00:04:48.000
(Laughter)

00:04:54.000 --> 00:04:58.000
On board that robot is a PC running Linux.

00:04:58.000 --> 00:05:04.000
It can withstand a 400G shock. The robot has local intelligence:

00:05:04.000 --> 00:05:10.000
it can flip itself over, can get itself into communication range,

00:05:10.000 --> 00:05:13.000
can go upstairs by itself, et cetera.

00:05:20.000 --> 00:05:24.000
Okay, so it's doing local navigation there.

00:05:24.000 --> 00:05:30.000
A soldier gives it a command to go upstairs, and it does.

00:05:31.000 --> 00:05:34.000
That was not a controlled descent.

00:05:34.000 --> 00:05:36.000
(Laughter)

00:05:36.000 --> 00:05:38.000
Now it's going to head off.

00:05:38.000 --> 00:05:43.000
And the big breakthrough for these robots, really, was September 11th.

00:05:43.000 --> 00:05:47.000
We had the robots down at the World Trade Center late that evening.

00:05:48.000 --> 00:05:50.000
Couldn't do a lot in the main rubble pile,

00:05:50.000 --> 00:05:53.000
things were just too -- there was nothing left to do.

00:05:53.000 --> 00:05:58.000
But we did go into all the surrounding buildings that had been evacuated,

00:05:58.000 --> 00:06:01.000
and searched for possible survivors in the buildings

00:06:01.000 --> 00:06:03.000
that were too dangerous to go into.

00:06:03.000 --> 00:06:05.000
Let's run this video.

00:06:05.000 --> 00:06:08.000
Reporter: ...battlefield companions are helping to reduce the combat risks.

00:06:08.000 --> 00:06:11.000
Nick Robertson has that story.

00:06:13.000 --> 00:06:15.000
Rodney Brooks: Can we have another one of these?

00:06:20.000 --> 00:06:22.000
Okay, good.

00:06:25.000 --> 00:06:28.000
So, this is a corporal who had seen a robot two weeks previously.

00:06:30.000 --> 00:06:34.000
He's sending robots into caves, looking at what's going on.

00:06:34.000 --> 00:06:36.000
The robot's being totally autonomous.

00:06:36.000 --> 00:06:40.000
The worst thing that's happened in the cave so far

00:06:40.000 --> 00:06:43.000
was one of the robots fell down ten meters.

00:06:50.000 --> 00:06:53.000
So one year ago, the US military didn't have these robots.

00:06:53.000 --> 00:06:55.000
Now they're on active duty in Afghanistan every day.

00:06:55.000 --> 00:06:58.000
And that's one of the reasons they say a robot invasion is happening.

00:06:58.000 --> 00:07:02.000
There's a sea change happening in how -- where technology's going.

00:07:02.000 --> 00:07:04.000
Thanks.

00:07:05.000 --> 00:07:07.000
And over the next couple of months,

00:07:07.000 --> 00:07:10.000
we're going to be sending robots in production

00:07:10.000 --> 00:07:14.000
down producing oil wells to get that last few years of oil out of the ground.

00:07:14.000 --> 00:07:18.000
Very hostile environments, 150Ëš C, 10,000 PSI.

00:07:18.000 --> 00:07:22.000
Autonomous robots going down, doing this sort of work.

00:07:22.000 --> 00:07:25.000
But robots like this, they're a little hard to program.

00:07:25.000 --> 00:07:27.000
How, in the future, are we going to program our robots

00:07:27.000 --> 00:07:29.000
and make them easier to use?

00:07:29.000 --> 00:07:32.000
And I want to actually use a robot here --

00:07:32.000 --> 00:07:37.000
a robot named Chris -- stand up. Yeah. Okay.

00:07:39.000 --> 00:07:43.000
Come over here. Now notice, he thinks robots have to be a bit stiff.

00:07:43.000 --> 00:07:46.000
He sort of does that. But I'm going to --

00:07:46.000 --> 00:07:48.000
Chris Anderson: I'm just British. RB: Oh.

00:07:48.000 --> 00:07:50.000
(Laughter)

00:07:50.000 --> 00:07:52.000
(Applause)

00:07:52.000 --> 00:07:55.000
I'm going to show this robot a task. It's a very complex task.

00:07:55.000 --> 00:07:58.000
Now notice, he nodded there, he was giving me some indication

00:07:58.000 --> 00:08:01.000
he was understanding the flow of communication.

00:08:01.000 --> 00:08:03.000
And if I'd said something completely bizarre

00:08:03.000 --> 00:08:06.000
he would have looked askance at me, and regulated the conversation.

00:08:06.000 --> 00:08:09.000
So now I brought this up in front of him.

00:08:09.000 --> 00:08:13.000
I'd looked at his eyes, and I saw his eyes looked at this bottle top.

00:08:13.000 --> 00:08:15.000
And I'm doing this task here, and he's checking up.

00:08:15.000 --> 00:08:18.000
His eyes are going back and forth up to me, to see what I'm looking at --

00:08:18.000 --> 00:08:20.000
so we've got shared attention.

00:08:20.000 --> 00:08:23.000
And so I do this task, and he looks, and he looks to me

00:08:23.000 --> 00:08:27.000
to see what's happening next. And now I'll give him the bottle,

00:08:27.000 --> 00:08:29.000
and we'll see if he can do the task. Can you do that?

00:08:29.000 --> 00:08:32.000
(Laughter)

00:08:32.000 --> 00:08:36.000
Okay. He's pretty good. Yeah. Good, good, good.

00:08:36.000 --> 00:08:38.000
I didn't show you how to do that.

00:08:38.000 --> 00:08:40.000
Now see if you can put it back together.

00:08:40.000 --> 00:08:42.000
(Laughter)

00:08:42.000 --> 00:08:43.000
And he thinks a robot has to be really slow.

00:08:43.000 --> 00:08:45.000
Good robot, that's good.

00:08:45.000 --> 00:08:47.000
So we saw a bunch of things there.

00:08:48.000 --> 00:08:51.000
We saw when we're interacting,

00:08:51.000 --> 00:08:55.000
we're trying to show someone how to do something, we direct their visual attention.

00:08:55.000 --> 00:08:59.000
The other thing communicates their internal state to us,

00:08:59.000 --> 00:09:02.000
whether he's understanding or not, regulates a social interaction.

00:09:02.000 --> 00:09:04.000
There was shared attention looking at the same sort of thing,

00:09:04.000 --> 00:09:08.000
and recognizing socially communicated reinforcement at the end.

00:09:08.000 --> 00:09:11.000
And we've been trying to put that into our lab robots

00:09:11.000 --> 00:09:15.000
because we think this is how you're going to want to interact with robots in the future.

00:09:15.000 --> 00:09:17.000
I just want to show you one technical diagram here.

00:09:17.000 --> 00:09:21.000
The most important thing for building a robot that you can interact with socially

00:09:21.000 --> 00:09:23.000
is its visual attention system.

00:09:23.000 --> 00:09:26.000
Because what it pays attention to is what it's seeing

00:09:26.000 --> 00:09:29.000
and interacting with, and what you're understanding what it's doing.

00:09:29.000 --> 00:09:32.000
So in the videos I'm about to show you,

00:09:32.000 --> 00:09:36.000
you're going to see a visual attention system on a robot

00:09:36.000 --> 00:09:40.000
which has -- it looks for skin tone in HSV space,

00:09:40.000 --> 00:09:44.000
so it works across all human colorings.

00:09:44.000 --> 00:09:46.000
It looks for highly saturated colors, from toys.

00:09:46.000 --> 00:09:48.000
And it looks for things that move around.

00:09:48.000 --> 00:09:51.000
And it weights those together into an attention window,

00:09:51.000 --> 00:09:53.000
and it looks for the highest-scoring place --

00:09:53.000 --> 00:09:55.000
the stuff where the most interesting stuff is happening --

00:09:55.000 --> 00:09:59.000
and that is what its eyes then segue to.

00:09:59.000 --> 00:10:01.000
And it looks right at that.

00:10:01.000 --> 00:10:04.000
At the same time, some top-down sort of stuff:

00:10:04.000 --> 00:10:07.000
might decide that it's lonely and look for skin tone,

00:10:07.000 --> 00:10:10.000
or might decide that it's bored and look for a toy to play with.

00:10:10.000 --> 00:10:12.000
And so these weights change.

00:10:12.000 --> 00:10:14.000
And over here on the right,

00:10:14.000 --> 00:10:17.000
this is what we call the Steven Spielberg memorial module.

00:10:17.000 --> 00:10:19.000
Did people see the movie "AI"? (Audience: Yes.)

00:10:19.000 --> 00:10:21.000
RB: Yeah, it was really bad, but --

00:10:21.000 --> 00:10:25.000
remember, especially when Haley Joel Osment, the little robot,

00:10:25.000 --> 00:10:29.000
looked at the blue fairy for 2,000 years without taking his eyes off it?

00:10:29.000 --> 00:10:31.000
Well, this gets rid of that,

00:10:31.000 --> 00:10:35.000
because this is a habituation Gaussian that gets negative,

00:10:35.000 --> 00:10:38.000
and more and more intense as it looks at one thing.

00:10:38.000 --> 00:10:41.000
And it gets bored, so it will then look away at something else.

00:10:41.000 --> 00:10:45.000
So, once you've got that -- and here's a robot, here's Kismet,

00:10:45.000 --> 00:10:49.000
looking around for a toy. You can tell what it's looking at.

00:10:49.000 --> 00:10:54.000
You can estimate its gaze direction from those eyeballs covering its camera,

00:10:54.000 --> 00:10:57.000
and you can tell when it's actually seeing the toy.

00:10:57.000 --> 00:10:59.000
And it's got a little bit of an emotional response here.

00:10:59.000 --> 00:11:00.000
(Laughter)

00:11:00.000 --> 00:11:02.000
But it's still going to pay attention

00:11:02.000 --> 00:11:06.000
if something more significant comes into its field of view --

00:11:06.000 --> 00:11:10.000
such as Cynthia Breazeal, the builder of this robot, from the right.

00:11:10.000 --> 00:11:15.000
It sees her, pays attention to her.

00:11:15.000 --> 00:11:19.000
Kismet has an underlying, three-dimensional emotional space,

00:11:19.000 --> 00:11:22.000
a vector space, of where it is emotionally.

00:11:22.000 --> 00:11:27.000
And at different places in that space, it expresses --

00:11:28.000 --> 00:11:30.000
can we have the volume on here?

00:11:30.000 --> 00:11:32.000
Can you hear that now, out there? (Audience: Yeah.)

00:11:32.000 --> 00:11:37.000
Kismet: Do you really think so? Do you really think so?

00:11:39.000 --> 00:11:41.000
Do you really think so?

00:11:42.000 --> 00:11:45.000
RB: So it's expressing its emotion through its face

00:11:45.000 --> 00:11:47.000
and the prosody in its voice.

00:11:47.000 --> 00:11:51.000
And when I was dealing with my robot over here,

00:11:51.000 --> 00:11:54.000
Chris, the robot, was measuring the prosody in my voice,

00:11:54.000 --> 00:11:59.000
and so we have the robot measure prosody for four basic messages

00:11:59.000 --> 00:12:03.000
that mothers give their children pre-linguistically.

00:12:03.000 --> 00:12:06.000
Here we've got naive subjects praising the robot:

00:12:08.000 --> 00:12:10.000
Voice: Nice robot.

00:12:11.000 --> 00:12:13.000
You're such a cute little robot.

00:12:13.000 --> 00:12:15.000
(Laughter)

00:12:15.000 --> 00:12:17.000
RB: And the robot's reacting appropriately.

00:12:17.000 --> 00:12:21.000
Voice: ...very good, Kismet.

00:12:22.000 --> 00:12:24.000
(Laughter)

00:12:24.000 --> 00:12:26.000
Voice: Look at my smile.

00:12:28.000 --> 00:12:31.000
RB: It smiles. She imitates the smile. This happens a lot.

00:12:31.000 --> 00:12:33.000
These are naive subjects.

00:12:33.000 --> 00:12:36.000
Here we asked them to get the robot's attention

00:12:36.000 --> 00:12:39.000
and indicate when they have the robot's attention.

00:12:39.000 --> 00:12:43.000
Voice: Hey, Kismet, ah, there it is.

00:12:43.000 --> 00:12:47.000
RB: So she realizes she has the robot's attention.

00:12:50.000 --> 00:12:54.000
Voice: Kismet, do you like the toy? Oh.

00:12:55.000 --> 00:12:57.000
RB: Now, here they're asked to prohibit the robot,

00:12:57.000 --> 00:13:01.000
and this first woman really pushes the robot into an emotional corner.

00:13:01.000 --> 00:13:06.000
Voice: No. No. You're not to do that. No.

00:13:06.000 --> 00:13:09.000
(Laughter)

00:13:09.000 --> 00:13:15.000
Not appropriate. No. No.

00:13:15.000 --> 00:13:18.000
(Laughter)

00:13:18.000 --> 00:13:20.000
RB: I'm going to leave it at that.

00:13:20.000 --> 00:13:22.000
We put that together. Then we put in turn taking.

00:13:22.000 --> 00:13:25.000
When we talk to someone, we talk.

00:13:25.000 --> 00:13:29.000
Then we sort of raise our eyebrows, move our eyes,

00:13:29.000 --> 00:13:32.000
give the other person the idea it's their turn to talk.

00:13:32.000 --> 00:13:36.000
And then they talk, and then we pass the baton back and forth between each other.

00:13:36.000 --> 00:13:38.000
So we put this in the robot.

00:13:38.000 --> 00:13:40.000
We got a bunch of naive subjects in,

00:13:40.000 --> 00:13:42.000
we didn't tell them anything about the robot,

00:13:42.000 --> 00:13:44.000
sat them down in front of the robot and said, talk to the robot.

00:13:44.000 --> 00:13:46.000
Now what they didn't know was,

00:13:46.000 --> 00:13:48.000
the robot wasn't understanding a word they said,

00:13:48.000 --> 00:13:51.000
and that the robot wasn't speaking English.

00:13:51.000 --> 00:13:53.000
It was just saying random English phonemes.

00:13:53.000 --> 00:13:55.000
And I want you to watch carefully, at the beginning of this,

00:13:55.000 --> 00:13:59.000
where this person, Ritchie, who happened to talk to the robot for 25 minutes --

00:13:59.000 --> 00:14:01.000
(Laughter)

00:14:01.000 --> 00:14:03.000
-- says, "I want to show you something.

00:14:03.000 --> 00:14:05.000
I want to show you my watch."

00:14:05.000 --> 00:14:10.000
And he brings the watch center, into the robot's field of vision,

00:14:10.000 --> 00:14:12.000
points to it, gives it a motion cue,

00:14:12.000 --> 00:14:14.000
and the robot looks at the watch quite successfully.

00:14:14.000 --> 00:14:17.000
We don't know whether he understood or not that the robot --

00:14:18.000 --> 00:14:20.000
Notice the turn-taking.

00:14:20.000 --> 00:14:23.000
Ritchie: OK, I want to show you something. OK, this is a watch

00:14:23.000 --> 00:14:26.000
that my girlfriend gave me.

00:14:26.000 --> 00:14:28.000
Robot: Oh, cool.

00:14:28.000 --> 00:14:32.000
Ritchie: Yeah, look, it's got a little blue light in it too. I almost lost it this week.

00:14:33.000 --> 00:14:37.000
(Laughter)

00:14:37.000 --> 00:14:40.000
RB: So it's making eye contact with him, following his eyes.

00:14:40.000 --> 00:14:42.000
Ritchie: Can you do the same thing? Robot: Yeah, sure.

00:14:42.000 --> 00:14:44.000
RB: And they successfully have that sort of communication.

00:14:44.000 --> 00:14:48.000
And here's another aspect of the sorts of things that Chris and I were doing.

00:14:48.000 --> 00:14:50.000
This is another robot, Cog.

00:14:50.000 --> 00:14:56.000
They first make eye contact, and then, when Christie looks over at this toy,

00:14:56.000 --> 00:14:58.000
the robot estimates her gaze direction

00:14:58.000 --> 00:15:00.000
and looks at the same thing that she's looking at.

00:15:00.000 --> 00:15:01.000
(Laughter)

00:15:01.000 --> 00:15:04.000
So we're going to see more and more of this sort of robot

00:15:04.000 --> 00:15:06.000
over the next few years in labs.

00:15:06.000 --> 00:15:11.000
But then the big questions, two big questions that people ask me are:

00:15:11.000 --> 00:15:13.000
if we make these robots more and more human-like,

00:15:13.000 --> 00:15:18.000
will we accept them, will we -- will they need rights eventually?

00:15:18.000 --> 00:15:21.000
And the other question people ask me is, will they want to take over?

00:15:21.000 --> 00:15:22.000
(Laughter)

00:15:22.000 --> 00:15:25.000
And on the first -- you know, this has been a very Hollywood theme

00:15:25.000 --> 00:15:28.000
with lots of movies. You probably recognize these characters here --

00:15:28.000 --> 00:15:32.000
where in each of these cases, the robots want more respect.

00:15:32.000 --> 00:15:35.000
Well, do you ever need to give robots respect?

00:15:36.000 --> 00:15:38.000
They're just machines, after all.

00:15:38.000 --> 00:15:42.000
But I think, you know, we have to accept that we are just machines.

00:15:42.000 --> 00:15:47.000
After all, that's certainly what modern molecular biology says about us.

00:15:47.000 --> 00:15:50.000
You don't see a description of how, you know,

00:15:50.000 --> 00:15:54.000
Molecule A, you know, comes up and docks with this other molecule.

00:15:54.000 --> 00:15:57.000
And it's moving forward, you know, propelled by various charges,

00:15:57.000 --> 00:16:01.000
and then the soul steps in and tweaks those molecules so that they connect.

00:16:01.000 --> 00:16:04.000
It's all mechanistic. We are mechanism.

00:16:04.000 --> 00:16:07.000
If we are machines, then in principle at least,

00:16:07.000 --> 00:16:11.000
we should be able to build machines out of other stuff,

00:16:11.000 --> 00:16:15.000
which are just as alive as we are.

00:16:15.000 --> 00:16:17.000
But I think for us to admit that,

00:16:17.000 --> 00:16:20.000
we have to give up on our special-ness, in a certain way.

00:16:20.000 --> 00:16:22.000
And we've had the retreat from special-ness

00:16:22.000 --> 00:16:25.000
under the barrage of science and technology many times

00:16:25.000 --> 00:16:27.000
over the last few hundred years, at least.

00:16:27.000 --> 00:16:29.000
500 years ago we had to give up the idea

00:16:29.000 --> 00:16:32.000
that we are the center of the universe

00:16:32.000 --> 00:16:34.000
when the earth started to go around the sun;

00:16:34.000 --> 00:16:39.000
150 years ago, with Darwin, we had to give up the idea we were different from animals.

00:16:39.000 --> 00:16:42.000
And to imagine -- you know, it's always hard for us.

00:16:42.000 --> 00:16:45.000
Recently we've been battered with the idea that maybe

00:16:45.000 --> 00:16:47.000
we didn't even have our own creation event, here on earth,

00:16:47.000 --> 00:16:50.000
which people didn't like much. And then the human genome said,

00:16:50.000 --> 00:16:53.000
maybe we only have 35,000 genes. And that was really --

00:16:53.000 --> 00:16:56.000
people didn't like that, we've got more genes than that.

00:16:56.000 --> 00:16:59.000
We don't like to give up our special-ness, so, you know,

00:16:59.000 --> 00:17:01.000
having the idea that robots could really have emotions,

00:17:01.000 --> 00:17:03.000
or that robots could be living creatures --

00:17:03.000 --> 00:17:05.000
I think is going to be hard for us to accept.

00:17:05.000 --> 00:17:09.000
But we're going to come to accept it over the next 50 years or so.

00:17:09.000 --> 00:17:12.000
And the second question is, will the machines want to take over?

00:17:12.000 --> 00:17:17.000
And here the standard scenario is that we create these things,

00:17:17.000 --> 00:17:20.000
they grow, we nurture them, they learn a lot from us,

00:17:20.000 --> 00:17:24.000
and then they start to decide that we're pretty boring, slow.

00:17:24.000 --> 00:17:26.000
They want to take over from us.

00:17:26.000 --> 00:17:29.000
And for those of you that have teenagers, you know what that's like.

00:17:29.000 --> 00:17:30.000
(Laughter)

00:17:30.000 --> 00:17:33.000
But Hollywood extends it to the robots.

00:17:33.000 --> 00:17:36.000
And the question is, you know,

00:17:36.000 --> 00:17:40.000
will someone accidentally build a robot that takes over from us?

00:17:40.000 --> 00:17:43.000
And that's sort of like this lone guy in the backyard,

00:17:43.000 --> 00:17:46.000
you know -- "I accidentally built a 747."

00:17:46.000 --> 00:17:48.000
I don't think that's going to happen.

00:17:48.000 --> 00:17:50.000
And I don't think --

00:17:50.000 --> 00:17:51.000
(Laughter)

00:17:51.000 --> 00:17:54.000
-- I don't think we're going to deliberately build robots

00:17:54.000 --> 00:17:56.000
that we're uncomfortable with.

00:17:56.000 --> 00:17:58.000
We'll -- you know, they're not going to have a super bad robot.

00:17:58.000 --> 00:18:01.000
Before that has to come to be a mildly bad robot,

00:18:01.000 --> 00:18:03.000
and before that a not so bad robot.

00:18:03.000 --> 00:18:04.000
(Laughter)

00:18:04.000 --> 00:18:06.000
And we're just not going to let it go that way.

00:18:06.000 --> 00:18:07.000
(Laughter)

00:18:07.000 --> 00:18:13.000
So, I think I'm going to leave it at that: the robots are coming,

00:18:13.000 --> 00:18:16.000
we don't have too much to worry about, it's going to be a lot of fun,

00:18:16.000 --> 00:18:20.000
and I hope you all enjoy the journey over the next 50 years.

00:18:20.000 --> 00:18:22.000
(Applause)

