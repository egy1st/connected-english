WEBVTT

00:00:00.000 --> 00:00:06.000
What technology can we really apply to reducing global poverty?

00:00:06.000 --> 00:00:10.000
And what I found was quite surprising.

00:00:10.000 --> 00:00:13.000
We started looking at things like death rates in the 20th century,

00:00:13.000 --> 00:00:16.000
and how they'd been improved, and very simple things turned out.

00:00:16.000 --> 00:00:19.000
You'd think maybe antibiotics made more difference than clean water,

00:00:19.000 --> 00:00:22.000
but it's actually the opposite.

00:00:22.000 --> 00:00:25.000
And so very simple things -- off-the-shelf technologies

00:00:25.000 --> 00:00:30.000
that we could easily find on the then-early Web --

00:00:30.000 --> 00:00:35.000
would clearly make a huge difference to that problem.

00:00:35.000 --> 00:00:39.000
But I also, in looking at more powerful technologies

00:00:39.000 --> 00:00:44.000
and nanotechnology and genetic engineering and other new emerging

00:00:44.000 --> 00:00:48.000
kind of digital technologies, became very concerned

00:00:48.000 --> 00:00:52.000
about the potential for abuse.

00:00:52.000 --> 00:00:57.000
If you think about it, in history, a long, long time ago

00:00:57.000 --> 00:01:00.000
we dealt with the problem of an individual abusing another individual.

00:01:00.000 --> 00:01:03.000
We came up with something -- the Ten Commandments: Thou shalt not kill.

00:01:03.000 --> 00:01:05.000
That's a, kind of a one-on-one thing.

00:01:05.000 --> 00:01:09.000
We organized into cities. We had many people.

00:01:09.000 --> 00:01:13.000
And to keep the many from tyrannizing the one,

00:01:13.000 --> 00:01:17.000
we came up with concepts like individual liberty.

00:01:17.000 --> 00:01:18.000
And then, to have to deal with large groups,

00:01:18.000 --> 00:01:21.000
say, at the nation-state level,

00:01:21.000 --> 00:01:23.000
and we had to have mutual non-aggression,

00:01:23.000 --> 00:01:27.000
or through a series of conflicts, we eventually came to

00:01:27.000 --> 00:01:33.000
a rough international bargain to largely keep the peace.

00:01:33.000 --> 00:01:38.000
But now we have a new situation, really what people call

00:01:38.000 --> 00:01:41.000
an asymmetric situation, where technology is so powerful

00:01:41.000 --> 00:01:45.000
that it extends beyond a nation-state.

00:01:45.000 --> 00:01:48.000
It's not the nation-states that have potential access

00:01:48.000 --> 00:01:53.000
to mass destruction, but individuals.

00:01:53.000 --> 00:01:58.000
And this is a consequence of the fact that these new technologies tend to be digital.

00:01:58.000 --> 00:02:02.000
We saw genome sequences.

00:02:02.000 --> 00:02:03.000
You can download the gene sequences

00:02:03.000 --> 00:02:07.000
of pathogens off the Internet if you want to,

00:02:07.000 --> 00:02:12.000
and clearly someone recently -- I saw in a science magazine --

00:02:12.000 --> 00:02:17.000
they said, well, the 1918 flu is too dangerous to FedEx around.

00:02:17.000 --> 00:02:20.000
If people want to use it in their labs for working on research,

00:02:20.000 --> 00:02:23.000
just reconstruct it yourself,

00:02:23.000 --> 00:02:27.000
because, you know, it might break in FedEx.

00:02:27.000 --> 00:02:32.000
So that this is possible to do this is not deniable.

00:02:32.000 --> 00:02:37.000
So individuals in small groups super-empowered by access to these

00:02:37.000 --> 00:02:42.000
kinds of self-replicating technologies, whether it be biological

00:02:42.000 --> 00:02:45.000
or other, are clearly a danger in our world.

00:02:45.000 --> 00:02:49.000
And the danger is that they can cause roughly what's a pandemic.

00:02:49.000 --> 00:02:52.000
And we really don't have experience with pandemics,

00:02:52.000 --> 00:02:55.000
and we're also not very good as a society at acting

00:02:55.000 --> 00:02:59.000
to things we don't have direct and sort of gut-level experience with.

00:02:59.000 --> 00:03:03.000
So it's not in our nature to pre-act.

00:03:03.000 --> 00:03:08.000
And in this case, piling on more technology doesn't solve the problem,

00:03:08.000 --> 00:03:11.000
because it only super-empowers people more.

00:03:11.000 --> 00:03:15.000
So the solution has to be, as people like Russell and Einstein

00:03:15.000 --> 00:03:17.000
and others imagine in a conversation that existed

00:03:17.000 --> 00:03:21.000
in a much stronger form, I think, early in the 20th century,

00:03:21.000 --> 00:03:24.000
that the solution had to be not just the head but the heart.

00:03:24.000 --> 00:03:29.000
You know, public policy and moral progress.

00:03:29.000 --> 00:03:35.000
The bargain that gives us civilization is a bargain to not use power.

00:03:35.000 --> 00:03:38.000
We get our individual rights by society protecting us from others

00:03:38.000 --> 00:03:43.000
not doing everything they can do but largely doing only what is legal.

00:03:43.000 --> 00:03:48.000
And so to limit the danger of these new things, we have to limit,

00:03:48.000 --> 00:03:50.000
ultimately, the ability of individuals

00:03:50.000 --> 00:03:53.000
to have access, essentially, to pandemic power.

00:03:53.000 --> 00:03:57.000
We also have to have sensible defense, because no limitation

00:03:57.000 --> 00:04:00.000
is going to prevent a crazy person from doing something.

00:04:00.000 --> 00:04:02.000
And you know, and the troubling thing is that

00:04:02.000 --> 00:04:04.000
it's much easier to do something bad than to defend

00:04:04.000 --> 00:04:06.000
against all possible bad things,

00:04:06.000 --> 00:04:10.000
so the offensive uses really have an asymmetric advantage.

00:04:10.000 --> 00:04:14.000
So these are the kind of thoughts I was thinking in 1999 and 2000,

00:04:14.000 --> 00:04:16.000
and my friends told me I was getting really depressed,

00:04:16.000 --> 00:04:18.000
and they were really worried about me.

00:04:18.000 --> 00:04:21.000
And then I signed a book contract to write more gloomy thoughts about this

00:04:21.000 --> 00:04:23.000
and moved into a hotel room in New York

00:04:23.000 --> 00:04:27.000
with one room full of books on the Plague,

00:04:27.000 --> 00:04:30.000
and you know, nuclear bombs exploding in New York

00:04:30.000 --> 00:04:33.000
where I would be within the circle, and so on.

00:04:33.000 --> 00:04:37.000
And then I was there on September 11th,

00:04:37.000 --> 00:04:38.000
and I stood in the streets with everyone.

00:04:38.000 --> 00:04:40.000
And it was quite an experience to be there.

00:04:40.000 --> 00:04:43.000
I got up the next morning and walked out of the city,

00:04:43.000 --> 00:04:46.000
and all the sanitation trucks were parked on Houston Street

00:04:46.000 --> 00:04:48.000
and ready to go down and start taking the rubble away.

00:04:48.000 --> 00:04:50.000
And I walked down the middle, up to the train station,

00:04:50.000 --> 00:04:53.000
and everything below 14th Street was closed.

00:04:53.000 --> 00:04:57.000
It was quite a compelling experience, but not really, I suppose,

00:04:57.000 --> 00:05:00.000
a surprise to someone who'd had his room full of the books.

00:05:00.000 --> 00:05:04.000
It was always a surprise that it happened then and there,

00:05:04.000 --> 00:05:08.000
but it wasn't a surprise that it happened at all.

00:05:08.000 --> 00:05:10.000
And everyone then started writing about this.

00:05:10.000 --> 00:05:11.000
Thousands of people started writing about this.

00:05:11.000 --> 00:05:13.000
And I eventually abandoned the book, and then Chris called me

00:05:13.000 --> 00:05:16.000
to talk at the conference. I really don't talk about this anymore

00:05:16.000 --> 00:05:21.000
because, you know, there's enough frustrating and depressing things going on.

00:05:21.000 --> 00:05:24.000
But I agreed to come and say a few things about this.

00:05:24.000 --> 00:05:27.000
And I would say that we can't give up the rule of law

00:05:27.000 --> 00:05:31.000
to fight an asymmetric threat, which is what we seem to be doing

00:05:31.000 --> 00:05:36.000
because of the present, the people that are in power,

00:05:36.000 --> 00:05:41.000
because that's to give up the thing that makes civilization.

00:05:41.000 --> 00:05:44.000
And we can't fight the threat in the kind of stupid way we're doing,

00:05:44.000 --> 00:05:46.000
because a million-dollar act

00:05:46.000 --> 00:05:49.000
causes a billion dollars of damage, causes a trillion dollar response

00:05:49.000 --> 00:05:52.000
which is largely ineffective and arguably, probably almost certainly,

00:05:52.000 --> 00:05:54.000
has made the problem worse.

00:05:54.000 --> 00:05:59.000
So we can't fight the thing with a million-to-one cost,

00:05:59.000 --> 00:06:05.000
one-to-a-million cost-benefit ratio.

00:06:06.000 --> 00:06:11.000
So after giving up on the book -- and I had the great honor

00:06:11.000 --> 00:06:15.000
to be able to join Kleiner Perkins about a year ago,

00:06:15.000 --> 00:06:22.000
and to work through venture capital on the innovative side,

00:06:22.000 --> 00:06:26.000
and to try to find some innovations that could address what I saw as

00:06:26.000 --> 00:06:28.000
some of these big problems.

00:06:28.000 --> 00:06:31.000
Things where, you know, a factor of 10 difference

00:06:31.000 --> 00:06:35.000
can make a factor of 1,000 difference in the outcome.

00:06:35.000 --> 00:06:38.000
I've been amazed in the last year at the incredible quality

00:06:38.000 --> 00:06:43.000
and excitement of the innovations that have come across my desk.

00:06:43.000 --> 00:06:46.000
It's overwhelming at times. I'm very thankful for Google and Wikipedia

00:06:46.000 --> 00:06:50.000
so I can understand at least a little of what people are talking about

00:06:50.000 --> 00:06:52.000
who come through the doors.

00:06:52.000 --> 00:06:55.000
But I wanted to share with you three areas

00:06:55.000 --> 00:06:58.000
that I'm particularly excited about and that relate to the problems

00:06:58.000 --> 00:07:03.000
that I was talking about in the Wired article.

00:07:03.000 --> 00:07:05.000
The first is this whole area of education,

00:07:05.000 --> 00:07:09.000
and it really relates to what Nicholas was talking about with a $100 computer.

00:07:09.000 --> 00:07:13.000
And that is to say that there's a lot of legs left in Moore's Law.

00:07:13.000 --> 00:07:17.000
The most advanced transistors today are at 65 nanometers,

00:07:17.000 --> 00:07:20.000
and we've seen, and I've had the pleasure to invest

00:07:20.000 --> 00:07:26.000
in, companies that give me great confidence that we'll extend Moore's Law

00:07:26.000 --> 00:07:29.000
all the way down to roughly the 10 nanometer scale.

00:07:29.000 --> 00:07:35.000
Another factor of, say, six in dimensional reduction,

00:07:35.000 --> 00:07:40.000
which should give us about another factor of 100 in raw improvement

00:07:40.000 --> 00:07:45.000
in what the chips can do. And so, to put that in practical terms,

00:07:45.000 --> 00:07:49.000
if something costs about 1,000 dollars today,

00:07:49.000 --> 00:07:54.000
say, the best personal computer you can buy, that might be its cost,

00:07:54.000 --> 00:08:00.000
I think we can have that in 2020 for 10 dollars. Okay?

00:08:00.000 --> 00:08:05.000
Now, just imagine what that $100 computer will be in 2020

00:08:05.000 --> 00:08:07.000
as a tool for education.

00:08:07.000 --> 00:08:09.000
I think the challenge for us is --

00:08:09.000 --> 00:08:11.000
I'm very certain that that will happen, the challenge is,

00:08:11.000 --> 00:08:16.000
will we develop the kind of educational tools and things with the net

00:08:16.000 --> 00:08:19.000
to let us take advantage of that device?

00:08:19.000 --> 00:08:23.000
I'd argue today that we have incredibly powerful computers,

00:08:23.000 --> 00:08:25.000
but we don't have very good software for them.

00:08:25.000 --> 00:08:28.000
And it's only in retrospect, after the better software comes along,

00:08:28.000 --> 00:08:30.000
and you take it and you run it on a ten-year-old machine, you say,

00:08:30.000 --> 00:08:32.000
God, the machine was that fast?

00:08:32.000 --> 00:08:34.000
I remember when they took the Apple Mac interface

00:08:34.000 --> 00:08:37.000
and they put it back on the Apple II.

00:08:37.000 --> 00:08:40.000
The Apple II was perfectly capable of running that kind of interface,

00:08:40.000 --> 00:08:43.000
we just didn't know how to do it at the time.

00:08:43.000 --> 00:08:45.000
So given that we know and should believe --

00:08:45.000 --> 00:08:48.000
because Moore's Law's been, like, a constant,

00:08:48.000 --> 00:08:51.000
I mean, it's just been very predictable progress

00:08:51.000 --> 00:08:54.000
over the last 40 years or whatever.

00:08:54.000 --> 00:08:58.000
We can know what the computers are going to be like in 2020.

00:08:58.000 --> 00:09:00.000
It's great that we have initiatives to say,

00:09:00.000 --> 00:09:03.000
let's go create the education and educate people in the world,

00:09:03.000 --> 00:09:05.000
because that's a great force for peace.

00:09:05.000 --> 00:09:08.000
And we can give everyone in the world a $100 computer

00:09:08.000 --> 00:09:13.000
or a $10 computer in the next 15 years.

00:09:13.000 --> 00:09:18.000
The second area that I'm focusing on is the environmental problem,

00:09:18.000 --> 00:09:22.000
because that's clearly going to put a lot of pressure on this world.

00:09:22.000 --> 00:09:26.000
We'll hear a lot more about that from Al Gore very shortly.

00:09:26.000 --> 00:09:29.000
The thing that we see as the kind of Moore's Law trend

00:09:29.000 --> 00:09:32.000
that's driving improvement in our ability to address

00:09:32.000 --> 00:09:36.000
the environmental problem is new materials.

00:09:36.000 --> 00:09:40.000
We have a challenge, because the urban population is growing

00:09:40.000 --> 00:09:43.000
in this century from two billion to six billion

00:09:43.000 --> 00:09:45.000
in a very short amount of time. People are moving to the cities.

00:09:45.000 --> 00:09:48.000
They all need clean water, they need energy, they need transportation,

00:09:48.000 --> 00:09:52.000
and we want them to develop in a green way.

00:09:52.000 --> 00:09:54.000
We're reasonably efficient in the industrial sectors.

00:09:54.000 --> 00:09:57.000
We've made improvements in energy and resource efficiency,

00:09:57.000 --> 00:10:01.000
but the consumer sector, especially in America, is very inefficient.

00:10:01.000 --> 00:10:05.000
But these new materials bring such incredible innovations

00:10:05.000 --> 00:10:09.000
that there's a strong basis for hope that these things

00:10:09.000 --> 00:10:11.000
will be so profitable that they can be brought to the market.

00:10:11.000 --> 00:10:14.000
And I want to give you a specific example of a new material

00:10:14.000 --> 00:10:17.000
that was discovered 15 years ago.

00:10:17.000 --> 00:10:22.000
If we take carbon nanotubes, you know, Iijima discovered them in 1991,

00:10:22.000 --> 00:10:24.000
they just have incredible properties.

00:10:24.000 --> 00:10:25.000
And these are the kinds of things we're going to discover

00:10:25.000 --> 00:10:28.000
as we start to engineer at the nano scale.

00:10:28.000 --> 00:10:31.000
Their strength: they're almost the strongest material,

00:10:31.000 --> 00:10:33.000
tensile strength material known.

00:10:34.000 --> 00:10:39.000
They're very, very stiff. They stretch very, very little.


00:10:39.000 --> 00:10:42.000
In two dimensions, if you make, like, a fabric out of them,

00:10:42.000 --> 00:10:45.000
they're 30 times stronger than Kevlar.

00:10:45.000 --> 00:10:48.000
And if you make a three-dimensional structure, like a buckyball,

00:10:48.000 --> 00:10:50.000
they have all sorts of incredible properties.

00:10:50.000 --> 00:10:53.000
If you shoot a particle at them and knock a hole in them,

00:10:53.000 --> 00:10:56.000
they repair themselves; they go zip and they repair the hole

00:10:56.000 --> 00:10:59.000
in femtoseconds, which is not -- is really quick.

00:10:59.000 --> 00:11:02.000
(Laughter)

00:11:02.000 --> 00:11:06.000
If you shine a light on them, they produce electricity.

00:11:06.000 --> 00:11:09.000
In fact, if you flash them with a camera they catch on fire.

00:11:09.000 --> 00:11:13.000
If you put electricity on them, they emit light.

00:11:13.000 --> 00:11:16.000
If you run current through them, you can run 1,000 times more current

00:11:16.000 --> 00:11:20.000
through one of these than through a piece of metal.

00:11:20.000 --> 00:11:23.000
You can make both p- and n-type semiconductors,

00:11:23.000 --> 00:11:25.000
which means you can make transistors out of them.

00:11:25.000 --> 00:11:28.000
They conduct heat along their length but not across --

00:11:28.000 --> 00:11:30.000
well, there is no width, but not in the other direction

00:11:30.000 --> 00:11:36.000
if you stack them up; that's a property of carbon fiber also.

00:11:36.000 --> 00:11:39.000
If you put particles in them, and they go shooting out the tip --

00:11:39.000 --> 00:11:42.000
they're like miniature linear accelerators or electron guns.

00:11:42.000 --> 00:11:45.000
The inside of the nanotubes is so small --

00:11:45.000 --> 00:11:47.000
the smallest ones are 0.7 nanometers --

00:11:47.000 --> 00:11:49.000
that it's basically a quantum world.

00:11:49.000 --> 00:11:52.000
It's a strange place inside a nanotube.

00:11:52.000 --> 00:11:55.000
And so we begin to see, and we've seen business plans already,

00:11:55.000 --> 00:11:58.000
where the kind of things Lisa Randall's talking about are in there.

00:11:58.000 --> 00:12:00.000
I had one business plan where I was trying to learn more about

00:12:00.000 --> 00:12:03.000
Witten's cosmic dimension strings to try to understand

00:12:03.000 --> 00:12:06.000
what the phenomenon was going on in this proposed nanomaterial.

00:12:06.000 --> 00:12:12.000
So inside of a nanotube, we're really at the limit here.

00:12:12.000 --> 00:12:16.000
So what we see is with these and other new materials

00:12:16.000 --> 00:12:20.000
that we can do things with different properties -- lighter, stronger --

00:12:20.000 --> 00:12:26.000
and apply these new materials to the environmental problems.

00:12:26.000 --> 00:12:27.000
New materials that can make water,

00:12:27.000 --> 00:12:29.000
new materials that can make fuel cells work better,

00:12:29.000 --> 00:12:33.000
new materials that catalyze chemical reactions,

00:12:33.000 --> 00:12:36.000
that cut pollution and so on.

00:12:36.000 --> 00:12:39.000
Ethanol -- new ways of making ethanol.

00:12:39.000 --> 00:12:42.000
New ways of making electric transportation.

00:12:42.000 --> 00:12:46.000
The whole green dream -- because it can be profitable.

00:12:46.000 --> 00:12:48.000
And we've dedicated -- we've just raised a new fund,

00:12:48.000 --> 00:12:51.000
we dedicated 100 million dollars to these kinds of investments.

00:12:51.000 --> 00:12:55.000
We believe that Genentech, the Compaq, the Lotus, the Sun,

00:12:55.000 --> 00:12:59.000
the Netscape, the Amazon, the Google in these fields

00:12:59.000 --> 00:13:02.000
are yet to be found, because this materials revolution

00:13:02.000 --> 00:13:05.000
will drive these things forward.

00:13:06.000 --> 00:13:08.000
The third area that we're working on,

00:13:08.000 --> 00:13:12.000
and we just announced last week -- we were all in New York.

00:13:12.000 --> 00:13:18.000
We raised 200 million dollars in a specialty fund

00:13:18.000 --> 00:13:22.000
to work on a pandemic in biodefense.

00:13:22.000 --> 00:13:25.000
And to give you an idea of the last fund that Kleiner raised

00:13:25.000 --> 00:13:30.000
was a $400 million fund, so this for us is a very substantial fund.

00:13:30.000 --> 00:13:34.000
And what we did, over the last few months -- well, a few months ago,

00:13:34.000 --> 00:13:37.000
Ray Kurzweil and I wrote an op-ed in the New York Times

00:13:37.000 --> 00:13:40.000
about how publishing the 1918 genome was very dangerous.

00:13:40.000 --> 00:13:44.000
And John Doerr and Brook and others got concerned, [unclear],

00:13:44.000 --> 00:13:48.000
and we started looking around at what the world was doing

00:13:48.000 --> 00:13:53.000
about being prepared for a pandemic. And we saw a lot of gaps.

00:13:53.000 --> 00:13:57.000
And so we asked ourselves, you know, can we find innovative things

00:13:57.000 --> 00:14:01.000
that will go fill these gaps? And Brooks told me in a break here,

00:14:01.000 --> 00:14:03.000
he said he's found so much stuff he can't sleep,

00:14:03.000 --> 00:14:06.000
because there's so many great technologies out there,

00:14:06.000 --> 00:14:09.000
we're essentially buried. And we need them, you know.

00:14:09.000 --> 00:14:12.000
We have one antiviral that people are talking about stockpiling

00:14:12.000 --> 00:14:15.000
that still works, roughly. That's Tamiflu.

00:14:15.000 --> 00:14:20.000
But Tamiflu -- the virus is resistant. It is resistant to Tamiflu.

00:14:20.000 --> 00:14:24.000
We've discovered with AIDS we need cocktails to work well

00:14:24.000 --> 00:14:27.000
so that the viral resistance -- we need several anti-virals.

00:14:27.000 --> 00:14:29.000
We need better surveillance.

00:14:29.000 --> 00:14:32.000
We need networks that can find out what's going on.

00:14:32.000 --> 00:14:36.000
We need rapid diagnostics so that we can tell if somebody has

00:14:36.000 --> 00:14:40.000
a strain of flu which we have only identified very recently.

00:14:40.000 --> 00:14:42.000
We've got to be able to make the rapid diagnostics quickly.

00:14:42.000 --> 00:14:45.000
We need new anti-virals and cocktails. We need new kinds of vaccines.

00:14:45.000 --> 00:14:47.000
Vaccines that are broad spectrum.

00:14:47.000 --> 00:14:51.000
Vaccines that we can manufacture quickly.

00:14:51.000 --> 00:14:53.000
Cocktails, more polyvalent vaccines.

00:14:53.000 --> 00:14:56.000
You normally get a trivalent vaccine against three possible strains.

00:14:56.000 --> 00:14:59.000
We need -- we don't know where this thing is going.

00:14:59.000 --> 00:15:02.000
We believe that if we could fill these 10 gaps,

00:15:02.000 --> 00:15:08.000
we have a chance to help really reduce the risk of a pandemic.

00:15:08.000 --> 00:15:12.000
And the difference between a normal flu season and a pandemic

00:15:12.000 --> 00:15:15.000
is about a factor of 1,000 in deaths

00:15:15.000 --> 00:15:18.000
and certainly enormous economic impact.

00:15:18.000 --> 00:15:21.000
So we're very excited because we think we can fund 10,

00:15:21.000 --> 00:15:25.000
or speed up 10 projects and see them come to market

00:15:25.000 --> 00:15:28.000
in the next couple years that will address this.

00:15:28.000 --> 00:15:31.000
So if we can address, use technology, help address education,

00:15:31.000 --> 00:15:34.000
help address the environment, help address the pandemic,

00:15:34.000 --> 00:15:38.000
does that solve the larger problem that I was talking about

00:15:38.000 --> 00:15:43.000
in the Wired article? And I'm afraid the answer is really no,

00:15:43.000 --> 00:15:47.000
because you can't solve a problem with the management of technology

00:15:47.000 --> 00:15:50.000
with more technology.

00:15:50.000 --> 00:15:55.000
If we let an unlimited amount of power loose, then we will --

00:15:55.000 --> 00:15:57.000
a very small number of people will be able to abuse it.

00:15:57.000 --> 00:16:01.000
We can't fight at a million-to-one disadvantage.

00:16:01.000 --> 00:16:04.000
So what we need to do is, we need better policy.

00:16:04.000 --> 00:16:07.000
And for example, some things we could do

00:16:07.000 --> 00:16:11.000
that would be policy solutions which are not really in the political air right now

00:16:11.000 --> 00:16:15.000
but perhaps with the change of administration would be -- use markets.

00:16:15.000 --> 00:16:17.000
Markets are a very strong force.

00:16:17.000 --> 00:16:20.000
For example, rather than trying to regulate away problems,

00:16:20.000 --> 00:16:22.000
which probably won't work, if we could price

00:16:22.000 --> 00:16:27.000
into the cost of doing business, the cost of catastrophe,

00:16:27.000 --> 00:16:30.000
so that people who are doing things that had a higher cost of catastrophe

00:16:30.000 --> 00:16:33.000
would have to take insurance against that risk.

00:16:33.000 --> 00:16:35.000
So if you wanted to put a drug on the market you could put it on.

00:16:35.000 --> 00:16:37.000
But it wouldn't have to be approved by regulators;

00:16:37.000 --> 00:16:41.000
you'd have to convince an actuary that it would be safe.

00:16:41.000 --> 00:16:44.000
And if you apply the notion of insurance more broadly,

00:16:44.000 --> 00:16:47.000
you can use a more powerful force, a market force,

00:16:47.000 --> 00:16:49.000
to provide feedback.

00:16:49.000 --> 00:16:50.000
How could you keep the law?

00:16:50.000 --> 00:16:52.000
I think the law would be a really good thing to keep.

00:16:52.000 --> 00:16:54.000
Well, you have to hold people accountable.

00:16:54.000 --> 00:16:56.000
The law requires accountability.

00:16:56.000 --> 00:16:59.000
Today scientists, technologists, businessmen, engineers

00:16:59.000 --> 00:17:01.000
don't have any personal responsibility

00:17:01.000 --> 00:17:03.000
for the consequences of their actions.

00:17:03.000 --> 00:17:07.000
So if you tie that -- you have to tie that back with the law.

00:17:07.000 --> 00:17:11.000
And finally, I think we have to do something that's not really --

00:17:11.000 --> 00:17:12.000
it's almost unacceptable to say this -- which,

00:17:12.000 --> 00:17:15.000
we have to begin to design the future.

00:17:15.000 --> 00:17:19.000
We can't pick the future, but we can steer the future.

00:17:19.000 --> 00:17:21.000
Our investment in trying to prevent pandemic flu

00:17:21.000 --> 00:17:25.000
is affecting the distribution of possible outcomes.

00:17:25.000 --> 00:17:27.000
We may not be able to stop it, but the likelihood

00:17:27.000 --> 00:17:31.000
that it will get past us is lower if we focus on that problem.

00:17:31.000 --> 00:17:35.000
So we can design the future if we choose what kind of things

00:17:35.000 --> 00:17:38.000
we want to have happen and not have happen,

00:17:38.000 --> 00:17:41.000
and steer us to a lower-risk place.

00:17:41.000 --> 00:17:47.000
Vice President Gore will talk about how we could steer the climate trajectory

00:17:47.000 --> 00:17:50.000
into a lower probability of catastrophic risk.

00:17:50.000 --> 00:17:53.000
But above all, what we have to do is we have to help the good guys,

00:17:53.000 --> 00:17:55.000
the people on the defensive side,

00:17:55.000 --> 00:17:59.000
have an advantage over the people who want to abuse things.

00:17:59.000 --> 00:18:01.000
And what we have to do to do that

00:18:01.000 --> 00:18:04.000
is we have to limit access to certain information.

00:18:04.000 --> 00:18:07.000
And growing up as we have, and holding very high

00:18:07.000 --> 00:18:11.000
the value of free speech, this is a hard thing for us to accept --

00:18:11.000 --> 00:18:12.000
for all of us to accept.

00:18:12.000 --> 00:18:17.000
It's especially hard for the scientists to accept who still remember,

00:18:17.000 --> 00:18:19.000
you know, Galileo essentially locked up,

00:18:19.000 --> 00:18:23.000
and who are still fighting this battle against the church.

00:18:23.000 --> 00:18:28.000
But that's the price of having a civilization.

00:18:28.000 --> 00:18:30.000
The price of retaining the rule of law

00:18:30.000 --> 00:18:35.000
is to limit the access to the great and kind of unbridled power.

00:18:35.000 --> 00:18:36.000
Thank you.

00:18:36.000 --> 00:18:38.000
(Applause)

