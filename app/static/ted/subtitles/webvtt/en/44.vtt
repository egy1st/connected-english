WEBVTT

00:00:00.000 --> 00:00:03.000
I want to talk today about --

00:00:03.000 --> 00:00:09.000
I've been asked to take the long view, and I'm going to tell you what

00:00:09.000 --> 00:00:13.000
I think are the three biggest problems for humanity

00:00:13.000 --> 00:00:16.000
from this long point of view.

00:00:16.000 --> 00:00:19.000
Some of these have already been touched upon by other speakers,

00:00:19.000 --> 00:00:21.000
which is encouraging.

00:00:21.000 --> 00:00:23.000
It seems that there's not just one person

00:00:23.000 --> 00:00:25.000
who thinks that these problems are important.

00:00:25.000 --> 00:00:29.000
The first is -- death is a big problem.

00:00:29.000 --> 00:00:32.000
If you look at the statistics,

00:00:32.000 --> 00:00:34.000
the odds are not very favorable to us.

00:00:34.000 --> 00:00:38.000
So far, most people who have lived have also died.

00:00:38.000 --> 00:00:42.000
Roughly 90 percent of everybody who has been alive has died by now.

00:00:42.000 --> 00:00:48.000
So the annual death rate adds up to 150,000 --

00:00:48.000 --> 00:00:51.000
sorry, the daily death rate -- 150,000 people per day,

00:00:51.000 --> 00:00:54.000
which is a huge number by any standard.

00:00:54.000 --> 00:00:59.000
The annual death rate, then, becomes 56 million.

00:00:59.000 --> 00:01:04.000
If we just look at the single, biggest cause of death -- aging --

00:01:05.000 --> 00:01:10.000
it accounts for roughly two-thirds of all human people who die.

00:01:10.000 --> 00:01:13.000
That adds up to an annual death toll

00:01:13.000 --> 00:01:15.000
of greater than the population of Canada.

00:01:15.000 --> 00:01:17.000
Sometimes, we don't see a problem

00:01:17.000 --> 00:01:21.000
because either it's too familiar or it's too big.

00:01:21.000 --> 00:01:23.000
Can't see it because it's too big.

00:01:23.000 --> 00:01:26.000
I think death might be both too familiar and too big

00:01:26.000 --> 00:01:29.000
for most people to see it as a problem.

00:01:29.000 --> 00:01:31.000
Once you think about it, you see this is not statistical points;

00:01:31.000 --> 00:01:33.000
these are -- let's see, how far have I talked?

00:01:33.000 --> 00:01:36.000
I've talked for three minutes.

00:01:36.000 --> 00:01:43.000
So that would be, roughly, 324 people have died since I've begun speaking.

00:01:43.000 --> 00:01:47.000
People like -- it's roughly the population in this room has just died.

00:01:48.000 --> 00:01:50.000
Now, the human cost of that is obvious,

00:01:50.000 --> 00:01:53.000
once you start to think about it -- the suffering, the loss --

00:01:53.000 --> 00:01:56.000
it's also, economically, enormously wasteful.

00:01:56.000 --> 00:01:59.000
I just look at the information, and knowledge, and experience

00:01:59.000 --> 00:02:02.000
that is lost due to natural causes of death in general,

00:02:02.000 --> 00:02:04.000
and aging, in particular.

00:02:04.000 --> 00:02:07.000
Suppose we approximated one person with one book?

00:02:07.000 --> 00:02:09.000
Now, of course, this is an underestimation.

00:02:09.000 --> 00:02:15.000
A person's lifetime of learning and experience

00:02:15.000 --> 00:02:17.000
is a lot more than you could put into a single book.

00:02:17.000 --> 00:02:19.000
But let's suppose we did this.

00:02:20.000 --> 00:02:25.000
52 million people die of natural causes each year

00:02:25.000 --> 00:02:29.000
corresponds, then, to 52 million volumes destroyed.

00:02:29.000 --> 00:02:32.000
Library of Congress holds 18 million volumes.

00:02:33.000 --> 00:02:36.000
We are upset about the burning of the Library of Alexandria.

00:02:36.000 --> 00:02:38.000
It's one of the great cultural tragedies

00:02:38.000 --> 00:02:41.000
that we remember, even today.

00:02:42.000 --> 00:02:44.000
But this is the equivalent of three Libraries of Congress --

00:02:44.000 --> 00:02:47.000
burnt down, forever lost -- each year.

00:02:47.000 --> 00:02:49.000
So that's the first big problem.

00:02:49.000 --> 00:02:52.000
And I wish Godspeed to Aubrey de Grey,

00:02:52.000 --> 00:02:54.000
and other people like him,

00:02:54.000 --> 00:02:57.000
to try to do something about this as soon as possible.

00:02:58.000 --> 00:03:01.000
Existential risk -- the second big problem.

00:03:01.000 --> 00:03:08.000
Existential risk is a threat to human survival, or to the long-term potential of our species.

00:03:08.000 --> 00:03:10.000
Now, why do I say that this is a big problem?

00:03:10.000 --> 00:03:14.000
Well, let's first look at the probability --

00:03:14.000 --> 00:03:17.000
and this is very, very difficult to estimate --

00:03:17.000 --> 00:03:20.000
but there have been only four studies on this in recent years,

00:03:20.000 --> 00:03:22.000
which is surprising.

00:03:22.000 --> 00:03:25.000
You would think that it would be of some interest

00:03:25.000 --> 00:03:29.000
to try to find out more about this given that the stakes are so big,

00:03:29.000 --> 00:03:31.000
but it's a very neglected area.

00:03:31.000 --> 00:03:33.000
But there have been four studies --

00:03:33.000 --> 00:03:35.000
one by John Lesley, wrote a book on this.

00:03:35.000 --> 00:03:37.000
He estimated a probability that we will fail

00:03:37.000 --> 00:03:40.000
to survive the current century: 50 percent.

00:03:40.000 --> 00:03:45.000
Similarly, the Astronomer Royal, whom we heard speak yesterday,

00:03:45.000 --> 00:03:48.000
also has a 50 percent probability estimate.

00:03:48.000 --> 00:03:51.000
Another author doesn't give any numerical estimate,

00:03:51.000 --> 00:03:54.000
but says the probability is significant that it will fail.

00:03:54.000 --> 00:03:57.000
I wrote a long paper on this.

00:03:57.000 --> 00:04:01.000
I said assigning a less than 20 percent probability would be a mistake

00:04:01.000 --> 00:04:04.000
in light of the current evidence we have.

00:04:04.000 --> 00:04:06.000
Now, the exact figures here,

00:04:06.000 --> 00:04:08.000
we should take with a big grain of salt,

00:04:08.000 --> 00:04:11.000
but there seems to be a consensus that the risk is substantial.

00:04:11.000 --> 00:04:14.000
Everybody who has looked at this and studied it agrees.

00:04:14.000 --> 00:04:16.000
Now, if we think about what just reducing

00:04:16.000 --> 00:04:21.000
the probability of human extinction by just one percentage point --

00:04:21.000 --> 00:04:26.000
not very much -- so that's equivalent to 60 million lives saved,

00:04:26.000 --> 00:04:30.000
if we just count the currently living people, the current generation.

00:04:30.000 --> 00:04:34.000
Now one percent of six billion people is equivalent to 60 million.

00:04:34.000 --> 00:04:36.000
So that's a large number.

00:04:36.000 --> 00:04:39.000
If we were to take into account future generations

00:04:39.000 --> 00:04:44.000
that will never come into existence if we blow ourselves up,

00:04:44.000 --> 00:04:47.000
then the figure becomes astronomical.

00:04:47.000 --> 00:04:50.000
If we could eventually colonize a chunk of the universe --

00:04:50.000 --> 00:04:52.000
the Virgo supercluster --

00:04:52.000 --> 00:04:54.000
maybe it will take us 100 million years to get there,

00:04:54.000 --> 00:04:57.000
but if we go extinct we never will.

00:04:57.000 --> 00:05:00.000
Then, even a one percentage point reduction

00:05:00.000 --> 00:05:04.000
in the extinction risk could be equivalent

00:05:04.000 --> 00:05:07.000
to this astronomical number -- 10 to the power of 32.

00:05:07.000 --> 00:05:11.000
So if you take into account future generations as much as our own,

00:05:11.000 --> 00:05:16.000
every other moral imperative of philanthropic cost just becomes irrelevant.

00:05:16.000 --> 00:05:18.000
The only thing you should focus on

00:05:18.000 --> 00:05:20.000
would be to reduce existential risk

00:05:20.000 --> 00:05:24.000
because even the tiniest decrease in existential risk

00:05:24.000 --> 00:05:28.000
would just overwhelm any other benefit you could hope to achieve.

00:05:28.000 --> 00:05:30.000
And even if you just look at the current people,

00:05:30.000 --> 00:05:35.000
and ignore the potential that would be lost if we went extinct,

00:05:35.000 --> 00:05:37.000
it should still have a high priority.

00:05:37.000 --> 00:05:42.000
Now, let me spend the rest of my time on the third big problem,

00:05:42.000 --> 00:05:47.000
because it's more subtle and perhaps difficult to grasp.

00:05:48.000 --> 00:05:52.000
Think about some time in your life --

00:05:52.000 --> 00:05:55.000
some people might never have experienced it -- but some people,

00:05:55.000 --> 00:05:58.000
there are just those moments that you have experienced

00:05:58.000 --> 00:06:00.000
where life was fantastic.

00:06:00.000 --> 00:06:07.000
It might have been at the moment of some great, creative inspiration

00:06:07.000 --> 00:06:09.000
you might have had when you just entered this flow stage.

00:06:09.000 --> 00:06:11.000
Or when you understood something you had never done before.

00:06:11.000 --> 00:06:15.000
Or perhaps in the ecstasy of romantic love.

00:06:15.000 --> 00:06:20.000
Or an aesthetic experience -- a sunset or a great piece of art.

00:06:20.000 --> 00:06:22.000
Every once in a while we have these moments,

00:06:22.000 --> 00:06:26.000
and we realize just how good life can be when it's at its best.

00:06:26.000 --> 00:06:31.000
And you wonder, why can't it be like that all the time?

00:06:31.000 --> 00:06:33.000
You just want to cling onto this.

00:06:33.000 --> 00:06:37.000
And then, of course, it drifts back into ordinary life and the memory fades.

00:06:37.000 --> 00:06:41.000
And it's really difficult to recall, in a normal frame of mind,

00:06:41.000 --> 00:06:44.000
just how good life can be at its best.

00:06:44.000 --> 00:06:47.000
Or how bad it can be at its worst.

00:06:47.000 --> 00:06:50.000
The third big problem is that life isn't usually

00:06:50.000 --> 00:06:52.000
as wonderful as it could be.

00:06:52.000 --> 00:06:56.000
I think that's a big, big problem.

00:06:56.000 --> 00:06:58.000
It's easy to say what we don't want.

00:06:59.000 --> 00:07:02.000
Here are a number of things that we don't want --

00:07:02.000 --> 00:07:05.000
illness, involuntary death, unnecessary suffering, cruelty,

00:07:05.000 --> 00:07:10.000
stunted growth, memory loss, ignorance, absence of creativity.

00:07:11.000 --> 00:07:14.000
Suppose we fixed these things -- we did something about all of these.

00:07:14.000 --> 00:07:16.000
We were very successful.

00:07:16.000 --> 00:07:18.000
We got rid of all of these things.

00:07:18.000 --> 00:07:21.000
We might end up with something like this,

00:07:21.000 --> 00:07:25.000
which is -- I mean, it's a heck of a lot better than that.

00:07:25.000 --> 00:07:30.000
But is this really the best we can dream of?

00:07:30.000 --> 00:07:32.000
Is this the best we can do?

00:07:32.000 --> 00:07:38.000
Or is it possible to find something a little bit more inspiring to work towards?

00:07:38.000 --> 00:07:40.000
And if we think about this,

00:07:40.000 --> 00:07:44.000
I think it's very clear that there are ways

00:07:44.000 --> 00:07:47.000
in which we could change things, not just by eliminating negatives,

00:07:47.000 --> 00:07:49.000
but adding positives.

00:07:49.000 --> 00:07:51.000
On my wish list, at least, would be:

00:07:51.000 --> 00:07:56.000
much longer, healthier lives, greater subjective well-being,

00:07:56.000 --> 00:08:01.000
enhanced cognitive capacities, more knowledge and understanding,

00:08:01.000 --> 00:08:03.000
unlimited opportunity for personal growth

00:08:03.000 --> 00:08:07.000
beyond our current biological limits, better relationships,

00:08:07.000 --> 00:08:09.000
an unbounded potential for spiritual, moral

00:08:09.000 --> 00:08:11.000
and intellectual development.

00:08:11.000 --> 00:08:19.000
If we want to achieve this, what, in the world, would have to change?

00:08:19.000 --> 00:08:24.000
And this is the answer -- we would have to change.

00:08:24.000 --> 00:08:27.000
Not just the world around us, but we, ourselves.

00:08:27.000 --> 00:08:31.000
Not just the way we think about the world, but the way we are -- our very biology.

00:08:31.000 --> 00:08:33.000
Human nature would have to change.

00:08:33.000 --> 00:08:35.000
Now, when we think about changing human nature,

00:08:35.000 --> 00:08:37.000
the first thing that comes to mind

00:08:37.000 --> 00:08:41.000
are these human modification technologies --

00:08:41.000 --> 00:08:43.000
growth hormone therapy, cosmetic surgery,

00:08:43.000 --> 00:08:46.000
stimulants like Ritalin, Adderall, anti-depressants,

00:08:46.000 --> 00:08:48.000
anabolic steroids, artificial hearts.

00:08:48.000 --> 00:08:51.000
It's a pretty pathetic list.

00:08:51.000 --> 00:08:53.000
They do great things for a few people

00:08:53.000 --> 00:08:55.000
who suffer from some specific condition,

00:08:55.000 --> 00:09:00.000
but for most people, they don't really transform

00:09:00.000 --> 00:09:02.000
what it is to be human.

00:09:02.000 --> 00:09:04.000
And they also all seem a little bit --

00:09:04.000 --> 00:09:07.000
most people have this instinct that, well, sure,

00:09:07.000 --> 00:09:09.000
there needs to be anti-depressants for the really depressed people.

00:09:09.000 --> 00:09:11.000
But there's a kind of queasiness

00:09:11.000 --> 00:09:14.000
that these are unnatural in some way.

00:09:14.000 --> 00:09:16.000
It's worth recalling that there are a lot of other

00:09:16.000 --> 00:09:19.000
modification technologies and enhancement technologies that we use.

00:09:19.000 --> 00:09:23.000
We have skin enhancements, clothing.

00:09:23.000 --> 00:09:27.000
As far as I can see, all of you are users of this

00:09:27.000 --> 00:09:32.000
enhancement technology in this room, so that's a great thing.

00:09:32.000 --> 00:09:35.000
Mood modifiers have been used from time immemorial --

00:09:35.000 --> 00:09:40.000
caffeine, alcohol, nicotine, immune system enhancement,

00:09:40.000 --> 00:09:42.000
vision enhancement, anesthetics --

00:09:42.000 --> 00:09:44.000
we take that very much for granted,

00:09:44.000 --> 00:09:48.000
but just think about how great progress that is --

00:09:48.000 --> 00:09:52.000
like, having an operation before anesthetics was not fun.

00:09:52.000 --> 00:09:58.000
Contraceptives, cosmetics and brain reprogramming techniques --

00:09:58.000 --> 00:10:00.000
that sounds ominous,

00:10:00.000 --> 00:10:04.000
but the distinction between what is a technology --

00:10:04.000 --> 00:10:06.000
a gadget would be the archetype --

00:10:06.000 --> 00:10:10.000
and other ways of changing and rewriting human nature is quite subtle.

00:10:10.000 --> 00:10:14.000
So if you think about what it means to learn arithmetic or to learn to read,

00:10:14.000 --> 00:10:17.000
you're actually, literally rewriting your own brain.

00:10:17.000 --> 00:10:20.000
You're changing the microstructure of your brain as you go along.

00:10:21.000 --> 00:10:24.000
So in a broad sense, we don't need to think about technology

00:10:24.000 --> 00:10:26.000
as only little gadgets, like these things here,

00:10:26.000 --> 00:10:30.000
but even institutions and techniques,

00:10:30.000 --> 00:10:32.000
psychological methods and so forth.

00:10:32.000 --> 00:10:37.000
Forms of organization can have a profound impact on human nature.

00:10:37.000 --> 00:10:39.000
Looking ahead, there is a range of technologies

00:10:39.000 --> 00:10:42.000
that are almost certain to be developed sooner or later.

00:10:42.000 --> 00:10:46.000
We are very ignorant about what the time scale for these things are,

00:10:46.000 --> 00:10:48.000
but they all are consistent with everything we know

00:10:48.000 --> 00:10:52.000
about physical laws, laws of chemistry, etc.

00:10:52.000 --> 00:10:54.000
It's possible to assume,

00:10:54.000 --> 00:10:57.000
setting aside a possibility of catastrophe,

00:10:57.000 --> 00:11:00.000
that sooner or later we will develop all of these.

00:11:00.000 --> 00:11:03.000
And even just a couple of these would be enough

00:11:03.000 --> 00:11:05.000
to transform the human condition.

00:11:05.000 --> 00:11:10.000
So let's look at some of the dimensions of human nature

00:11:10.000 --> 00:11:13.000
that seem to leave room for improvement.

00:11:13.000 --> 00:11:15.000
Health span is a big and urgent thing,

00:11:15.000 --> 00:11:17.000
because if you're not alive,

00:11:17.000 --> 00:11:20.000
then all the other things will be to little avail.

00:11:20.000 --> 00:11:22.000
Intellectual capacity -- let's take that box,

00:11:22.000 --> 00:11:27.000
which falls into a lot of different sub-categories:

00:11:27.000 --> 00:11:30.000
memory, concentration, mental energy, intelligence, empathy.

00:11:30.000 --> 00:11:32.000
These are really great things.

00:11:32.000 --> 00:11:34.000
Part of the reason why we value these traits

00:11:34.000 --> 00:11:38.000
is that they make us better at competing with other people --

00:11:38.000 --> 00:11:40.000
they're positional goods.

00:11:40.000 --> 00:11:42.000
But part of the reason --

00:11:42.000 --> 00:11:46.000
and that's the reason why we have ethical ground for pursuing these --

00:11:46.000 --> 00:11:49.000
is that they're also intrinsically valuable.

00:11:49.000 --> 00:11:53.000
It's just better to be able to understand more of the world around you

00:11:53.000 --> 00:11:55.000
and the people that you are communicating with,

00:11:55.000 --> 00:11:59.000
and to remember what you have learned.

00:11:59.000 --> 00:12:01.000
Modalities and special faculties.

00:12:01.000 --> 00:12:06.000
Now, the human mind is not a single unitary information processor,

00:12:06.000 --> 00:12:10.000
but it has a lot of different, special, evolved modules

00:12:10.000 --> 00:12:12.000
that do specific things for us.

00:12:12.000 --> 00:12:16.000
If you think about what we normally take as giving life a lot of its meaning --

00:12:16.000 --> 00:12:20.000
music, humor, eroticism, spirituality, aesthetics,

00:12:20.000 --> 00:12:25.000
nurturing and caring, gossip, chatting with people --

00:12:25.000 --> 00:12:29.000
all of these, very likely, are enabled by a special circuitry

00:12:29.000 --> 00:12:31.000
that we humans have,

00:12:31.000 --> 00:12:34.000
but that you could have another intelligent life form that lacks these.

00:12:34.000 --> 00:12:37.000
We're just lucky that we have the requisite neural machinery

00:12:37.000 --> 00:12:41.000
to process music and to appreciate it and enjoy it.

00:12:41.000 --> 00:12:44.000
All of these would enable, in principle -- be amenable to enhancement.

00:12:44.000 --> 00:12:46.000
Some people have a better musical ability

00:12:46.000 --> 00:12:48.000
and ability to appreciate music than others have.

00:12:48.000 --> 00:12:51.000
It's also interesting to think about what other things are --

00:12:51.000 --> 00:12:54.000
so if these all enabled great values,

00:12:55.000 --> 00:12:58.000
why should we think that evolution has happened to provide us

00:12:58.000 --> 00:13:01.000
with all the modalities we would need to engage

00:13:01.000 --> 00:13:03.000
with other values that there might be?

00:13:03.000 --> 00:13:05.000
Imagine a species

00:13:05.000 --> 00:13:09.000
that just didn't have this neural machinery for processing music.

00:13:09.000 --> 00:13:12.000
And they would just stare at us with bafflement

00:13:12.000 --> 00:13:16.000
when we spend time listening to a beautiful performance,

00:13:16.000 --> 00:13:18.000
like the one we just heard -- because of people making stupid movements,

00:13:18.000 --> 00:13:21.000
and they would be really irritated and wouldn't see what we were up to.

00:13:21.000 --> 00:13:24.000
But maybe they have another faculty, something else

00:13:24.000 --> 00:13:27.000
that would seem equally irrational to us,

00:13:27.000 --> 00:13:30.000
but they actually tap into some great possible value there.

00:13:30.000 --> 00:13:34.000
But we are just literally deaf to that kind of value.

00:13:34.000 --> 00:13:36.000
So we could think of adding on different,

00:13:36.000 --> 00:13:40.000
new sensory capacities and mental faculties.

00:13:40.000 --> 00:13:45.000
Bodily functionality and morphology and affective self-control.

00:13:45.000 --> 00:13:47.000
Greater subjective well-being.

00:13:47.000 --> 00:13:50.000
Be able to switch between relaxation and activity --

00:13:50.000 --> 00:13:54.000
being able to go slow when you need to do that, and to speed up.

00:13:54.000 --> 00:13:56.000
Able to switch back and forth more easily

00:13:56.000 --> 00:13:58.000
would be a neat thing to be able to do --

00:13:58.000 --> 00:14:00.000
easier to achieve the flow state,

00:14:00.000 --> 00:14:04.000
when you're totally immersed in something you are doing.

00:14:04.000 --> 00:14:06.000
Conscientiousness and sympathy.

00:14:06.000 --> 00:14:09.000
The ability to -- it's another interesting application

00:14:09.000 --> 00:14:12.000
that would have large social ramification, perhaps.

00:14:12.000 --> 00:14:18.000
If you could actually choose to preserve your romantic attachments to one person,

00:14:18.000 --> 00:14:20.000
undiminished through time,

00:14:20.000 --> 00:14:23.000
so that wouldn't have to -- love would never have to fade if you didn't want it to.

00:14:25.000 --> 00:14:28.000
That's probably not all that difficult.

00:14:28.000 --> 00:14:31.000
It might just be a simple hormone or something that could do this.

00:14:33.000 --> 00:14:35.000
It's been done in voles.

00:14:37.000 --> 00:14:40.000
You can engineer a prairie vole to become monogamous

00:14:40.000 --> 00:14:42.000
when it's naturally polygamous.

00:14:42.000 --> 00:14:44.000
It's just a single gene.

00:14:44.000 --> 00:14:46.000
Might be more complicated in humans, but perhaps not that much.

00:14:46.000 --> 00:14:48.000
This is the last picture that I want to --

00:14:49.000 --> 00:14:51.000
now we've got to use the laser pointer.

00:14:52.000 --> 00:14:55.000
A possible mode of being here would be a way of life --

00:14:55.000 --> 00:14:59.000
a way of being, experiencing, thinking, seeing,

00:14:59.000 --> 00:15:01.000
interacting with the world.

00:15:01.000 --> 00:15:06.000
Down here in this little corner, here, we have the little sub-space

00:15:06.000 --> 00:15:10.000
of this larger space that is accessible to human beings --

00:15:10.000 --> 00:15:13.000
beings with our biological capacities.

00:15:13.000 --> 00:15:16.000
It's a part of the space that's accessible to animals;

00:15:16.000 --> 00:15:19.000
since we are animals, we are a subset of that.

00:15:19.000 --> 00:15:23.000
And then you can imagine some enhancements of human capacities.

00:15:23.000 --> 00:15:26.000
There would be different modes of being you could experience

00:15:26.000 --> 00:15:29.000
if you were able to stay alive for, say, 200 years.

00:15:29.000 --> 00:15:33.000
Then you could live sorts of lives and accumulate wisdoms

00:15:33.000 --> 00:15:36.000
that are just not possible for humans as we currently are.

00:15:36.000 --> 00:15:40.000
So then, you move off to this larger sphere of "human +,"

00:15:40.000 --> 00:15:43.000
and you could continue that process and eventually

00:15:43.000 --> 00:15:47.000
explore a lot of this larger space of possible modes of being.

00:15:47.000 --> 00:15:49.000
Now, why is that a good thing to do?

00:15:49.000 --> 00:15:53.000
Well, we know already that in this little human circle there,

00:15:53.000 --> 00:15:57.000
there are these enormously wonderful and worthwhile modes of being --

00:15:57.000 --> 00:16:00.000
human life at its best is wonderful.

00:16:00.000 --> 00:16:05.000
We have no reason to believe that within this much, much larger space

00:16:05.000 --> 00:16:09.000
there would not also be extremely worthwhile modes of being,

00:16:09.000 --> 00:16:15.000
perhaps ones that would be way beyond our wildest ability

00:16:15.000 --> 00:16:17.000
even to imagine or dream about.

00:16:17.000 --> 00:16:19.000
And so, to fix this third problem,

00:16:19.000 --> 00:16:25.000
I think we need -- slowly, carefully, with ethical wisdom and constraint --

00:16:25.000 --> 00:16:30.000
develop the means that enable us to go out in this larger space and explore it

00:16:30.000 --> 00:16:32.000
and find the great values that might hide there.

00:16:32.000 --> 00:16:34.000
Thanks.

