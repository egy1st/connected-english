WEBVTT

00:00:01.000 --> 00:00:03.000
I thought I'd begin with a scene of war.

00:00:03.000 --> 00:00:05.000
There was little to warn of the danger ahead.

00:00:05.000 --> 00:00:07.000
The Iraqi insurgent had placed the IED,

00:00:07.000 --> 00:00:10.000
an Improvised Explosive Device,

00:00:10.000 --> 00:00:12.000
along the side of the road with great care.

00:00:12.000 --> 00:00:16.000
By 2006, there were more than 2,500

00:00:16.000 --> 00:00:19.000
of these attacks every single month,

00:00:19.000 --> 00:00:21.000
and they were the leading cause of

00:00:21.000 --> 00:00:23.000
casualties among American soldiers

00:00:23.000 --> 00:00:25.000
and Iraqi civilians.

00:00:25.000 --> 00:00:27.000
The team that was hunting for this IED

00:00:27.000 --> 00:00:29.000
is called an EOD team—

00:00:29.000 --> 00:00:31.000
Explosives Ordinance Disposal—and

00:00:31.000 --> 00:00:33.000
they're the pointy end of the spear in the

00:00:33.000 --> 00:00:36.000
American effort to suppress these roadside bombs.

00:00:36.000 --> 00:00:38.000
Each EOD team goes out on about

00:00:38.000 --> 00:00:40.000
600 of these bomb calls every year,

00:00:40.000 --> 00:00:43.000
defusing about two bombs a day.

00:00:43.000 --> 00:00:45.000
Perhaps the best sign of how valuable they

00:00:45.000 --> 00:00:47.000
are to the war effort, is that

00:00:47.000 --> 00:00:49.000
the Iraqi insurgents put a $50,000 bounty

00:00:49.000 --> 00:00:52.000
on the head of a single EOD soldier.

00:00:52.000 --> 00:00:54.000
Unfortunately, this particular call

00:00:54.000 --> 00:00:56.000
would not end well.

00:00:56.000 --> 00:00:58.000
By the time the soldier advanced close

00:00:58.000 --> 00:01:00.000
enough to see the telltale wires

00:01:00.000 --> 00:01:03.000
of the bomb, it exploded in a wave of flame.

00:01:03.000 --> 00:01:05.000
Now, depending how close you are

00:01:05.000 --> 00:01:07.000
and how much explosive has been packed

00:01:07.000 --> 00:01:09.000
into that bomb, it can cause death

00:01:09.000 --> 00:01:11.000
or injury. You have to be as far as

00:01:11.000 --> 00:01:13.000
50 yards away to escape that.

00:01:13.000 --> 00:01:15.000
The blast is so strong it can even break

00:01:15.000 --> 00:01:17.000
your limbs, even if you're not hit.

00:01:17.000 --> 00:01:19.000
That soldier had been on top of the bomb.

00:01:19.000 --> 00:01:22.000
And so when the rest of the team advanced

00:01:22.000 --> 00:01:24.000
they found little left. And that night the unit's

00:01:24.000 --> 00:01:26.000
commander did a sad duty, and he wrote

00:01:26.000 --> 00:01:28.000
a condolence letter back to the United

00:01:28.000 --> 00:01:30.000
States, and he talked about how hard the

00:01:30.000 --> 00:01:32.000
loss had been on his unit, about the fact

00:01:32.000 --> 00:01:34.000
that they had lost their bravest soldier,

00:01:34.000 --> 00:01:36.000
a soldier who had saved their lives

00:01:36.000 --> 00:01:38.000
many a time.

00:01:38.000 --> 00:01:40.000
And he apologized

00:01:40.000 --> 00:01:42.000
for not being able to bring them home.

00:01:42.000 --> 00:01:44.000
But then he talked up the silver lining

00:01:44.000 --> 00:01:46.000
that he took away from the loss.

00:01:46.000 --> 00:01:48.000
"At least," as he wrote, "when a robot dies,

00:01:48.000 --> 00:01:50.000
you don't have to write a letter

00:01:50.000 --> 00:01:52.000
to its mother."

00:01:52.000 --> 00:01:54.000
That scene sounds like science fiction,

00:01:54.000 --> 00:01:56.000
but is battlefield reality already.

00:01:56.000 --> 00:01:59.000
The soldier in that case

00:01:59.000 --> 00:02:02.000
was a 42-pound robot called a PackBot.

00:02:02.000 --> 00:02:05.000
The chief's letter went, not to some

00:02:05.000 --> 00:02:07.000
farmhouse in Iowa like you see

00:02:07.000 --> 00:02:10.000
in the old war movies, but went to

00:02:10.000 --> 00:02:12.000
the iRobot Company, which is

00:02:12.000 --> 00:02:15.000
named after the Asimov novel

00:02:15.000 --> 00:02:17.000
and the not-so-great Will Smith movie,

00:02:17.000 --> 00:02:19.000
and... um... (Laughter)...

00:02:19.000 --> 00:02:21.000
if you remember that

00:02:21.000 --> 00:02:23.000
in that fictional world, robots started out

00:02:23.000 --> 00:02:25.000
carrying out mundane chores, and then

00:02:25.000 --> 00:02:27.000
they started taking on life-and-death decisions.

00:02:27.000 --> 00:02:29.000
That's a reality we face today.

00:02:29.000 --> 00:02:31.000
What we're going to do is actually just

00:02:31.000 --> 00:02:33.000
flash a series of photos behind me that

00:02:33.000 --> 00:02:36.000
show you the reality of robots used in war

00:02:36.000 --> 00:02:38.000
right now or already at the prototype stage.

00:02:38.000 --> 00:02:41.000
It's just to give you a taste.

00:02:41.000 --> 00:02:43.000
Another way of putting it is you're not

00:02:43.000 --> 00:02:45.000
going to see anything that's powered

00:02:45.000 --> 00:02:47.000
by Vulcan technology, or teenage

00:02:47.000 --> 00:02:49.000
wizard hormones or anything like that.

00:02:49.000 --> 00:02:51.000
This is all real. So why don't we

00:02:51.000 --> 00:02:53.000
go ahead and start those pictures.

00:02:53.000 --> 00:02:55.000
Something big is going on in war today,

00:02:55.000 --> 00:02:57.000
and maybe even the history of humanity

00:02:57.000 --> 00:03:00.000
itself. The U.S. military went into Iraq with

00:03:00.000 --> 00:03:02.000
a handful of drones in the air.

00:03:02.000 --> 00:03:05.000
We now have 5,300.

00:03:05.000 --> 00:03:07.000
We went in with zero unmanned ground

00:03:07.000 --> 00:03:11.000
systems. We now have 12,000.

00:03:11.000 --> 00:03:13.000
And the tech term "killer application"

00:03:13.000 --> 00:03:16.000
takes on new meaning in this space.

00:03:16.000 --> 00:03:18.000
And we need to remember that we're

00:03:18.000 --> 00:03:20.000
talking about the Model T Fords,

00:03:20.000 --> 00:03:22.000
the Wright Flyers, compared

00:03:22.000 --> 00:03:24.000
to what's coming soon.

00:03:24.000 --> 00:03:26.000
That's where we're at right now.

00:03:26.000 --> 00:03:28.000
One of the people that I recently met with

00:03:28.000 --> 00:03:30.000
was an Air Force three-star general, and he

00:03:30.000 --> 00:03:32.000
said basically, where we're headed very

00:03:32.000 --> 00:03:34.000
soon is tens of thousands of robots

00:03:34.000 --> 00:03:36.000
operating in our conflicts, and these

00:03:36.000 --> 00:03:38.000
numbers matter, because we're not just

00:03:38.000 --> 00:03:40.000
talking about tens of thousands of today's

00:03:40.000 --> 00:03:42.000
robots, but tens of thousands of these

00:03:42.000 --> 00:03:44.000
prototypes and tomorrow's robots, because

00:03:44.000 --> 00:03:47.000
of course, one of the things that's operating

00:03:47.000 --> 00:03:49.000
in technology is Moore's Law,

00:03:49.000 --> 00:03:51.000
that you can pack in more and more

00:03:51.000 --> 00:03:53.000
computing power into those robots, and so

00:03:53.000 --> 00:03:55.000
flash forward around 25 years,

00:03:55.000 --> 00:03:57.000
if Moore's Law holds true,

00:03:57.000 --> 00:04:00.000
those robots will be close to a billion times

00:04:00.000 --> 00:04:03.000
more powerful in their computing than today.

00:04:03.000 --> 00:04:05.000
And so what that means is the kind of

00:04:05.000 --> 00:04:07.000
things that we used to only talk about at

00:04:07.000 --> 00:04:09.000
science fiction conventions like Comic-Con

00:04:09.000 --> 00:04:11.000
have to be talked about in the halls

00:04:11.000 --> 00:04:13.000
of power and places like the Pentagon.

00:04:13.000 --> 00:04:16.000
A robots revolution is upon us.

00:04:16.000 --> 00:04:18.000
Now, I need to be clear here.

00:04:18.000 --> 00:04:20.000
I'm not talking about a revolution where you

00:04:20.000 --> 00:04:22.000
have to worry about the Governor of

00:04:22.000 --> 00:04:24.000
California showing up at your door,

00:04:24.000 --> 00:04:26.000
a la the Terminator. (Laughter)

00:04:26.000 --> 00:04:28.000
When historians look at this period, they're

00:04:28.000 --> 00:04:30.000
going to conclude that we're in a different

00:04:30.000 --> 00:04:32.000
type of revolution: a revolution in war,

00:04:32.000 --> 00:04:34.000
like the invention of the atomic bomb.

00:04:34.000 --> 00:04:36.000
But it may be even bigger than that,

00:04:36.000 --> 00:04:38.000
because our unmanned systems don't just

00:04:38.000 --> 00:04:40.000
affect the "how" of war-fighting,

00:04:40.000 --> 00:04:42.000
they affect the "who" of fighting

00:04:42.000 --> 00:04:44.000
at its most fundamental level.

00:04:44.000 --> 00:04:46.000
That is, every previous revolution in war, be

00:04:46.000 --> 00:04:48.000
it the machine gun, be it the atomic bomb,

00:04:48.000 --> 00:04:51.000
was about a system that either shot faster,

00:04:51.000 --> 00:04:54.000
went further, had a bigger boom.

00:04:54.000 --> 00:04:57.000
That's certainly the case with robotics, but

00:04:57.000 --> 00:05:00.000
they also change the experience of the warrior

00:05:00.000 --> 00:05:03.000
and even the very identity of the warrior.

00:05:03.000 --> 00:05:06.000
Another way of putting this is that

00:05:06.000 --> 00:05:08.000
mankind's 5,000-year-old monopoly

00:05:08.000 --> 00:05:11.000
on the fighting of war is breaking down

00:05:11.000 --> 00:05:13.000
in our very lifetime. I've spent

00:05:13.000 --> 00:05:15.000
the last several years going around

00:05:15.000 --> 00:05:17.000
meeting with all the players in this field,

00:05:17.000 --> 00:05:19.000
from the robot scientists to the science

00:05:19.000 --> 00:05:21.000
fiction authors who inspired them to the

00:05:21.000 --> 00:05:23.000
19-year-old drone pilots who are fighting

00:05:23.000 --> 00:05:25.000
from Nevada, to the four-star generals

00:05:25.000 --> 00:05:27.000
who command them, to even the Iraqi

00:05:27.000 --> 00:05:29.000
insurgents who they are targeting and what

00:05:29.000 --> 00:05:31.000
they think about our systems, and

00:05:31.000 --> 00:05:33.000
what I found interesting is not just

00:05:33.000 --> 00:05:35.000
their stories, but how their experiences

00:05:35.000 --> 00:05:37.000
point to these ripple effects that are going

00:05:37.000 --> 00:05:39.000
outwards in our society, in our law

00:05:39.000 --> 00:05:41.000
and our ethics, etc. And so what I'd like

00:05:41.000 --> 00:05:43.000
to do with my remaining time is basically

00:05:43.000 --> 00:05:45.000
flesh out a couple of these.

00:05:45.000 --> 00:05:47.000
So the first is that the future of war,

00:05:47.000 --> 00:05:49.000
even a robotics one, is not going to be

00:05:49.000 --> 00:05:51.000
purely an American one.

00:05:51.000 --> 00:05:53.000
The U.S. is currently ahead in military

00:05:53.000 --> 00:05:55.000
robotics right now, but we know that in

00:05:55.000 --> 00:05:57.000
technology there's no such thing as

00:05:57.000 --> 00:06:00.000
a permanent first move or advantage.

00:06:00.000 --> 00:06:02.000
In a quick show of hands, how many

00:06:02.000 --> 00:06:04.000
people in this room still use

00:06:04.000 --> 00:06:06.000
Wang Computers? (Laughter)

00:06:06.000 --> 00:06:08.000
It's the same thing in war. The British and

00:06:08.000 --> 00:06:11.000
the French invented the tank.

00:06:11.000 --> 00:06:13.000
The Germans figured out how

00:06:13.000 --> 00:06:15.000
to use it right, and so what we have to

00:06:15.000 --> 00:06:17.000
think about for the U.S. is that we are

00:06:17.000 --> 00:06:19.000
ahead right now, but you have

00:06:19.000 --> 00:06:21.000
43 other countries out there

00:06:21.000 --> 00:06:23.000
working on military robotics, and they

00:06:23.000 --> 00:06:25.000
include all the interesting countries like

00:06:25.000 --> 00:06:28.000
Russia, China, Pakistan, Iran.

00:06:28.000 --> 00:06:31.000
And this raises a bigger worry for me.

00:06:31.000 --> 00:06:33.000
How do we move forward in this revolution

00:06:33.000 --> 00:06:35.000
given the state of our manufacturing

00:06:35.000 --> 00:06:37.000
and the state of our science and

00:06:37.000 --> 00:06:39.000
mathematics training in our schools?

00:06:39.000 --> 00:06:41.000
Or another way of thinking about this is,

00:06:41.000 --> 00:06:43.000
what does it mean to go to war increasingly

00:06:43.000 --> 00:06:46.000
with soldiers whose hardware is made

00:06:46.000 --> 00:06:51.000
in China and software is written in India?

00:06:51.000 --> 00:06:54.000
But just as software has gone open-source,

00:06:54.000 --> 00:06:56.000
so has warfare.

00:06:56.000 --> 00:06:59.000
Unlike an aircraft carrier or an atomic bomb,

00:06:59.000 --> 00:07:01.000
you don't need a massive manufacturing

00:07:01.000 --> 00:07:03.000
system to build robotics. A lot of it is

00:07:03.000 --> 00:07:05.000
off the shelf. A lot of it's even do-it-yourself.

00:07:05.000 --> 00:07:07.000
One of those things you just saw flashed

00:07:07.000 --> 00:07:09.000
before you was a raven drone, the handheld

00:07:09.000 --> 00:07:11.000
tossed one. For about a thousand dollars,

00:07:11.000 --> 00:07:13.000
you can build one yourself, equivalent to

00:07:13.000 --> 00:07:15.000
what the soldiers use in Iraq.

00:07:15.000 --> 00:07:17.000
That raises another wrinkle when it comes

00:07:17.000 --> 00:07:19.000
to war and conflict. Good guys might play

00:07:19.000 --> 00:07:21.000
around and work on these as hobby kits,

00:07:21.000 --> 00:07:23.000
but so might bad guys.

00:07:23.000 --> 00:07:25.000
This cross between robotics and things like

00:07:25.000 --> 00:07:27.000
terrorism is going to be fascinating

00:07:27.000 --> 00:07:29.000
and even disturbing,

00:07:29.000 --> 00:07:31.000
and we've already seen it start.

00:07:31.000 --> 00:07:33.000
During the war between Israel, a state,

00:07:33.000 --> 00:07:36.000
and Hezbollah, a non-state actor,

00:07:36.000 --> 00:07:38.000
the non-state actor flew

00:07:38.000 --> 00:07:40.000
four different drones against Israel.

00:07:40.000 --> 00:07:42.000
There's already a jihadi website

00:07:42.000 --> 00:07:44.000
that you can go on and remotely

00:07:44.000 --> 00:07:46.000
detonate an IED in Iraq while sitting

00:07:46.000 --> 00:07:48.000
at your home computer.

00:07:48.000 --> 00:07:50.000
And so I think what we're going to see is

00:07:50.000 --> 00:07:52.000
two trends take place with this.

00:07:52.000 --> 00:07:54.000
First is, you're going to reinforce the power

00:07:54.000 --> 00:07:58.000
of individuals against governments,

00:07:58.000 --> 00:08:00.000
but then the second is that

00:08:00.000 --> 00:08:02.000
we are going to see an expansion

00:08:02.000 --> 00:08:04.000
in the realm of terrorism.

00:08:04.000 --> 00:08:06.000
The future of it may be a cross between

00:08:06.000 --> 00:08:08.000
al Qaeda 2.0 and the

00:08:08.000 --> 00:08:10.000
next generation of the Unabomber.

00:08:10.000 --> 00:08:12.000
And another way of thinking about this

00:08:12.000 --> 00:08:14.000
is the fact that, remember, you don't have

00:08:14.000 --> 00:08:16.000
to convince a robot that they're gonna

00:08:16.000 --> 00:08:19.000
receive 72 virgins after they die

00:08:19.000 --> 00:08:22.000
to convince them to blow themselves up.

00:08:22.000 --> 00:08:24.000
But the ripple effects of this are going to go

00:08:24.000 --> 00:08:26.000
out into our politics. One of the people that

00:08:26.000 --> 00:08:28.000
I met with was a former Assistant Secretary of

00:08:28.000 --> 00:08:30.000
Defense for Ronald Reagan, and he put it

00:08:30.000 --> 00:08:32.000
this way: "I like these systems because

00:08:32.000 --> 00:08:34.000
they save American lives, but I worry about

00:08:34.000 --> 00:08:36.000
more marketization of wars,

00:08:36.000 --> 00:08:39.000
more shock-and-awe talk,

00:08:39.000 --> 00:08:41.000
to defray discussion of the costs.

00:08:41.000 --> 00:08:43.000
People are more likely to support the use

00:08:43.000 --> 00:08:46.000
of force if they view it as costless."

00:08:46.000 --> 00:08:48.000
Robots for me take certain trends

00:08:48.000 --> 00:08:51.000
that are already in play in our body politic,

00:08:51.000 --> 00:08:53.000
and maybe take them to

00:08:53.000 --> 00:08:55.000
their logical ending point.

00:08:55.000 --> 00:08:57.000
We don't have a draft. We don't

00:08:57.000 --> 00:09:00.000
have declarations of war anymore.

00:09:00.000 --> 00:09:02.000
We don't buy war bonds anymore.

00:09:02.000 --> 00:09:04.000
And now we have the fact that we're

00:09:04.000 --> 00:09:06.000
converting more and more of our American

00:09:06.000 --> 00:09:08.000
soldiers that we would send into harm's

00:09:08.000 --> 00:09:11.000
way into machines, and so we may take

00:09:11.000 --> 00:09:14.000
those already lowering bars to war

00:09:14.000 --> 00:09:17.000
and drop them to the ground.

00:09:17.000 --> 00:09:19.000
But the future of war is also going to be

00:09:19.000 --> 00:09:21.000
a YouTube war.

00:09:21.000 --> 00:09:23.000
That is, our new technologies don't merely

00:09:23.000 --> 00:09:25.000
remove humans from risk.

00:09:25.000 --> 00:09:28.000
They also record everything that they see.

00:09:28.000 --> 00:09:31.000
So they don't just delink the public:

00:09:31.000 --> 00:09:34.000
they reshape its relationship with war.

00:09:34.000 --> 00:09:36.000
There's already several thousand

00:09:36.000 --> 00:09:38.000
video clips of combat footage from Iraq

00:09:38.000 --> 00:09:40.000
on YouTube right now,

00:09:40.000 --> 00:09:42.000
most of it gathered by drones.

00:09:42.000 --> 00:09:44.000
Now, this could be a good thing.

00:09:44.000 --> 00:09:46.000
It could be building connections between

00:09:46.000 --> 00:09:48.000
the home front and the war front

00:09:48.000 --> 00:09:50.000
as never before.

00:09:50.000 --> 00:09:52.000
But remember, this is taking place

00:09:52.000 --> 00:09:55.000
in our strange, weird world, and so

00:09:55.000 --> 00:09:57.000
inevitably the ability to download these

00:09:57.000 --> 00:09:59.000
video clips to, you know, your iPod

00:09:59.000 --> 00:10:02.000
or your Zune gives you

00:10:02.000 --> 00:10:06.000
the ability to turn it into entertainment.

00:10:06.000 --> 00:10:08.000
Soldiers have a name for these clips.

00:10:08.000 --> 00:10:10.000
They call it war porn.

00:10:10.000 --> 00:10:12.000
The typical one that I was sent was

00:10:12.000 --> 00:10:14.000
an email that had an attachment of

00:10:14.000 --> 00:10:16.000
video of a Predator strike taking out

00:10:16.000 --> 00:10:18.000
an enemy site. Missile hits,

00:10:18.000 --> 00:10:21.000
bodies burst into the air with the explosion.

00:10:21.000 --> 00:10:23.000
It was set to music.

00:10:23.000 --> 00:10:25.000
It was set to the pop song

00:10:25.000 --> 00:10:28.000
"I Just Want To Fly" by Sugar Ray.

00:10:28.000 --> 00:10:31.000
This ability to watch more

00:10:31.000 --> 00:10:34.000
but experience less creates a wrinkle

00:10:34.000 --> 00:10:36.000
in the public's relationship with war.

00:10:36.000 --> 00:10:38.000
I think about this with a sports parallel.

00:10:38.000 --> 00:10:41.000
It's like the difference between

00:10:41.000 --> 00:10:44.000
watching an NBA game, a professional

00:10:44.000 --> 00:10:47.000
basketball game on TV, where the athletes

00:10:47.000 --> 00:10:49.000
are tiny figures on the screen, and

00:10:49.000 --> 00:10:52.000
being at that basketball game in person

00:10:52.000 --> 00:10:54.000
and realizing what someone seven feet

00:10:54.000 --> 00:10:56.000
really does look like.

00:10:56.000 --> 00:10:58.000
But we have to remember,

00:10:58.000 --> 00:11:00.000
these are just the clips.

00:11:00.000 --> 00:11:02.000
These are just the ESPN SportsCenter

00:11:02.000 --> 00:11:04.000
version of the game. They lose the context.

00:11:04.000 --> 00:11:06.000
They lose the strategy.

00:11:06.000 --> 00:11:08.000
They lose the humanity. War just

00:11:08.000 --> 00:11:11.000
becomes slam dunks and smart bombs.

00:11:11.000 --> 00:11:14.000
Now the irony of all this is that

00:11:14.000 --> 00:11:16.000
while the future of war may involve

00:11:16.000 --> 00:11:18.000
more and more machines,

00:11:18.000 --> 00:11:20.000
it's our human psychology that's driving

00:11:20.000 --> 00:11:22.000
all of this, it's our human failings

00:11:22.000 --> 00:11:24.000
that are leading to these wars.

00:11:24.000 --> 00:11:26.000
So one example of this that has

00:11:26.000 --> 00:11:28.000
big resonance in the policy realm is

00:11:28.000 --> 00:11:30.000
how this plays out on our very real

00:11:30.000 --> 00:11:32.000
war of ideas that we're fighting

00:11:32.000 --> 00:11:34.000
against radical groups.

00:11:34.000 --> 00:11:36.000
What is the message that we think we are

00:11:36.000 --> 00:11:38.000
sending with these machines versus what

00:11:38.000 --> 00:11:41.000
is being received in terms of the message.

00:11:41.000 --> 00:11:43.000
So one of the people that I met was

00:11:43.000 --> 00:11:45.000
a senior Bush Administration official,

00:11:45.000 --> 00:11:47.000
who had this to say about

00:11:47.000 --> 00:11:49.000
our unmanning of war:

00:11:49.000 --> 00:11:51.000
"It plays to our strength. The thing that

00:11:51.000 --> 00:11:53.000
scares people is our technology."

00:11:53.000 --> 00:11:55.000
But when you go out and meet with people,

00:11:55.000 --> 00:11:57.000
for example in Lebanon, it's a very

00:11:57.000 --> 00:11:59.000
different story. One of the people

00:11:59.000 --> 00:12:01.000
I met with there was a news editor, and

00:12:01.000 --> 00:12:03.000
we're talking as a drone is flying above him,

00:12:03.000 --> 00:12:05.002
and this is what he had to say.

00:12:05.002 --> 00:12:07.000
"This is just another sign of the coldhearted

00:12:07.000 --> 00:12:10.006
cruel Israelis and Americans,

00:12:10.006 --> 00:12:11.998
who are cowards because

00:12:11.998 --> 00:12:14.004
they send out machines to fight us.

00:12:14.004 --> 00:12:15.996
They don't want to fight us like real men,

00:12:15.996 --> 00:12:18.003
but they're afraid to fight,

00:12:18.003 --> 00:12:19.994
so we just have to kill a few of their soldiers

00:12:19.994 --> 00:12:22.999
to defeat them."

00:12:22.999 --> 00:12:25.006
The future of war also is featuring

00:12:25.006 --> 00:12:26.983
a new type of warrior,

00:12:26.999 --> 00:12:30.005
and it's actually redefining the experience

00:12:30.005 --> 00:12:32.003
of going to war.

00:12:32.003 --> 00:12:33.979
You can call this a cubicle warrior.

00:12:33.994 --> 00:12:36.001
This is what one Predator drone pilot

00:12:36.001 --> 00:12:38.008
described of his experience fighting

00:12:38.008 --> 00:12:40.997
in the Iraq War while never leaving Nevada.

00:12:40.997 --> 00:12:43.002
"You're going to war for 12 hours,

00:12:43.002 --> 00:12:44.993
shooting weapons at targets,

00:12:45.008 --> 00:12:47.999
directing kills on enemy combatants,

00:12:47.999 --> 00:12:50.005
and then you get in the car

00:12:50.005 --> 00:12:51.987
and you drive home and within 20 minutes,

00:12:52.003 --> 00:12:53.994
you're sitting at the dinner table

00:12:54.009 --> 00:12:56.002
talking to your kids about their homework."

00:12:56.002 --> 00:12:57.993
Now, the psychological balancing

00:12:58.008 --> 00:13:00.001
of those experiences is incredibly tough,

00:13:00.001 --> 00:13:02.993
and in fact those drone pilots have

00:13:02.993 --> 00:13:05.001
higher rates of PTSD than many

00:13:05.001 --> 00:13:08.006
of the units physically in Iraq.

00:13:08.006 --> 00:13:09.998
But some have worries that this

00:13:09.998 --> 00:13:11.992
disconnection will lead to something else,

00:13:12.007 --> 00:13:14.005
that it might make the contemplation of war

00:13:14.005 --> 00:13:16.010
crimes a lot easier when you have

00:13:16.010 --> 00:13:18.009
this distance. "It's like a video game,"

00:13:18.009 --> 00:13:20.008
is what one young pilot described to me

00:13:20.008 --> 00:13:21.968
of taking out enemy troops from afar.

00:13:21.998 --> 00:13:24.993
As anyone who's played Grand Theft Auto

00:13:25.008 --> 00:13:28.008
knows, we do things in the video world

00:13:28.008 --> 00:13:31.004
that we wouldn't do face to face.

00:13:31.004 --> 00:13:33.002
So much of what you're hearing from me

00:13:33.002 --> 00:13:34.995
is that there's another side

00:13:34.995 --> 00:13:37.003
to technologic revolutions,

00:13:37.003 --> 00:13:38.997
and that it's shaping our present

00:13:38.997 --> 00:13:41.998
and maybe will shape our future of war.

00:13:41.998 --> 00:13:43.999
Moore's Law is operative,

00:13:43.999 --> 00:13:45.997
but so's Murphy's Law.

00:13:45.997 --> 00:13:48.002
The fog of war isn't being lifted.

00:13:48.002 --> 00:13:50.012
The enemy has a vote.

00:13:50.012 --> 00:13:52.001
We're gaining incredible new capabilities,

00:13:52.001 --> 00:13:54.007
but we're also seeing and experiencing

00:13:54.007 --> 00:13:56.007
new human dilemmas. Now,

00:13:56.007 --> 00:13:58.006
sometimes these are just "oops" moments,

00:13:58.006 --> 00:13:59.998
which is what the head of a robotics

00:13:59.998 --> 00:14:02.009
company described it, you just have

00:14:02.009 --> 00:14:04.009
"oops" moments. Well, what are

00:14:04.009 --> 00:14:06.005
"oops" moments with robots in war?

00:14:06.005 --> 00:14:08.006
Well, sometimes they're funny. Sometimes,

00:14:08.006 --> 00:14:10.008
they're like that scene from the

00:14:10.008 --> 00:14:12.009
Eddie Murphy movie "Best Defense,"

00:14:12.009 --> 00:14:14.010
playing out in reality, where they tested out

00:14:14.010 --> 00:14:15.998
a machine gun-armed robot, and during

00:14:15.998 --> 00:14:17.999
the demonstration it started spinning

00:14:17.999 --> 00:14:21.008
in a circle and pointed its machine gun

00:14:21.008 --> 00:14:24.003
at the reviewing stand of VIPs.

00:14:24.003 --> 00:14:25.996
Fortunately the weapon wasn't loaded

00:14:25.996 --> 00:14:28.006
and no one was hurt, but other times

00:14:28.006 --> 00:14:29.998
"oops" moments are tragic,

00:14:29.998 --> 00:14:32.000
such as last year in South Africa, where

00:14:32.000 --> 00:14:34.999
an anti-aircraft cannon had a

00:14:34.999 --> 00:14:37.998
"software glitch," and actually did turn on

00:14:37.998 --> 00:14:41.002
and fired, and nine soldiers were killed.

00:14:41.002 --> 00:14:43.999
We have new wrinkles in the laws of war

00:14:43.999 --> 00:14:45.996
and accountability. What do we do

00:14:45.996 --> 00:14:48.006
with things like unmanned slaughter?

00:14:48.006 --> 00:14:50.004
What is unmanned slaughter?

00:14:50.004 --> 00:14:52.004
We've already had three instances of

00:14:52.004 --> 00:14:54.004
Predator drone strikes where we thought

00:14:54.004 --> 00:14:55.998
we got bin Laden, and it turned out

00:14:55.998 --> 00:14:58.000
not to be the case.

00:14:58.000 --> 00:15:00.008
And this is where we're at right now.

00:15:00.008 --> 00:15:02.008
This is not even talking about armed,

00:15:02.008 --> 00:15:03.997
autonomous systems

00:15:03.997 --> 00:15:06.010
with full authority to use force.

00:15:06.010 --> 00:15:08.004
And do not believe that that isn't coming.

00:15:08.004 --> 00:15:10.011
During my research I came across

00:15:10.011 --> 00:15:11.996
four different Pentagon projects

00:15:11.996 --> 00:15:14.008
on different aspects of that.

00:15:14.008 --> 00:15:16.010
And so you have this question:

00:15:16.010 --> 00:15:18.003
what does this lead to issues like

00:15:18.003 --> 00:15:19.995
war crimes? Robots are emotionless, so

00:15:19.995 --> 00:15:23.007
they don't get upset if their buddy is killed.

00:15:23.007 --> 00:15:25.001
They don't commit crimes of rage

00:15:25.001 --> 00:15:27.008
and revenge.

00:15:27.008 --> 00:15:30.005
But robots are emotionless.

00:15:30.005 --> 00:15:32.004
They see an 80-year-old grandmother

00:15:32.004 --> 00:15:34.004
in a wheelchair the same way they see

00:15:34.004 --> 00:15:37.006
a T-80 tank: they're both

00:15:37.006 --> 00:15:39.999
just a series of zeroes and ones.

00:15:39.999 --> 00:15:43.002
And so we have this question to figure out:

00:15:43.002 --> 00:15:45.005
How do we catch up our 20th century

00:15:45.005 --> 00:15:46.994
laws of war, that are so old right now

00:15:46.994 --> 00:15:50.002
that they could qualify for Medicare,

00:15:50.002 --> 00:15:53.010
to these 21st century technologies?

00:15:53.010 --> 00:15:55.997
And so, in conclusion, I've talked about

00:15:55.997 --> 00:15:59.008
what seems the future of war,

00:15:59.008 --> 00:16:00.998
but notice that I've only used

00:16:00.998 --> 00:16:03.010
real world examples and you've only seen

00:16:03.010 --> 00:16:05.011
real world pictures and videos.

00:16:05.011 --> 00:16:07.011
And so this sets a great challenge for

00:16:07.011 --> 00:16:08.997
all of us that we have to worry about well

00:16:08.997 --> 00:16:11.006
before you have to worry about your

00:16:11.006 --> 00:16:13.007
Roomba sucking the life away from you.

00:16:13.007 --> 00:16:15.006
Are we going to let the fact that what's

00:16:15.006 --> 00:16:18.006
unveiling itself right now in war

00:16:18.006 --> 00:16:20.997
sounds like science fiction and therefore

00:16:20.997 --> 00:16:23.002
keeps us in denial?

00:16:23.002 --> 00:16:25.006
Are we going to face the reality

00:16:25.006 --> 00:16:26.999
of 21st century war?

00:16:26.999 --> 00:16:29.000
Is our generation going to make the same

00:16:29.000 --> 00:16:31.009
mistake that a past generation did

00:16:31.009 --> 00:16:33.007
with atomic weaponry, and not deal with

00:16:33.007 --> 00:16:35.002
the issues that surround it until

00:16:35.002 --> 00:16:37.009
Pandora's box is already opened up?

00:16:37.009 --> 00:16:39.007
Now, I could be wrong on this, and

00:16:39.007 --> 00:16:41.011
one Pentagon robot scientist told me

00:16:41.011 --> 00:16:43.005
that I was. He said, "There's no real

00:16:43.005 --> 00:16:45.000
social, ethical, moral issues when it comes

00:16:45.000 --> 00:16:46.998
to robots.

00:16:46.998 --> 00:16:48.999
That is," he added, "unless the machine

00:16:48.999 --> 00:16:52.009
kills the wrong people repeatedly.

00:16:52.009 --> 00:16:54.998
Then it's just a product recall issue."

00:16:54.998 --> 00:16:58.009
And so the ending point for this is

00:16:58.009 --> 00:17:03.011
that actually, we can turn to Hollywood.

00:17:03.011 --> 00:17:05.007
A few years ago, Hollywood gathered

00:17:05.007 --> 00:17:08.013
all the top characters and created

00:17:08.013 --> 00:17:10.010
a list of the top 100 heroes and

00:17:10.010 --> 00:17:13.000
top 100 villains of all of Hollywood history,

00:17:13.000 --> 00:17:15.006
the characters that represented the best

00:17:15.006 --> 00:17:16.998
and worst of humanity.

00:17:16.998 --> 00:17:21.013
Only one character made it onto both lists:

00:17:21.013 --> 00:17:24.009
The Terminator, a robot killing machine.

00:17:24.009 --> 00:17:25.997
And so that points to the fact that

00:17:25.997 --> 00:17:28.010
our machines can be used

00:17:28.010 --> 00:17:30.009
for both good and evil, but for me

00:17:30.009 --> 00:17:32.005
it points to the fact that there's a duality

00:17:32.005 --> 00:17:35.005
of humans as well.

00:17:35.005 --> 00:17:37.003
This week is a celebration

00:17:37.003 --> 00:17:39.002
of our creativity. Our creativity

00:17:39.002 --> 00:17:41.000
has taken our species to the stars.

00:17:41.000 --> 00:17:43.007
Our creativity has created works of arts

00:17:43.007 --> 00:17:46.003
and literature to express our love.

00:17:46.003 --> 00:17:48.001
And now, we're using our creativity

00:17:48.001 --> 00:17:49.994
in a certain direction, to build fantastic

00:17:50.009 --> 00:17:52.996
machines with incredible capabilities,

00:17:52.996 --> 00:17:55.005
maybe even one day

00:17:55.005 --> 00:17:58.002
an entirely new species.

00:17:58.002 --> 00:18:00.007
But one of the main reasons that we're

00:18:00.007 --> 00:18:02.005
doing that is because of our drive

00:18:02.005 --> 00:18:05.005
to destroy each other, and so the question

00:18:05.005 --> 00:18:06.999
we all should ask:

00:18:06.999 --> 00:18:08.999
is it our machines, or is it us

00:18:08.999 --> 00:18:11.006
that's wired for war?

00:18:11.006 --> 00:18:13.104
Thank you. (Applause)

