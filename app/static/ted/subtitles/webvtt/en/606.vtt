WEBVTT

00:00:00.000 --> 00:00:03.000
Hello! My name is Golan Levin.

00:00:03.000 --> 00:00:05.000
I'm an artist and an engineer,

00:00:05.000 --> 00:00:07.000
which is, increasingly, a more common kind of hybrid.

00:00:07.000 --> 00:00:10.000
But I still fall into this weird crack

00:00:10.000 --> 00:00:12.000
where people don't seem to understand me.

00:00:12.000 --> 00:00:16.000
And I was looking around and I found this wonderful picture.

00:00:16.000 --> 00:00:19.000
It's a letter from "Artforum" in 1967

00:00:19.000 --> 00:00:22.000
saying "We can't imagine ever doing a special issue

00:00:22.000 --> 00:00:25.000
on electronics or computers in art." And they still haven't.

00:00:25.000 --> 00:00:30.000
And lest you think that you all, as the digerati, are more enlightened,

00:00:30.000 --> 00:00:33.000
I went to the Apple iPhone app store the other day.

00:00:33.000 --> 00:00:37.000
Where's art? I got productivity. I got sports.

00:00:37.000 --> 00:00:41.000
And somehow the idea that one would want to make art for the iPhone,

00:00:41.000 --> 00:00:43.000
which my friends and I are doing now,

00:00:43.000 --> 00:00:46.000
is still not reflected in our understanding

00:00:46.000 --> 00:00:48.000
of what computers are for.

00:00:48.000 --> 00:00:50.000
So, from both directions, there is kind of, I think, a lack of understanding

00:00:50.000 --> 00:00:52.000
about what it could mean to be an artist who uses the materials

00:00:52.000 --> 00:00:54.000
of his own day, or her own day,

00:00:54.000 --> 00:00:56.000
which I think artists are obliged to do,

00:00:56.000 --> 00:01:00.000
is to really explore the expressive potential of the new tools that we have.

00:01:00.000 --> 00:01:02.000
In my own case, I'm an artist,

00:01:02.000 --> 00:01:04.000
and I'm really interested in

00:01:04.000 --> 00:01:06.000
expanding the vocabulary of human action,

00:01:06.000 --> 00:01:09.000
and basically empowering people through interactivity.

00:01:09.000 --> 00:01:12.000
I want people to discover themselves as actors,

00:01:12.000 --> 00:01:16.000
as creative actors, by having interactive experiences.

00:01:16.000 --> 00:01:19.000
A lot of my work is about trying to get away from this.

00:01:19.000 --> 00:01:21.000
This a photograph of the desktop of a student of mine.

00:01:21.000 --> 00:01:23.000
And when I say desktop, I don't just mean

00:01:23.000 --> 00:01:26.000
the actual desk where his mouse has worn away the surface of the desk.

00:01:26.000 --> 00:01:28.000
If you look carefully, you can even see

00:01:28.000 --> 00:01:31.000
a hint of the Apple menu, up here in the upper left,

00:01:31.000 --> 00:01:33.000
where the virtual world has literally

00:01:33.000 --> 00:01:35.000
punched through to the physical.

00:01:35.000 --> 00:01:39.000
So this is, as Joy Mountford once said,

00:01:39.000 --> 00:01:41.000
"The mouse is probably the narrowest straw

00:01:41.000 --> 00:01:43.000
you could try to suck all of human expression through."

00:01:43.000 --> 00:01:46.000
(Laughter)

00:01:46.000 --> 00:01:49.000
And the thing I'm really trying to do is enabling people to have more rich

00:01:49.000 --> 00:01:51.000
kinds of interactive experiences.

00:01:51.000 --> 00:01:53.000
How can we get away from the mouse and use our full bodies

00:01:53.000 --> 00:01:56.000
as a way of exploring aesthetic experiences,

00:01:56.000 --> 00:01:58.000
not necessarily utilitarian ones.

00:01:58.000 --> 00:02:01.000
So I write software. And that's how I do it.

00:02:01.000 --> 00:02:03.000
And a lot of my experiences

00:02:03.000 --> 00:02:05.000
resemble mirrors in some way.

00:02:05.000 --> 00:02:07.000
Because this is, in some sense, the first way,

00:02:07.000 --> 00:02:09.000
that people discover their own potential as actors,

00:02:09.000 --> 00:02:11.000
and discover their own agency.

00:02:11.000 --> 00:02:14.000
By saying "Who is that person in the mirror? Oh it's actually me."

00:02:14.000 --> 00:02:16.000
And so, to give an example,

00:02:16.000 --> 00:02:18.000
this is a project from last year,

00:02:18.000 --> 00:02:20.000
which is called the Interstitial Fragment Processor.

00:02:20.000 --> 00:02:24.000
And it allows people to explore the negative shapes that they create

00:02:24.000 --> 00:02:27.000
when they're just going about their everyday business.

00:02:41.000 --> 00:02:43.000
So as people make shapes with their hands or their heads

00:02:43.000 --> 00:02:45.000
and so forth, or with each other,

00:02:45.000 --> 00:02:48.000
these shapes literally produce sounds and drop out of thin air --

00:02:48.000 --> 00:02:52.000
basically taking what's often this, kind of, unseen space,

00:02:52.000 --> 00:02:55.000
or this undetected space, and making it something real,

00:02:55.000 --> 00:02:58.000
that people then can appreciate and become creative with.

00:02:58.000 --> 00:03:01.000
So again, people discover their creative agency in this way.

00:03:01.000 --> 00:03:03.000
And their own personalities come out

00:03:03.000 --> 00:03:06.000
in totally unique ways.

00:03:06.000 --> 00:03:09.000
So in addition to using full-body input,

00:03:09.000 --> 00:03:11.000
something that I've explored now, for a while,

00:03:11.000 --> 00:03:13.000
has been the use of the voice,

00:03:13.000 --> 00:03:17.000
which is an immensely expressive system for us, vocalizing.

00:03:17.000 --> 00:03:19.000
Song is one of our oldest ways

00:03:19.000 --> 00:03:22.000
of making ourselves heard and understood.

00:03:22.000 --> 00:03:24.000
And I came across this fantastic research by Wolfgang KÃ¶hler,

00:03:24.000 --> 00:03:28.000
the so-called father of gestalt psychology, from 1927,

00:03:28.000 --> 00:03:30.000
who submitted to an audience like yourselves

00:03:30.000 --> 00:03:32.000
the following two shapes.

00:03:32.000 --> 00:03:34.000
And he said one of them is called Maluma.

00:03:34.000 --> 00:03:36.000
And one of them is called Taketa. Which is which?

00:03:36.000 --> 00:03:40.000
Anyone want to hazard a guess?

00:03:40.000 --> 00:03:42.000
Maluma is on top. Yeah. So.

00:03:42.000 --> 00:03:45.000
As he says here, most people answer without any hesitation.

00:03:45.000 --> 00:03:47.000
So what we're really seeing here is a phenomenon

00:03:47.000 --> 00:03:49.000
called phonaesthesia,

00:03:49.000 --> 00:03:51.000
which is a kind of synesthesia that all of you have.

00:03:51.000 --> 00:03:53.000
And so, whereas Dr. Oliver Sacks has talked about

00:03:53.000 --> 00:03:55.000
how perhaps one person in a million

00:03:55.000 --> 00:03:57.000
actually has true synesthesia,

00:03:57.000 --> 00:03:59.000
where they hear colors or taste shapes, and things like this,

00:03:59.000 --> 00:04:01.000
phonaesthesia is something we can all experience to some extent.

00:04:01.000 --> 00:04:04.000
It's about mappings between different perceptual domains,

00:04:04.000 --> 00:04:07.000
like hardness, sharpness, brightness and darkness,

00:04:07.000 --> 00:04:09.000
and the phonemes that we're able to speak with.

00:04:09.000 --> 00:04:11.000
So 70 years on, there's been some research where

00:04:11.000 --> 00:04:13.000
cognitive psychologists have actually sussed out

00:04:13.000 --> 00:04:15.000
the extent to which, you know,

00:04:15.000 --> 00:04:19.000
L, M and B are more associated with shapes that look like this,

00:04:19.000 --> 00:04:23.000
and P, T and K are perhaps more associated with shapes like this.

00:04:23.000 --> 00:04:25.000
And here we suddenly begin to have a mapping between curvature

00:04:25.000 --> 00:04:27.000
that we can exploit numerically,

00:04:27.000 --> 00:04:30.000
a relative mapping between curvature and shape.

00:04:30.000 --> 00:04:33.000
So it occurred to me, what happens if we could run these backwards?

00:04:33.000 --> 00:04:35.000
And thus was born the project called Remark,

00:04:35.000 --> 00:04:37.000
which is a collaboration with Zachary Lieberman

00:04:37.000 --> 00:04:39.000
and the Ars Electronica Futurelab.

00:04:39.000 --> 00:04:41.000
And this is an interactive installation which presents

00:04:41.000 --> 00:04:43.000
the fiction that speech casts visible shadows.

00:04:43.000 --> 00:04:46.000
So the idea is you step into a kind of a magic light.

00:04:46.000 --> 00:04:49.000
And as you do, you see the shadows of your own speech.

00:04:49.000 --> 00:04:51.000
And they sort of fly away, out of your head.

00:04:51.000 --> 00:04:54.000
If a computer speech recognition system

00:04:54.000 --> 00:04:58.000
is able to recognize what you're saying, then it spells it out.

00:04:58.000 --> 00:05:00.000
And if it isn't then it produces a shape which is very phonaesthetically

00:05:00.000 --> 00:05:02.000
tightly coupled to the sounds you made.

00:05:02.000 --> 00:05:05.000
So let's bring up a video of that.

00:05:51.000 --> 00:05:53.000
(Applause)

00:05:53.000 --> 00:05:56.000
Thanks. So. And this project here,

00:05:56.000 --> 00:05:59.000
I was working with the great abstract vocalist, Jaap Blonk.

00:05:59.000 --> 00:06:02.000
And he is a world expert in performing "The Ursonate,"

00:06:02.000 --> 00:06:04.000
which is a half-an-hour nonsense poem

00:06:04.000 --> 00:06:06.000
by Kurt Schwitters, written in the 1920s,

00:06:06.000 --> 00:06:10.000
which is half an hour of very highly patterned nonsense.

00:06:10.000 --> 00:06:12.000
And it's almost impossible to perform.

00:06:12.000 --> 00:06:15.000
But Jaap is one of the world experts in performing it.

00:06:15.000 --> 00:06:17.000
And in this project we've developed

00:06:17.000 --> 00:06:20.000
a form of intelligent real-time subtitles.

00:06:20.000 --> 00:06:23.000
So these are our live subtitles,

00:06:23.000 --> 00:06:26.000
that are being produced by a computer that knows the text of "The Ursonate" --

00:06:26.000 --> 00:06:29.000
fortunately Jaap does too, very well --

00:06:29.000 --> 00:06:34.000
and it is delivering that text at the same time as Jaap is.

00:06:41.000 --> 00:06:43.000
So all the text you're going to see

00:06:43.000 --> 00:06:45.000
is real-time generated by the computer,

00:06:45.000 --> 00:06:48.000
visualizing what he's doing with his voice.

00:07:58.000 --> 00:08:01.000
Here you can see the set-up where there is a screen with the subtitles behind him.

00:08:22.000 --> 00:08:24.000
Okay. So ...

00:08:24.000 --> 00:08:29.000
(Applause)

00:08:29.000 --> 00:08:31.000
The full videos are online if you are interested.

00:08:31.000 --> 00:08:33.000
I got a split reaction to that during the live performance,

00:08:33.000 --> 00:08:35.000
because there is some people who understand

00:08:35.000 --> 00:08:37.000
live subtitles are a kind of an oxymoron,

00:08:37.000 --> 00:08:40.000
because usually there is someone making them afterwards.

00:08:40.000 --> 00:08:43.000
And then a bunch of people who were like, "What's the big deal?

00:08:43.000 --> 00:08:45.000
I see subtitles all the time on television."

00:08:45.000 --> 00:08:48.000
You know? They don't imagine the person in the booth, typing it all.

00:08:48.000 --> 00:08:51.000
So in addition to the full body, and in addition to the voice,

00:08:51.000 --> 00:08:53.000
another thing that I've been really interested in,

00:08:53.000 --> 00:08:55.000
most recently, is the use of the eyes,

00:08:55.000 --> 00:08:59.000
or the gaze, in terms of how people relate to each other.

00:08:59.000 --> 00:09:01.000
It's a really profound amount of nonverbal information

00:09:01.000 --> 00:09:03.000
that's communicated with the eyes.

00:09:03.000 --> 00:09:05.000
And it's one of the most interesting technical challenges

00:09:05.000 --> 00:09:07.000
that's very currently active in the computer sciences:

00:09:07.000 --> 00:09:09.000
being able to have a camera that can understand,

00:09:09.000 --> 00:09:11.000
from a fairly big distance away,

00:09:11.000 --> 00:09:14.000
how these little tiny balls are actually pointing in one way or another

00:09:14.000 --> 00:09:16.000
to reveal what you're interested in,

00:09:16.000 --> 00:09:18.000
and where your attention is directed.

00:09:18.000 --> 00:09:21.000
So there is a lot of emotional communication that happens there.

00:09:21.000 --> 00:09:25.000
And so I've been beginning, with a variety of different projects,

00:09:25.000 --> 00:09:28.000
to understand how people can relate to machines with their eyes.

00:09:28.000 --> 00:09:31.000
And basically to ask the questions:

00:09:31.000 --> 00:09:36.000
What if art was aware that we were looking at it?

00:09:36.000 --> 00:09:38.000
How could it respond, in a way,

00:09:38.000 --> 00:09:41.000
to acknowledge or subvert the fact that we're looking at it?

00:09:41.000 --> 00:09:44.000
And what could it do if it could look back at us?

00:09:44.000 --> 00:09:46.000
And so those are the questions that are happening in the next projects.

00:09:46.000 --> 00:09:49.000
In the first one which I'm going to show you, called Eyecode,

00:09:49.000 --> 00:09:51.000
it's a piece of interactive software

00:09:51.000 --> 00:09:53.000
in which, if we read this little circle,

00:09:53.000 --> 00:09:56.000
"the trace left by the looking of the previous observer

00:09:56.000 --> 00:09:59.000
looks at the trace left by the looking of previous observer."

00:09:59.000 --> 00:10:01.000
The idea is that it's an image wholly constructed

00:10:01.000 --> 00:10:03.000
from its own history of being viewed

00:10:03.000 --> 00:10:05.000
by different people in an installation.

00:10:05.000 --> 00:10:10.000
So let me just switch over so we can do the live demo.

00:10:10.000 --> 00:10:14.000
So let's run this and see if it works.

00:10:14.000 --> 00:10:17.000
Okay. Ah, there is lots of nice bright video.

00:10:17.000 --> 00:10:19.000
There is just a little test screen that shows that it's working.

00:10:19.000 --> 00:10:21.000
And what I'm just going to do is -- I'm going to hide that.

00:10:21.000 --> 00:10:23.000
And you can see here that what it's doing

00:10:23.000 --> 00:10:26.000
is it's recording my eyes every time I blink.

00:10:32.000 --> 00:10:36.000
Hello? And I can ... hello ... okay.

00:10:36.000 --> 00:10:38.000
And no matter where I am, what's really going on here

00:10:38.000 --> 00:10:41.000
is that it's an eye-tracking system that tries to locate my eyes.

00:10:41.000 --> 00:10:43.000
And if I get really far away I'm blurry.

00:10:43.000 --> 00:10:45.000
You know, you're going to have these kind of blurry spots like this

00:10:45.000 --> 00:10:48.000
that maybe only resemble eyes in a very very abstract way.

00:10:48.000 --> 00:10:51.000
But if I come up really close and stare directly at the camera

00:10:51.000 --> 00:10:53.000
on this laptop then you'll see these nice crisp eyes.

00:10:53.000 --> 00:10:57.000
You can think of it as a way of, sort of, typing, with your eyes.

00:10:57.000 --> 00:10:59.000
And what you're typing are recordings of your eyes

00:10:59.000 --> 00:11:01.000
as you're looking at other peoples' eyes.

00:11:01.000 --> 00:11:04.000
So each person is looking at the looking

00:11:04.000 --> 00:11:06.000
of everyone else before them.

00:11:06.000 --> 00:11:08.000
And this exists in larger installations

00:11:08.000 --> 00:11:10.000
where there are thousands and thousands of eyes

00:11:10.000 --> 00:11:12.000
that people could be staring at,

00:11:12.000 --> 00:11:14.000
as you see who's looking at the people looking

00:11:14.000 --> 00:11:16.000
at the people looking before them.

00:11:16.000 --> 00:11:19.000
So I'll just add a couple more. Blink. Blink.

00:11:19.000 --> 00:11:22.000
And you can see, just once again, how it's sort of finding my eyes

00:11:22.000 --> 00:11:25.000
and doing its best to estimate when it's blinking.

00:11:25.000 --> 00:11:27.000
Alright. Let's leave that.

00:11:27.000 --> 00:11:30.000
So that's this kind of recursive observation system.

00:11:30.000 --> 00:11:32.000
(Applause)

00:11:32.000 --> 00:11:34.000
Thank you.

00:11:34.000 --> 00:11:36.000
The last couple pieces I'm going to show

00:11:36.000 --> 00:11:38.000
are basically in the new realm of robotics -- for me, new for me.

00:11:38.000 --> 00:11:40.000
It's called Opto-Isolator.

00:11:40.000 --> 00:11:43.000
And I'm going to show a video of the older version of it,

00:11:43.000 --> 00:11:45.000
which is just a minute long. Okay.

00:11:54.000 --> 00:11:56.000
In this case, the Opto-Isolator is blinking

00:11:56.000 --> 00:11:58.000
in response to one's own blinks.

00:11:58.000 --> 00:12:01.000
So it blinks one second after you do.

00:12:01.000 --> 00:12:04.000
This is a device which is intended to reduce

00:12:04.000 --> 00:12:07.000
the phenomenon of gaze down to the simplest possible materials.

00:12:07.000 --> 00:12:09.000
Just one eye,

00:12:09.000 --> 00:12:11.000
looking at you, and eliminating everything else about a face,

00:12:11.000 --> 00:12:14.000
but just to consider gaze in an isolated way

00:12:14.000 --> 00:12:17.000
as a kind of, as an element.

00:12:17.000 --> 00:12:20.000
And at the same time, it attempts to engage in what you might call

00:12:20.000 --> 00:12:22.000
familiar psycho-social gaze behaviors.

00:12:22.000 --> 00:12:24.000
Like looking away if you look at it too long

00:12:24.000 --> 00:12:26.000
because it gets shy,

00:12:26.000 --> 00:12:29.000
or things like that.

00:12:29.000 --> 00:12:32.000
Okay. So the last project I'm going to show

00:12:32.000 --> 00:12:35.000
is this new one called Snout.

00:12:35.000 --> 00:12:37.000
(Laughter)

00:12:37.000 --> 00:12:39.000
It's an eight-foot snout,

00:12:39.000 --> 00:12:41.000
with a googly eye.

00:12:41.000 --> 00:12:42.000
(Laughter)

00:12:42.000 --> 00:12:45.000
And inside it's got an 800-pound robot arm

00:12:45.000 --> 00:12:47.000
that I borrowed,

00:12:47.000 --> 00:12:48.000
(Laughter)

00:12:48.000 --> 00:12:50.000
from a friend.

00:12:50.000 --> 00:12:51.000
(Laughter)

00:12:51.000 --> 00:12:53.000
It helps to have good friends.

00:12:53.000 --> 00:12:56.000
I'm at Carnegie Mellon; we've got a great Robotics Institute there.

00:12:56.000 --> 00:12:58.000
I'd like to show you thing called Snout, which is --

00:12:58.000 --> 00:13:00.000
The idea behind this project is to

00:13:00.000 --> 00:13:04.000
make a robot that appears as if it's continually surprised to see you.

00:13:04.000 --> 00:13:08.000
(Laughter)

00:13:08.000 --> 00:13:10.000
The idea is that basically --

00:13:10.000 --> 00:13:12.000
if it's constantly like "Huh? ... Huh?"

00:13:12.000 --> 00:13:16.000
That's why its other name is Doubletaker, Taker of Doubles.

00:13:16.000 --> 00:13:18.000
It's always kind of doing a double take: "What?"

00:13:18.000 --> 00:13:20.000
And the idea is basically, can it look at you

00:13:20.000 --> 00:13:22.000
and make you feel as if like,

00:13:22.000 --> 00:13:24.000
"What? Is it my shoes?"

00:13:24.000 --> 00:13:27.000
"Got something on my hair?" Here we go. Alright.

00:13:58.000 --> 00:14:00.000
Checking him out ...

00:14:08.000 --> 00:14:10.000
For you nerds, here's a little behind-the-scenes.

00:14:10.000 --> 00:14:12.000
It's got a computer vision system,

00:14:12.000 --> 00:14:15.000
and it tries to look at the people who are moving around the most.

00:14:27.000 --> 00:14:29.000
Those are its targets.

00:14:30.000 --> 00:14:32.000
Up there is the skeleton,

00:14:32.000 --> 00:14:35.000
which is actually what it's trying to do.

00:14:42.000 --> 00:14:45.000
It's really about trying to create a novel body language for a new creature.

00:14:45.000 --> 00:14:47.000
Hollywood does this all the time, of course.

00:14:47.000 --> 00:14:49.000
But also have the body language communicate something

00:14:49.000 --> 00:14:51.000
to the person who is looking at it.

00:14:51.000 --> 00:14:53.000
This language is communicating that it is surprised to see you,

00:14:53.000 --> 00:14:56.000
and it's interested in looking at you.

00:14:56.000 --> 00:14:58.000
(Laughter)

00:14:58.000 --> 00:15:07.000
(Applause)

00:15:07.000 --> 00:15:09.000
Thank you very much. That's all I've got for today.

00:15:09.000 --> 00:15:12.000
And I'm really happy to be here. Thank you so much.

00:15:12.000 --> 00:15:15.000
(Applause)

