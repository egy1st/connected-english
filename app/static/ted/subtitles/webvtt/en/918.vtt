WEBVTT

00:00:00.000 --> 00:00:02.000
Chris Anderson: Julian, welcome.

00:00:02.000 --> 00:00:04.000
It's been reported that WikiLeaks, your baby,

00:00:04.000 --> 00:00:06.000
has, in the last few years

00:00:06.000 --> 00:00:09.000
has released more classified documents

00:00:09.000 --> 00:00:11.000
than the rest of the world's media combined.

00:00:11.000 --> 00:00:13.000
Can that possibly be true?

00:00:13.000 --> 00:00:15.000
Julian Assange: Yeah, can it possibly be true?

00:00:15.000 --> 00:00:18.000
It's a worry -- isn't it? -- that the rest of the world's media

00:00:18.000 --> 00:00:20.000
is doing such a bad job

00:00:20.000 --> 00:00:22.000
that a little group of activists

00:00:22.000 --> 00:00:24.000
is able to release more

00:00:24.000 --> 00:00:26.000
of that type of information

00:00:26.000 --> 00:00:28.000
than the rest of the world press combined.

00:00:28.000 --> 00:00:30.000
CA: How does it work?

00:00:30.000 --> 00:00:33.000
How do people release the documents?

00:00:33.000 --> 00:00:36.000
And how do you secure their privacy?

00:00:36.000 --> 00:00:38.000
JA: So these are -- as far as we can tell --

00:00:38.000 --> 00:00:40.000
classical whistleblowers,

00:00:40.000 --> 00:00:42.000
and we have a number of ways for them

00:00:42.000 --> 00:00:44.000
to get information to us.

00:00:44.000 --> 00:00:46.000
So we use this state-of-the-art encryption

00:00:46.000 --> 00:00:48.000
to bounce stuff around the Internet, to hide trails,

00:00:48.000 --> 00:00:50.000
pass it through legal jurisdictions

00:00:50.000 --> 00:00:53.000
like Sweden and Belgium

00:00:53.000 --> 00:00:56.000
to enact those legal protections.

00:00:57.000 --> 00:00:59.000
We get information in the mail,

00:00:59.000 --> 00:01:02.000
the regular postal mail,

00:01:02.000 --> 00:01:04.000
encrypted or not,

00:01:04.000 --> 00:01:07.000
vet it like a regular news organization, format it --

00:01:07.000 --> 00:01:10.000
which is sometimes something that's quite hard to do,

00:01:10.000 --> 00:01:12.000
when you're talking about

00:01:12.000 --> 00:01:14.000
giant databases of information --

00:01:14.000 --> 00:01:16.000
release it to the public

00:01:16.000 --> 00:01:18.000
and then defend ourselves

00:01:18.000 --> 00:01:21.000
against the inevitable legal and political attacks.

00:01:21.000 --> 00:01:23.000
CA: So you make an effort to ensure

00:01:23.000 --> 00:01:25.000
the documents are legitimate,

00:01:25.000 --> 00:01:27.000
but you actually

00:01:27.000 --> 00:01:30.000
almost never know who the identity of the source is?

00:01:30.000 --> 00:01:33.000
JA: That's right, yeah. Very rarely do we ever know,

00:01:34.000 --> 00:01:37.000
and if we find out at some stage

00:01:37.000 --> 00:01:40.000
then we destroy that information as soon as possible.

00:01:40.000 --> 00:01:42.000
(Phone ring) God damn it.

00:01:42.000 --> 00:01:46.000
(Laughter)

00:01:46.000 --> 00:01:48.000
CA: I think that's the CIA asking what the code is

00:01:48.000 --> 00:01:50.000
for a TED membership.

00:01:50.000 --> 00:01:53.000
(Laughter)

00:01:53.000 --> 00:01:55.000
So let's take [an] example, actually.

00:01:55.000 --> 00:01:57.000
This is something

00:01:57.000 --> 00:01:59.000
you leaked a few years ago.

00:01:59.000 --> 00:02:01.000
If we can have this document up ...

00:02:01.000 --> 00:02:03.000
So this was a story in Kenya a few years ago.

00:02:03.000 --> 00:02:06.000
Can you tell us what you leaked and what happened?

00:02:06.000 --> 00:02:08.000
JA: So this is the Kroll Report.

00:02:08.000 --> 00:02:11.000
This was a secret intelligence report

00:02:11.000 --> 00:02:13.000
commissioned by the Kenyan government

00:02:13.000 --> 00:02:16.000
after its election in 2004.

00:02:16.000 --> 00:02:18.000
Prior to 2004, Kenya was ruled

00:02:18.000 --> 00:02:20.000
by Daniel arap Moi

00:02:20.000 --> 00:02:22.000
for about 18 years.

00:02:22.000 --> 00:02:25.000
He was a soft dictator of Kenya.

00:02:25.000 --> 00:02:27.000
And when Kibaki got into power --

00:02:27.000 --> 00:02:29.000
through a coalition of forces that were trying

00:02:29.000 --> 00:02:31.000
to clean up corruption in Kenya --

00:02:31.000 --> 00:02:33.000
they commissioned this report,

00:02:33.000 --> 00:02:35.000
spent about two million pounds

00:02:35.000 --> 00:02:37.000
on this and an associated report.

00:02:37.000 --> 00:02:40.000
And then the government sat on it

00:02:40.000 --> 00:02:42.000
and used it for political leverage on Moi,

00:02:42.000 --> 00:02:44.000
who was the richest man --

00:02:44.000 --> 00:02:47.000
still is the richest man -- in Kenya.

00:02:47.000 --> 00:02:50.000
It's the Holy Grail of Kenyan journalism.

00:02:50.000 --> 00:02:53.000
So I went there in 2007,

00:02:53.000 --> 00:02:55.000
and we managed to get hold of this

00:02:55.000 --> 00:02:57.000
just prior to the election --

00:02:57.000 --> 00:03:00.000
the national election, December 28.

00:03:02.000 --> 00:03:05.000
When we released that report,

00:03:05.000 --> 00:03:08.000
we did so three days after the new president, Kibaki,

00:03:08.000 --> 00:03:10.000
had decided to pal up with

00:03:10.000 --> 00:03:12.000
the man that he was going to clean out,

00:03:12.000 --> 00:03:14.000
Daniel arap Moi,

00:03:14.000 --> 00:03:17.000
so this report then

00:03:17.000 --> 00:03:19.000
became a dead albatross

00:03:19.000 --> 00:03:22.000
around President Kibaki's neck.

00:03:23.000 --> 00:03:26.000
CA: And -- I mean, to cut a long story short --

00:03:26.000 --> 00:03:29.000
word of the report leaked into Kenya,

00:03:29.000 --> 00:03:32.000
not from the official media, but indirectly,

00:03:32.000 --> 00:03:35.000
and in your opinion, it actually shifted the election.

00:03:35.000 --> 00:03:38.000
JA: Yeah. So this became front page of the Guardian

00:03:38.000 --> 00:03:41.000
and was then printed in all the surrounding countries of Kenya,

00:03:41.000 --> 00:03:44.000
in Tanzanian and South African press.

00:03:44.000 --> 00:03:46.000
And so it came in from the outside.

00:03:46.000 --> 00:03:48.000
And that, after a couple of days,

00:03:48.000 --> 00:03:50.000
made the Kenyan press feel safe to talk about it.

00:03:50.000 --> 00:03:53.000
And it ran for 20 nights straight on Kenyan TV,

00:03:53.000 --> 00:03:56.000
shifted the vote by 10 percent,

00:03:56.000 --> 00:03:58.000
according to a Kenyan intelligence report,

00:03:58.000 --> 00:04:00.000
which changed the result of the election.

00:04:00.000 --> 00:04:02.000
CA: Wow, so your leak

00:04:02.000 --> 00:04:04.000
really substantially changed the world?

00:04:04.000 --> 00:04:06.000
JA: Yep.

00:04:06.000 --> 00:04:10.000
(Applause)

00:04:10.000 --> 00:04:12.000
CA: Here's -- We're going to just show

00:04:12.000 --> 00:04:15.000
a short clip from this

00:04:15.000 --> 00:04:17.000
Baghdad airstrike video.

00:04:17.000 --> 00:04:19.000
The video itself is longer,

00:04:19.000 --> 00:04:21.000
but here's a short clip.

00:04:21.000 --> 00:04:24.000
This is -- this is intense material, I should warn you.

00:04:24.000 --> 00:04:27.000
Radio: ... just fuckin', once you get on 'em just open 'em up.

00:04:27.000 --> 00:04:31.000
I see your element, uh, got about four Humvees, uh, out along ...

00:04:31.000 --> 00:04:34.000
You're clear. All right. Firing.

00:04:34.000 --> 00:04:37.000
Let me know when you've got them. Let's shoot.

00:04:37.000 --> 00:04:39.000
Light 'em all up.

00:04:39.000 --> 00:04:41.000
C'mon, fire!

00:04:41.000 --> 00:04:44.000
(Machine gun fire)

00:04:44.000 --> 00:04:47.000
Keep shoot 'n. Keep shoot 'n.

00:04:47.000 --> 00:04:50.000
(Machine gun fire)

00:04:50.000 --> 00:04:53.000
Keep shoot 'n.

00:04:53.000 --> 00:04:55.000
Hotel ... Bushmaster Two-Six, Bushmaster Two-Six,

00:04:55.000 --> 00:04:57.000
we need to move, time now!

00:04:57.000 --> 00:05:00.000
All right, we just engaged all eight individuals.

00:05:00.000 --> 00:05:03.000
Yeah, we see two birds [helicopters], and we're still firing.

00:05:03.000 --> 00:05:05.000
Roger. I got 'em.

00:05:05.000 --> 00:05:07.000
Two-Six, this is Two-Six, we're mobile.

00:05:07.000 --> 00:05:09.000
Oops, I'm sorry. What was going on?

00:05:09.000 --> 00:05:11.000
God damn it, Kyle. All right, hahaha. I hit 'em.

00:05:14.000 --> 00:05:17.000
CA: So, what was the impact of that?

00:05:17.000 --> 00:05:20.000
JA: The impact on the people who worked on it

00:05:20.000 --> 00:05:22.000
was severe.

00:05:22.000 --> 00:05:24.000
We ended up sending two people to Baghdad

00:05:24.000 --> 00:05:26.000
to further research that story.

00:05:26.000 --> 00:05:29.000
So this is just the first of three attacks

00:05:29.000 --> 00:05:31.000
that occurred in that scene.

00:05:31.000 --> 00:05:33.000
CA: So, I mean, 11 people died in that attack, right,

00:05:33.000 --> 00:05:35.000
including two Reuters employees?

00:05:35.000 --> 00:05:37.000
JA: Yeah. Two Reuters employees,

00:05:37.000 --> 00:05:40.000
two young children were wounded.

00:05:40.000 --> 00:05:43.000
There were between 18 and 26 people killed all together.

00:05:43.000 --> 00:05:45.000
CA: And releasing this caused

00:05:45.000 --> 00:05:47.000
widespread outrage.

00:05:47.000 --> 00:05:49.000
What was the key element of this

00:05:49.000 --> 00:05:52.000
that actually caused the outrage, do you think?

00:05:52.000 --> 00:05:54.000
JA: I don't know. I guess people can see

00:05:54.000 --> 00:05:57.000
the gross disparity in force.

00:05:57.000 --> 00:05:59.000
You have guys walking in a relaxed way down the street,

00:05:59.000 --> 00:06:02.000
and then an Apache helicopter sitting up at one kilometer

00:06:02.000 --> 00:06:04.000
firing 30-millimeter cannon shells

00:06:04.000 --> 00:06:06.000
on everyone --

00:06:06.000 --> 00:06:09.000
looking for any excuse to do so --

00:06:09.000 --> 00:06:11.000
and killing people rescuing the wounded.

00:06:11.000 --> 00:06:14.000
And there was two journalists involved that clearly weren't insurgents

00:06:14.000 --> 00:06:16.000
because that's their full-time job.

00:06:18.000 --> 00:06:21.000
CA: I mean, there's been this U.S. intelligence analyst,

00:06:21.000 --> 00:06:23.000
Bradley Manning, arrested,

00:06:23.000 --> 00:06:26.000
and it's alleged that he confessed in a chat room

00:06:26.000 --> 00:06:29.000
to have leaked this video to you,

00:06:29.000 --> 00:06:31.000
along with 280,000

00:06:31.000 --> 00:06:33.000
classified U.S. embassy cables.

00:06:33.000 --> 00:06:36.000
I mean, did he?

00:06:36.000 --> 00:06:38.000
JA: We have denied receiving those cables.

00:06:38.000 --> 00:06:40.000
He has been charged,

00:06:40.000 --> 00:06:42.000
about five days ago,

00:06:42.000 --> 00:06:45.000
with obtaining 150,000 cables

00:06:45.000 --> 00:06:47.000
and releasing 50.

00:06:47.000 --> 00:06:50.000
Now, we had released,

00:06:50.000 --> 00:06:52.000
early in the year,

00:06:52.000 --> 00:06:55.000
a cable from the Reykjavik U.S. embassy,

00:06:56.000 --> 00:06:58.000
but this is not necessarily connected.

00:06:58.000 --> 00:07:00.000
I mean, I was a known visitor of that embassy.

00:07:00.000 --> 00:07:02.000
CA: I mean, if you did receive thousands

00:07:02.000 --> 00:07:05.000
of U.S. embassy diplomatic cables ...

00:07:05.000 --> 00:07:07.000
JA: We would have released them. (CA: You would?)

00:07:07.000 --> 00:07:10.000
JA: Yeah. (CA: Because?)

00:07:10.000 --> 00:07:12.000
JA: Well, because these sort of things

00:07:12.000 --> 00:07:15.000
reveal what the true state

00:07:15.000 --> 00:07:17.000
of, say,

00:07:17.000 --> 00:07:19.000
Arab governments are like,

00:07:19.000 --> 00:07:22.000
the true human-rights abuses in those governments.

00:07:22.000 --> 00:07:24.000
If you look at declassified cables,

00:07:24.000 --> 00:07:26.000
that's the sort of material that's there.

00:07:26.000 --> 00:07:28.000
CA: So let's talk a little more broadly about this.

00:07:28.000 --> 00:07:30.000
I mean, in general, what's your philosophy?

00:07:30.000 --> 00:07:32.000
Why is it right

00:07:32.000 --> 00:07:35.000
to encourage leaking of secret information?

00:07:36.000 --> 00:07:39.000
JA: Well, there's a question as to what sort of information is important in the world,

00:07:39.000 --> 00:07:41.000
what sort of information

00:07:41.000 --> 00:07:43.000
can achieve reform.

00:07:43.000 --> 00:07:45.000
And there's a lot of information.

00:07:45.000 --> 00:07:47.000
So information that organizations

00:07:47.000 --> 00:07:50.000
are spending economic effort into concealing,

00:07:50.000 --> 00:07:52.000
that's a really good signal

00:07:52.000 --> 00:07:54.000
that when the information gets out,

00:07:54.000 --> 00:07:56.000
there's a hope of it doing some good --

00:07:56.000 --> 00:07:58.000
because the organizations that know it best,

00:07:58.000 --> 00:08:00.000
that know it from the inside out,

00:08:00.000 --> 00:08:03.000
are spending work to conceal it.

00:08:03.000 --> 00:08:05.000
And that's what we've found in practice,

00:08:05.000 --> 00:08:08.000
and that's what the history of journalism is.

00:08:08.000 --> 00:08:11.000
CA: But are there risks with that,

00:08:11.000 --> 00:08:14.000
either to the individuals concerned

00:08:14.000 --> 00:08:16.000
or indeed to society at large,

00:08:16.000 --> 00:08:18.000
where leaking can actually have

00:08:18.000 --> 00:08:20.000
an unintended consequence?

00:08:20.000 --> 00:08:22.000
JA: Not that we have seen with anything we have released.

00:08:22.000 --> 00:08:24.000
I mean, we have a harm immunization policy.

00:08:24.000 --> 00:08:26.000
We have a way of dealing with information

00:08:26.000 --> 00:08:28.000
that has sort of personal --

00:08:28.000 --> 00:08:30.000
personally identifying information in it.

00:08:31.000 --> 00:08:34.000
But there are legitimate secrets --

00:08:34.000 --> 00:08:37.000
you know, your records with your doctor;

00:08:37.000 --> 00:08:39.000
that's a legitimate secret --

00:08:39.000 --> 00:08:41.000
but we deal with whistleblowers that are coming forward

00:08:41.000 --> 00:08:44.000
that are really sort of well-motivated.

00:08:44.000 --> 00:08:46.000
CA: So they are well-motivated.

00:08:46.000 --> 00:08:48.000
And what would you say to, for example,

00:08:48.000 --> 00:08:51.000
the, you know, the parent of someone

00:08:51.000 --> 00:08:54.000
whose son is out serving the U.S. military,

00:08:54.000 --> 00:08:56.000
and he says, "You know what,

00:08:56.000 --> 00:08:58.000
you've put up something that someone had an incentive to put out.

00:08:58.000 --> 00:09:00.000
It shows a U.S. soldier laughing

00:09:00.000 --> 00:09:02.000
at people dying.

00:09:02.000 --> 00:09:04.000
That gives the impression, has given the impression,

00:09:04.000 --> 00:09:06.000
to millions of people around the world

00:09:06.000 --> 00:09:08.000
that U.S. soldiers are inhuman people.

00:09:08.000 --> 00:09:10.000
Actually, they're not. My son isn't. How dare you?"

00:09:10.000 --> 00:09:12.000
What would you say to that?

00:09:12.000 --> 00:09:14.000
JA: Yeah, we do get a lot of that.

00:09:14.000 --> 00:09:16.000
But remember, the people in Baghdad,

00:09:16.000 --> 00:09:19.000
the people in Iraq, the people in Afghanistan --

00:09:19.000 --> 00:09:21.000
they don't need to see the video;

00:09:21.000 --> 00:09:23.000
they see it every day.

00:09:23.000 --> 00:09:26.000
So it's not going to change their opinion. It's not going to change their perception.

00:09:26.000 --> 00:09:28.000
That's what they see every day.

00:09:28.000 --> 00:09:31.000
It will change the perception and opinion

00:09:31.000 --> 00:09:33.000
of the people who are paying for it all,

00:09:33.000 --> 00:09:36.000
and that's our hope.

00:09:36.000 --> 00:09:39.000
CA: So you found a way to shine light

00:09:39.000 --> 00:09:42.000
into what you see

00:09:42.000 --> 00:09:45.000
as these sort of dark secrets in companies and in government.

00:09:46.000 --> 00:09:48.000
Light is good.

00:09:48.000 --> 00:09:50.000
But do you see any irony in the fact that,

00:09:50.000 --> 00:09:52.000
in order for you to shine that light,

00:09:52.000 --> 00:09:54.000
you have to, yourself,

00:09:54.000 --> 00:09:57.000
create secrecy around your sources?

00:09:57.000 --> 00:10:00.000
JA: Not really. I mean, we don't have

00:10:00.000 --> 00:10:03.000
any WikiLeaks dissidents yet.

00:10:04.000 --> 00:10:07.000
We don't have sources who are dissidents on other sources.

00:10:08.000 --> 00:10:11.000
Should they come forward, that would be a tricky situation for us,

00:10:11.000 --> 00:10:14.000
but we're presumably acting in such a way

00:10:14.000 --> 00:10:16.000
that people feel

00:10:16.000 --> 00:10:18.000
morally compelled

00:10:18.000 --> 00:10:21.000
to continue our mission, not to screw it up.

00:10:22.000 --> 00:10:25.000
CA: I'd actually be interested, just based on what we've heard so far --

00:10:25.000 --> 00:10:28.000
I'm curious as to the opinion in the TED audience.

00:10:30.000 --> 00:10:32.000
You know, there might be a couple of views

00:10:32.000 --> 00:10:34.000
of WikiLeaks and of Julian.

00:10:34.000 --> 00:10:37.000
You know, hero -- people's hero --

00:10:37.000 --> 00:10:40.000
bringing this important light.

00:10:40.000 --> 00:10:42.000
Dangerous troublemaker.

00:10:43.000 --> 00:10:46.000
Who's got the hero view?

00:10:47.000 --> 00:10:50.000
Who's got the dangerous troublemaker view?

00:10:51.000 --> 00:10:53.000
JA: Oh, come on. There must be some.

00:10:54.000 --> 00:10:56.000
CA: It's a soft crowd, Julian, a soft crowd.

00:10:56.000 --> 00:10:58.000
We have to try better. Let's show them another example.

00:10:58.000 --> 00:11:01.000
Now here's something that you haven't yet leaked,

00:11:01.000 --> 00:11:04.000
but I think for TED you are.

00:11:04.000 --> 00:11:06.000
I mean it's an intriguing story that's just happened, right?

00:11:06.000 --> 00:11:08.000
What is this?

00:11:08.000 --> 00:11:10.000
JA: So this is a sample of what we do

00:11:10.000 --> 00:11:12.000
sort of every day.

00:11:12.000 --> 00:11:15.000
So late last year -- in November last year --

00:11:15.000 --> 00:11:17.000
there was a series of well blowouts

00:11:17.000 --> 00:11:19.000
in Albania,

00:11:19.000 --> 00:11:22.000
like the well blowout in the Gulf of Mexico,

00:11:22.000 --> 00:11:24.000
but not quite as big.

00:11:24.000 --> 00:11:27.000
And we got a report --

00:11:27.000 --> 00:11:30.000
a sort of engineering analysis into what happened --

00:11:30.000 --> 00:11:33.000
saying that, in fact, security guards

00:11:33.000 --> 00:11:36.000
from some rival, various competing oil firms

00:11:36.000 --> 00:11:39.000
had, in fact, parked trucks there and blown them up.

00:11:40.000 --> 00:11:43.000
And part of the Albanian government was in this, etc., etc.

00:11:44.000 --> 00:11:45.000
And the engineering report

00:11:45.000 --> 00:11:47.000
had nothing on the top of it,

00:11:47.000 --> 00:11:49.000
so it was an extremely difficult document for us.

00:11:49.000 --> 00:11:51.000
We couldn't verify it because we didn't know

00:11:51.000 --> 00:11:53.000
who wrote it and knew what it was about.

00:11:53.000 --> 00:11:55.000
So we were kind of skeptical that maybe it was

00:11:55.000 --> 00:11:57.000
a competing oil firm just sort of playing the issue up.

00:11:57.000 --> 00:11:59.000
So under that basis, we put it out and said,

00:11:59.000 --> 00:12:01.000
"Look, we're skeptical about this thing.

00:12:01.000 --> 00:12:03.000
We don't know, but what can we do?

00:12:03.000 --> 00:12:05.000
The material looks good, it feels right,

00:12:05.000 --> 00:12:07.000
but we just can't verify it."

00:12:07.000 --> 00:12:10.000
And we then got a letter

00:12:10.000 --> 00:12:13.000
just this week

00:12:13.000 --> 00:12:16.000
from the company who wrote it,

00:12:16.000 --> 00:12:19.000
wanting to track down the source --

00:12:19.000 --> 00:12:22.000
(Laughter)

00:12:23.000 --> 00:12:26.000
saying, "Hey, we want to track down the source."

00:12:26.000 --> 00:12:28.000
And we were like, "Oh, tell us more.

00:12:28.000 --> 00:12:31.000
What document is it, precisely, you're talking about?

00:12:31.000 --> 00:12:34.000
Can you show that you had legal authority over that document?

00:12:34.000 --> 00:12:36.000
Is it really yours?"

00:12:36.000 --> 00:12:39.000
So they sent us this screen shot

00:12:39.000 --> 00:12:41.000
with the author

00:12:41.000 --> 00:12:44.000
in the Microsoft Word ID.

00:12:46.000 --> 00:12:48.000
Yeah.

00:12:48.000 --> 00:12:53.000
(Applause)

00:12:53.000 --> 00:12:55.000
That's happened quite a lot though.

00:12:55.000 --> 00:12:57.000
This is like one of our methods

00:12:57.000 --> 00:13:00.000
of identifying, of verifying, what a material is,

00:13:00.000 --> 00:13:02.000
is to try and get these guys to write letters.

00:13:02.000 --> 00:13:05.000
CA: Yeah. Have you had information

00:13:05.000 --> 00:13:07.000
from inside BP?

00:13:07.000 --> 00:13:10.000
JA: Yeah, we have a lot, but I mean, at the moment,

00:13:10.000 --> 00:13:13.000
we are undergoing a sort of serious fundraising and engineering effort.

00:13:13.000 --> 00:13:15.000
So our publication rate

00:13:15.000 --> 00:13:17.000
over the past few months

00:13:17.000 --> 00:13:19.000
has been sort of minimized

00:13:19.000 --> 00:13:22.000
while we're re-engineering our back systems

00:13:22.000 --> 00:13:25.000
for the phenomenal public interest that we have.

00:13:25.000 --> 00:13:27.000
That's a problem.

00:13:27.000 --> 00:13:30.000
I mean, like any sort of growing startup organization,

00:13:30.000 --> 00:13:32.000
we are sort of overwhelmed

00:13:32.000 --> 00:13:34.000
by our growth,

00:13:34.000 --> 00:13:36.000
and that means we're getting enormous quantity

00:13:36.000 --> 00:13:38.000
of whistleblower disclosures

00:13:38.000 --> 00:13:40.000
of a very high caliber

00:13:40.000 --> 00:13:42.000
but don't have enough people to actually

00:13:42.000 --> 00:13:44.000
process and vet this information.

00:13:44.000 --> 00:13:46.000
CA: So that's the key bottleneck,

00:13:46.000 --> 00:13:48.000
basically journalistic volunteers

00:13:48.000 --> 00:13:51.000
and/or the funding of journalistic salaries?

00:13:51.000 --> 00:13:53.000
JA: Yep. Yeah, and trusted people.

00:13:53.000 --> 00:13:55.000
I mean, we're an organization

00:13:55.000 --> 00:13:57.000
that is hard to grow very quickly

00:13:57.000 --> 00:13:59.000
because of the sort of material we deal with,

00:13:59.000 --> 00:14:02.000
so we have to restructure

00:14:02.000 --> 00:14:04.000
in order to have people

00:14:04.000 --> 00:14:07.000
who will deal with the highest national security stuff,

00:14:07.000 --> 00:14:09.000
and then lower security cases.

00:14:09.000 --> 00:14:12.000
CA: So help us understand a bit about you personally

00:14:12.000 --> 00:14:14.000
and how you came to do this.

00:14:14.000 --> 00:14:16.000
And I think I read that as a kid

00:14:16.000 --> 00:14:19.000
you went to 37 different schools.

00:14:19.000 --> 00:14:21.000
Can that be right?

00:14:21.000 --> 00:14:24.000
JA: Well, my parents were in the movie business

00:14:24.000 --> 00:14:26.000
and then on the run from a cult,

00:14:26.000 --> 00:14:28.000
so the combination between the two ...

00:14:28.000 --> 00:14:32.000
(Laughter)

00:14:32.000 --> 00:14:34.000
CA: I mean, a psychologist might say

00:14:34.000 --> 00:14:37.000
that's a recipe for breeding paranoia.

00:14:37.000 --> 00:14:39.000
JA: What, the movie business?

00:14:39.000 --> 00:14:42.000
(Laughter)

00:14:42.000 --> 00:14:45.000
(Applause)

00:14:45.000 --> 00:14:47.000
CA: And you were also -- I mean,

00:14:47.000 --> 00:14:49.000
you were also a hacker at an early age

00:14:49.000 --> 00:14:52.000
and ran into the authorities early on.

00:14:52.000 --> 00:14:55.000
JA: Well, I was a journalist.

00:14:55.000 --> 00:14:57.000
You know, I was a very young journalist activist at an early age.

00:14:57.000 --> 00:14:59.000
I wrote a magazine,

00:14:59.000 --> 00:15:02.000
was prosecuted for it when I was a teenager.

00:15:02.000 --> 00:15:04.000
So you have to be careful with hacker.

00:15:04.000 --> 00:15:06.000
I mean there's like -- there's a method

00:15:06.000 --> 00:15:08.000
that can be deployed for various things.

00:15:08.000 --> 00:15:10.000
Unfortunately, at the moment,

00:15:10.000 --> 00:15:12.000
it's mostly deployed by the Russian mafia

00:15:12.000 --> 00:15:14.000
in order to steal your grandmother's bank accounts.

00:15:14.000 --> 00:15:17.000
So this phrase is not,

00:15:17.000 --> 00:15:19.000
not as nice as it used to be.

00:15:19.000 --> 00:15:21.000
CA: Yeah, well, I certainly don't think

00:15:21.000 --> 00:15:24.000
you're stealing anyone's grandmother's bank account,

00:15:24.000 --> 00:15:26.000
but what about

00:15:26.000 --> 00:15:28.000
your core values?

00:15:28.000 --> 00:15:31.000
Can you give us a sense of what they are

00:15:31.000 --> 00:15:33.000
and maybe some incident in your life

00:15:33.000 --> 00:15:36.000
that helped determine them?

00:15:38.000 --> 00:15:40.000
JA: I'm not sure about the incident.

00:15:40.000 --> 00:15:43.000
But the core values:

00:15:43.000 --> 00:15:46.000
well, capable, generous men

00:15:46.000 --> 00:15:48.000
do not create victims;

00:15:48.000 --> 00:15:50.000
they nurture victims.

00:15:50.000 --> 00:15:52.000
And that's something from my father

00:15:52.000 --> 00:15:55.000
and something from other capable, generous men

00:15:55.000 --> 00:15:58.000
that have been in my life.

00:15:58.000 --> 00:16:00.000
CA: Capable, generous men do not create victims;

00:16:00.000 --> 00:16:02.000
they nurture victims?

00:16:02.000 --> 00:16:04.000
JA: Yeah. And you know,

00:16:04.000 --> 00:16:08.000
I'm a combative person,

00:16:08.000 --> 00:16:10.000
so I'm not actually so big on the nurture,

00:16:10.000 --> 00:16:13.000
but some way --

00:16:13.000 --> 00:16:16.000
there is another way of nurturing victims,

00:16:16.000 --> 00:16:19.000
which is to police perpetrators

00:16:19.000 --> 00:16:21.000
of crime.

00:16:21.000 --> 00:16:23.000
And so that is something

00:16:23.000 --> 00:16:25.000
that has been in my character

00:16:25.000 --> 00:16:27.000
for a long time.

00:16:27.000 --> 00:16:30.000
CA: So just tell us, very quickly in the last minute, the story:

00:16:30.000 --> 00:16:33.000
what happened in Iceland?

00:16:33.000 --> 00:16:36.000
You basically published something there,

00:16:36.000 --> 00:16:39.000
ran into trouble with a bank,

00:16:39.000 --> 00:16:41.000
then the news service there

00:16:41.000 --> 00:16:44.000
was injuncted from running the story.

00:16:44.000 --> 00:16:46.000
Instead, they publicized your side.

00:16:46.000 --> 00:16:49.000
That made you very high-profile in Iceland. What happened next?

00:16:49.000 --> 00:16:51.000
JA: Yeah, this is a great case, you know.

00:16:51.000 --> 00:16:53.000
Iceland went through this financial crisis.

00:16:53.000 --> 00:16:55.000
It was the hardest hit of any country in the world.

00:16:55.000 --> 00:16:57.000
Its banking sector was 10 times the GDP

00:16:57.000 --> 00:16:59.000
of the rest of the economy.

00:16:59.000 --> 00:17:02.000
Anyway, so we release this report

00:17:02.000 --> 00:17:05.000
in July last year.

00:17:05.000 --> 00:17:07.000
And the national TV station was injuncted

00:17:07.000 --> 00:17:09.000
five minutes before it went on air,

00:17:09.000 --> 00:17:11.000
like out of a movie: injunction landed on the news desk,

00:17:11.000 --> 00:17:13.000
and the news reader was like,

00:17:13.000 --> 00:17:15.000
"This has never happened before. What do we do?"

00:17:15.000 --> 00:17:17.000
Well, we just show the website instead,

00:17:17.000 --> 00:17:20.000
for all that time, as a filler,

00:17:20.000 --> 00:17:22.000
and we became very famous in Iceland,

00:17:22.000 --> 00:17:25.000
went to Iceland and spoke about this issue.

00:17:25.000 --> 00:17:27.000
And there was a feeling in the community

00:17:27.000 --> 00:17:29.000
that that should never happen again,

00:17:29.000 --> 00:17:31.000
and as a result,

00:17:31.000 --> 00:17:33.000
working with Icelandic politicians

00:17:33.000 --> 00:17:35.000
and some other international legal experts,

00:17:35.000 --> 00:17:37.000
we put together a new sort of

00:17:37.000 --> 00:17:40.000
package of legislation for Iceland

00:17:40.000 --> 00:17:43.000
to sort of become an offshore haven

00:17:43.000 --> 00:17:46.000
for the free press,

00:17:46.000 --> 00:17:49.000
with the strongest journalistic protections in the world,

00:17:49.000 --> 00:17:51.000
with a new Nobel Prize

00:17:51.000 --> 00:17:53.000
for freedom of speech.

00:17:53.000 --> 00:17:55.000
Iceland's a Nordic country,

00:17:55.000 --> 00:17:58.000
so, like Norway, it's able to tap into the system.

00:17:58.000 --> 00:18:00.000
And just a month ago,

00:18:00.000 --> 00:18:03.000
this was passed by the Icelandic parliament unanimously.

00:18:03.000 --> 00:18:05.000
CA: Wow.

00:18:05.000 --> 00:18:11.000
(Applause)

00:18:11.000 --> 00:18:13.000
Last question, Julian.

00:18:13.000 --> 00:18:15.000
When you think of the future then,

00:18:15.000 --> 00:18:17.000
do you think it's more likely to be

00:18:17.000 --> 00:18:19.000
Big Brother exerting more control,

00:18:19.000 --> 00:18:21.000
more secrecy,

00:18:21.000 --> 00:18:23.000
or us watching

00:18:23.000 --> 00:18:25.000
Big Brother,

00:18:25.000 --> 00:18:28.000
or it's just all to be played for either way?

00:18:28.000 --> 00:18:30.000
JA: I'm not sure which way it's going to go.

00:18:30.000 --> 00:18:32.000
I mean, there's enormous pressures

00:18:32.000 --> 00:18:35.000
to harmonize freedom of speech legislation

00:18:35.000 --> 00:18:38.000
and transparency legislation around the world --

00:18:38.000 --> 00:18:40.000
within the E.U.,

00:18:40.000 --> 00:18:42.000
between China and the United States.

00:18:42.000 --> 00:18:45.000
Which way is it going to go? It's hard to see.

00:18:45.000 --> 00:18:47.000
That's why it's a very interesting time to be in --

00:18:47.000 --> 00:18:49.000
because with just a little bit of effort,

00:18:49.000 --> 00:18:52.000
we can shift it one way or the other.

00:18:52.000 --> 00:18:55.000
CA: Well, it looks like I'm reflecting the audience's opinion

00:18:55.000 --> 00:18:57.000
to say, Julian, be careful,

00:18:57.000 --> 00:18:59.000
and all power to you.

00:18:59.000 --> 00:19:01.000
JA: Thank you, Chris. (CA: Thank you.)

00:19:01.000 --> 00:19:11.000
(Applause)

