WEBVTT

00:00:01.000 --> 00:00:03.000
Up until now, our communication with machines

00:00:03.000 --> 00:00:05.000
has always been limited

00:00:05.000 --> 00:00:07.000
to conscious and direct forms.

00:00:07.000 --> 00:00:09.000
Whether it's something simple

00:00:09.000 --> 00:00:11.000
like turning on the lights with a switch,

00:00:11.000 --> 00:00:14.000
or even as complex as programming robotics,

00:00:14.000 --> 00:00:17.000
we have always had to give a command to a machine,

00:00:17.000 --> 00:00:19.000
or even a series of commands,

00:00:19.000 --> 00:00:22.000
in order for it to do something for us.

00:00:22.000 --> 00:00:24.000
Communication between people, on the other hand,

00:00:24.000 --> 00:00:27.000
is far more complex and a lot more interesting

00:00:27.000 --> 00:00:29.000
because we take into account

00:00:29.000 --> 00:00:32.000
so much more than what is explicitly expressed.

00:00:32.000 --> 00:00:35.000
We observe facial expressions, body language,

00:00:35.000 --> 00:00:37.000
and we can intuit feelings and emotions

00:00:37.000 --> 00:00:40.000
from our dialogue with one another.

00:00:40.000 --> 00:00:42.000
This actually forms a large part

00:00:42.000 --> 00:00:44.000
of our decision-making process.

00:00:44.000 --> 00:00:46.000
Our vision is to introduce

00:00:46.000 --> 00:00:49.000
this whole new realm of human interaction

00:00:49.000 --> 00:00:51.000
into human-computer interaction

00:00:51.000 --> 00:00:53.000
so that computers can understand

00:00:53.000 --> 00:00:55.000
not only what you direct it to do,

00:00:55.000 --> 00:00:57.000
but it can also respond

00:00:57.000 --> 00:00:59.000
to your facial expressions

00:00:59.000 --> 00:01:01.000
and emotional experiences.

00:01:01.000 --> 00:01:03.000
And what better way to do this

00:01:03.000 --> 00:01:05.000
than by interpreting the signals

00:01:05.000 --> 00:01:07.000
naturally produced by our brain,

00:01:07.000 --> 00:01:10.000
our center for control and experience.

00:01:10.000 --> 00:01:12.000
Well, it sounds like a pretty good idea,

00:01:12.000 --> 00:01:14.000
but this task, as Bruno mentioned,

00:01:14.000 --> 00:01:17.000
isn't an easy one for two main reasons:

00:01:17.000 --> 00:01:20.000
First, the detection algorithms.

00:01:20.000 --> 00:01:22.000
Our brain is made up of

00:01:22.000 --> 00:01:24.000
billions of active neurons,

00:01:24.000 --> 00:01:27.000
around 170,000 km

00:01:27.000 --> 00:01:29.000
of combined axon length.

00:01:29.000 --> 00:01:31.000
When these neurons interact,

00:01:31.000 --> 00:01:33.000
the chemical reaction emits an electrical impulse,

00:01:33.000 --> 00:01:35.000
which can be measured.

00:01:35.000 --> 00:01:38.000
The majority of our functional brain

00:01:38.000 --> 00:01:40.000
is distributed over

00:01:40.000 --> 00:01:42.000
the outer surface layer of the brain,

00:01:42.000 --> 00:01:45.000
and to increase the area that's available for mental capacity,

00:01:45.000 --> 00:01:48.000
the brain surface is highly folded.

00:01:48.000 --> 00:01:50.000
Now this cortical folding

00:01:50.000 --> 00:01:52.000
presents a significant challenge

00:01:52.000 --> 00:01:55.000
for interpreting surface electrical impulses.

00:01:55.000 --> 00:01:57.000
Each individual's cortex

00:01:57.000 --> 00:01:59.000
is folded differently,

00:01:59.000 --> 00:02:01.000
very much like a fingerprint.

00:02:01.000 --> 00:02:03.000
So even though a signal

00:02:03.000 --> 00:02:06.000
may come from the same functional part of the brain,

00:02:06.000 --> 00:02:08.000
by the time the structure has been folded,

00:02:08.000 --> 00:02:10.000
its physical location

00:02:10.000 --> 00:02:12.000
is very different between individuals,

00:02:12.000 --> 00:02:15.000
even identical twins.

00:02:15.000 --> 00:02:17.000
There is no longer any consistency

00:02:17.000 --> 00:02:19.000
in the surface signals.

00:02:19.000 --> 00:02:21.000
Our breakthrough was to create an algorithm

00:02:21.000 --> 00:02:23.000
that unfolds the cortex,

00:02:23.000 --> 00:02:25.000
so that we can map the signals

00:02:25.000 --> 00:02:27.000
closer to its source,

00:02:27.000 --> 00:02:30.000
and therefore making it capable of working across a mass population.

00:02:31.000 --> 00:02:33.000
The second challenge

00:02:33.000 --> 00:02:36.000
is the actual device for observing brainwaves.

00:02:36.000 --> 00:02:38.000
EEG measurements typically involve

00:02:38.000 --> 00:02:41.000
a hairnet with an array of sensors,

00:02:41.000 --> 00:02:44.000
like the one that you can see here in the photo.

00:02:44.000 --> 00:02:46.000
A technician will put the electrodes

00:02:46.000 --> 00:02:48.000
onto the scalp

00:02:48.000 --> 00:02:50.000
using a conductive gel or paste

00:02:50.000 --> 00:02:53.000
and usually after a procedure of preparing the scalp

00:02:53.000 --> 00:02:55.000
by light abrasion.

00:02:55.000 --> 00:02:57.000
Now this is quite time consuming

00:02:57.000 --> 00:02:59.000
and isn't the most comfortable process.

00:02:59.000 --> 00:03:01.000
And on top of that, these systems

00:03:01.000 --> 00:03:04.000
actually cost in the tens of thousands of dollars.

00:03:05.000 --> 00:03:08.000
So with that, I'd like to invite onstage

00:03:08.000 --> 00:03:10.000
Evan Grant, who is one of last year's speakers,

00:03:10.000 --> 00:03:12.000
who's kindly agreed

00:03:12.000 --> 00:03:14.000
to help me to demonstrate

00:03:14.000 --> 00:03:16.000
what we've been able to develop.

00:03:16.000 --> 00:03:22.000
(Applause)

00:03:22.000 --> 00:03:24.000
So the device that you see

00:03:24.000 --> 00:03:26.000
is a 14-channel, high-fidelity

00:03:26.000 --> 00:03:28.000
EEG acquisition system.

00:03:28.000 --> 00:03:31.000
It doesn't require any scalp preparation,

00:03:31.000 --> 00:03:33.000
no conductive gel or paste.

00:03:33.000 --> 00:03:36.000
It only takes a few minutes to put on

00:03:36.000 --> 00:03:38.000
and for the signals to settle.

00:03:38.000 --> 00:03:40.000
It's also wireless,

00:03:40.000 --> 00:03:43.000
so it gives you the freedom to move around.

00:03:43.000 --> 00:03:46.000
And compared to the tens of thousands of dollars

00:03:46.000 --> 00:03:49.000
for a traditional EEG system,

00:03:49.000 --> 00:03:51.000
this headset only costs

00:03:51.000 --> 00:03:53.000
a few hundred dollars.

00:03:53.000 --> 00:03:56.000
Now on to the detection algorithms.

00:03:56.000 --> 00:03:58.000
So facial expressions --

00:03:58.000 --> 00:04:00.000
as I mentioned before in emotional experiences --

00:04:00.000 --> 00:04:02.000
are actually designed to work out of the box

00:04:02.000 --> 00:04:04.000
with some sensitivity adjustments

00:04:04.000 --> 00:04:07.000
available for personalization.

00:04:07.000 --> 00:04:09.000
But with the limited time we have available,

00:04:09.000 --> 00:04:11.000
I'd like to show you the cognitive suite,

00:04:11.000 --> 00:04:13.000
which is the ability for you

00:04:13.000 --> 00:04:16.000
to basically move virtual objects with your mind.

00:04:17.000 --> 00:04:19.000
Now, Evan is new to this system,

00:04:19.000 --> 00:04:21.000
so what we have to do first

00:04:21.000 --> 00:04:23.000
is create a new profile for him.

00:04:23.000 --> 00:04:26.000
He's obviously not Joanne -- so we'll "add user."

00:04:26.000 --> 00:04:28.000
Evan. Okay.

00:04:28.000 --> 00:04:31.000
So the first thing we need to do with the cognitive suite

00:04:31.000 --> 00:04:33.000
is to start with training

00:04:33.000 --> 00:04:35.000
a neutral signal.

00:04:35.000 --> 00:04:37.000
With neutral, there's nothing in particular

00:04:37.000 --> 00:04:39.000
that Evan needs to do.

00:04:39.000 --> 00:04:41.000
He just hangs out. He's relaxed.

00:04:41.000 --> 00:04:43.000
And the idea is to establish a baseline

00:04:43.000 --> 00:04:45.000
or normal state for his brain,

00:04:45.000 --> 00:04:47.000
because every brain is different.

00:04:47.000 --> 00:04:49.000
It takes eight seconds to do this,

00:04:49.000 --> 00:04:51.000
and now that that's done,

00:04:51.000 --> 00:04:53.000
we can choose a movement-based action.

00:04:53.000 --> 00:04:55.000
So Evan, choose something

00:04:55.000 --> 00:04:57.000
that you can visualize clearly in your mind.

00:04:57.000 --> 00:04:59.000
Evan Grant: Let's do "pull."

00:04:59.000 --> 00:05:01.000
Tan Le: Okay, so let's choose "pull."

00:05:01.000 --> 00:05:03.000
So the idea here now

00:05:03.000 --> 00:05:05.000
is that Evan needs to

00:05:05.000 --> 00:05:07.000
imagine the object coming forward

00:05:07.000 --> 00:05:09.000
into the screen,

00:05:09.000 --> 00:05:12.000
and there's a progress bar that will scroll across the screen

00:05:12.000 --> 00:05:14.000
while he's doing that.

00:05:14.000 --> 00:05:16.000
The first time, nothing will happen,

00:05:16.000 --> 00:05:19.000
because the system has no idea how he thinks about "pull."

00:05:19.000 --> 00:05:21.000
But maintain that thought

00:05:21.000 --> 00:05:23.000
for the entire duration of the eight seconds.

00:05:23.000 --> 00:05:26.000
So: one, two, three, go.

00:05:34.000 --> 00:05:36.000
Okay.

00:05:36.000 --> 00:05:38.000
So once we accept this,

00:05:38.000 --> 00:05:40.000
the cube is live.

00:05:40.000 --> 00:05:42.000
So let's see if Evan

00:05:42.000 --> 00:05:45.000
can actually try and imagine pulling.

00:05:45.000 --> 00:05:47.000
Ah, good job!

00:05:47.000 --> 00:05:50.000
(Applause)

00:05:50.000 --> 00:05:52.000
That's really amazing.

00:05:52.000 --> 00:05:56.000
(Applause)

00:05:56.000 --> 00:05:58.000
So we have a little bit of time available,

00:05:58.000 --> 00:06:00.000
so I'm going to ask Evan

00:06:00.000 --> 00:06:02.000
to do a really difficult task.

00:06:02.000 --> 00:06:04.000
And this one is difficult

00:06:04.000 --> 00:06:07.000
because it's all about being able to visualize something

00:06:07.000 --> 00:06:09.000
that doesn't exist in our physical world.

00:06:09.000 --> 00:06:11.000
This is "disappear."

00:06:11.000 --> 00:06:13.000
So what you want to do -- at least with movement-based actions,

00:06:13.000 --> 00:06:16.000
we do that all the time, so you can visualize it.

00:06:16.000 --> 00:06:18.000
But with "disappear," there's really no analogies --

00:06:18.000 --> 00:06:20.000
so Evan, what you want to do here

00:06:20.000 --> 00:06:23.000
is to imagine the cube slowly fading out, okay.

00:06:23.000 --> 00:06:26.000
Same sort of drill. So: one, two, three, go.

00:06:35.000 --> 00:06:38.000
Okay. Let's try that.

00:06:38.000 --> 00:06:41.000
Oh, my goodness. He's just too good.

00:06:42.000 --> 00:06:44.000
Let's try that again.

00:06:49.000 --> 00:06:51.000
EG: Losing concentration.

00:06:51.000 --> 00:06:53.000
(Laughter)

00:06:53.000 --> 00:06:55.000
TL: But we can see that it actually works,

00:06:55.000 --> 00:06:57.000
even though you can only hold it

00:06:57.000 --> 00:06:59.000
for a little bit of time.

00:06:59.000 --> 00:07:02.000
As I said, it's a very difficult process

00:07:02.000 --> 00:07:04.000
to imagine this.

00:07:04.000 --> 00:07:06.000
And the great thing about it is that

00:07:06.000 --> 00:07:08.000
we've only given the software one instance

00:07:08.000 --> 00:07:11.000
of how he thinks about "disappear."

00:07:11.000 --> 00:07:14.000
As there is a machine learning algorithm in this --

00:07:14.000 --> 00:07:18.000
(Applause)

00:07:18.000 --> 00:07:20.000
Thank you.

00:07:20.000 --> 00:07:23.000
Good job. Good job.

00:07:23.000 --> 00:07:25.000
(Applause)

00:07:25.000 --> 00:07:28.000
Thank you, Evan, you're a wonderful, wonderful

00:07:28.000 --> 00:07:31.000
example of the technology.

00:07:31.000 --> 00:07:33.000
So, as you can see, before,

00:07:33.000 --> 00:07:36.000
there is a leveling system built into this software

00:07:36.000 --> 00:07:38.000
so that as Evan, or any user,

00:07:38.000 --> 00:07:40.000
becomes more familiar with the system,

00:07:40.000 --> 00:07:43.000
they can continue to add more and more detections,

00:07:43.000 --> 00:07:45.000
so that the system begins to differentiate

00:07:45.000 --> 00:07:48.000
between different distinct thoughts.

00:07:49.000 --> 00:07:51.000
And once you've trained up the detections,

00:07:51.000 --> 00:07:53.000
these thoughts can be assigned or mapped

00:07:53.000 --> 00:07:55.000
to any computing platform,

00:07:55.000 --> 00:07:57.000
application or device.

00:07:57.000 --> 00:07:59.000
So I'd like to show you a few examples,

00:07:59.000 --> 00:08:01.000
because there are many possible applications

00:08:01.000 --> 00:08:03.000
for this new interface.

00:08:04.000 --> 00:08:06.000
In games and virtual worlds, for example,

00:08:06.000 --> 00:08:08.000
your facial expressions

00:08:08.000 --> 00:08:10.000
can naturally and intuitively be used

00:08:10.000 --> 00:08:13.000
to control an avatar or virtual character.

00:08:14.000 --> 00:08:16.000
Obviously, you can experience the fantasy of magic

00:08:16.000 --> 00:08:19.000
and control the world with your mind.

00:08:21.000 --> 00:08:24.000
And also, colors, lighting,

00:08:24.000 --> 00:08:26.000
sound and effects

00:08:26.000 --> 00:08:28.000
can dynamically respond to your emotional state

00:08:28.000 --> 00:08:31.000
to heighten the experience that you're having, in real time.

00:08:32.000 --> 00:08:34.000
And moving on to some applications

00:08:34.000 --> 00:08:37.000
developed by developers and researchers around the world,

00:08:37.000 --> 00:08:40.000
with robots and simple machines, for example --

00:08:40.000 --> 00:08:42.000
in this case, flying a toy helicopter

00:08:42.000 --> 00:08:45.000
simply by thinking "lift" with your mind.

00:08:45.000 --> 00:08:47.000
The technology can also be applied

00:08:47.000 --> 00:08:49.000
to real world applications --

00:08:49.000 --> 00:08:51.000
in this example, a smart home.

00:08:51.000 --> 00:08:54.000
You know, from the user interface of the control system

00:08:54.000 --> 00:08:56.000
to opening curtains

00:08:56.000 --> 00:08:59.000
or closing curtains.

00:09:07.000 --> 00:09:10.000
And of course, also to the lighting --

00:09:10.000 --> 00:09:13.000
turning them on

00:09:13.000 --> 00:09:15.000
or off.

00:09:15.000 --> 00:09:17.000
And finally,

00:09:17.000 --> 00:09:19.000
to real life-changing applications,

00:09:19.000 --> 00:09:22.000
such as being able to control an electric wheelchair.

00:09:22.000 --> 00:09:24.000
In this example,

00:09:24.000 --> 00:09:27.000
facial expressions are mapped to the movement commands.

00:09:27.000 --> 00:09:30.000
Man: Now blink right to go right.

00:09:35.000 --> 00:09:38.000
Now blink left to turn back left.

00:09:47.000 --> 00:09:50.000
Now smile to go straight.

00:09:53.000 --> 00:09:55.000
TL: We really -- Thank you.

00:09:55.000 --> 00:10:00.000
(Applause)

00:10:00.000 --> 00:10:03.000
We are really only scratching the surface of what is possible today,

00:10:03.000 --> 00:10:05.000
and with the community's input,

00:10:05.000 --> 00:10:07.000
and also with the involvement of developers

00:10:07.000 --> 00:10:10.000
and researchers from around the world,

00:10:10.000 --> 00:10:12.000
we hope that you can help us to shape

00:10:12.000 --> 00:10:15.000
where the technology goes from here. Thank you so much.

